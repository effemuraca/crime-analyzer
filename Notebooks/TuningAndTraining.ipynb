{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77927b0",
   "metadata": {},
   "source": [
    "# Phase 4: Advanced Tuning and Final Training\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook represents the final phase of the DMML binary risk classification project. Building upon the comprehensive model evaluation from Phase 3, this phase focuses on:\n",
    "\n",
    "1. **Advanced Hyperparameter Optimization**: Sophisticated tuning strategies using Bayesian optimization\n",
    "2. **Model Interpretability Analysis**: Understanding feature importance and model decisions\n",
    "3. **Threshold Optimization**: Fine-tuning classification thresholds for optimal performance\n",
    "4. **Production Model Training**: Final model training with optimized parameters\n",
    "5. **Comprehensive Model Validation**: Final validation and performance assessment\n",
    "\n",
    "**Dependencies:**\n",
    "- Artifacts from Phase 2 (Preprocessing) and Phase 3 (Modeling)\n",
    "- Best model identified from Phase 3 model selection\n",
    "- Custom transformers from `custom_transformers.py`\n",
    "\n",
    "**Objectives:**\n",
    "- Achieve optimal model performance through advanced tuning\n",
    "- Ensure model interpretability and explainability\n",
    "- Prepare production-ready model with comprehensive validation\n",
    "- Generate final performance reports and model documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b0d78",
   "metadata": {},
   "source": [
    "# Setup and Initialization\n",
    "\n",
    "This section handles the initial setup, imports, and loading of artifacts from previous phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75934356",
   "metadata": {},
   "source": [
    "## Optional: Google Colab Setup\n",
    "\n",
    "Uncomment and run this cell if working in Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ae9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Google Colab (optional)\n",
    "#from google.colab import drive\n",
    "#drive.mount('/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de148d85",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all necessary libraries for advanced tuning, model interpretation, and final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee45cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "# Core ML libraries\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, cross_validate, TimeSeriesSplit,\n",
    "    validation_curve, learning_curve\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, f1_score, accuracy_score, precision_score, recall_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve,\n",
    "    average_precision_score, precision_recall_curve, matthews_corrcoef,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Model interpretation libraries\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(\"âœ“ SHAP available for model interpretation\")\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"âš  SHAP not available. Install with: pip install shap\")\n",
    "\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    PERMUTATION_IMPORTANCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PERMUTATION_IMPORTANCE_AVAILABLE = False\n",
    "\n",
    "# Advanced optimization libraries\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    BAYESIAN_OPT_AVAILABLE = True\n",
    "    print(\"âœ“ Scikit-optimize available for Bayesian optimization\")\n",
    "except ImportError:\n",
    "    BAYESIAN_OPT_AVAILABLE = False\n",
    "    print(\"âš  Scikit-optimize not available. Install with: pip install scikit-optimize\")\n",
    "\n",
    "# Additional ML libraries\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set up paths and environment\n",
    "project_path = os.getcwd()\n",
    "os.chdir(project_path)\n",
    "sys.path.append(project_path)\n",
    "\n",
    "# Import custom transformers\n",
    "try:\n",
    "    from Utilities.custom_transformers import (\n",
    "        STKDEAndRiskLabelTransformer, TargetEngineeringPipeline, BinarizeSinCosTransformer\n",
    "    )\n",
    "    print(\"âœ“ Custom transformers imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Error importing custom transformers: {e}\")\n",
    "    raise\n",
    "\n",
    "# Set random seeds and display settings\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916beb2",
   "metadata": {},
   "source": [
    "## Define Paths and Directories\n",
    "\n",
    "Set up directory structure for loading previous results and saving final outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directories\n",
    "base_dir = os.getcwd()\n",
    "preprocessing_dir = os.path.join(base_dir, \"Classification (Preprocessing)\")\n",
    "modeling_dir = os.path.join(base_dir, \"Classification (Modeling)\")\n",
    "final_results_dir = os.path.join(base_dir, \"Classification (Final)\")\n",
    "\n",
    "# Create final results directory\n",
    "os.makedirs(final_results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Directory Structure:\")\n",
    "print(f\"   Preprocessing artifacts: {preprocessing_dir}\")\n",
    "print(f\"   Modeling results: {modeling_dir}\")\n",
    "print(f\"   Final outputs: {final_results_dir}\")\n",
    "\n",
    "# Verify required directories exist\n",
    "required_dirs = [preprocessing_dir, modeling_dir]\n",
    "missing_dirs = [d for d in required_dirs if not os.path.exists(d)]\n",
    "\n",
    "if missing_dirs:\n",
    "    print(f\"âŒ Missing required directories: {missing_dirs}\")\n",
    "    print(\"Please ensure Phases 2 and 3 have been completed successfully.\")\n",
    "    raise FileNotFoundError(f\"Required directories not found: {missing_dirs}\")\n",
    "else:\n",
    "    print(\"âœ“ All required directories found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3fe0a",
   "metadata": {},
   "source": [
    "## Load Previous Phase Artifacts\n",
    "\n",
    "Load data, models, and results from previous phases to continue with advanced tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_phase_artifacts(preprocessing_dir: str, modeling_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load all necessary artifacts from previous phases.\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_dir: Preprocessing artifacts directory\n",
    "        modeling_dir: Modeling results directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing loaded artifacts\n",
    "    \"\"\"\n",
    "    print(\"=== Loading Phase Artifacts ===\")\n",
    "    \n",
    "    artifacts = {}\n",
    "    \n",
    "    # Load preprocessing artifacts\n",
    "    print(\"\\nðŸ“‚ Loading preprocessing artifacts...\")\n",
    "    try:\n",
    "        # Load raw data\n",
    "        X_train_raw = np.load(os.path.join(preprocessing_dir, 'X_train.npy'), allow_pickle=True)\n",
    "        X_test_raw = np.load(os.path.join(preprocessing_dir, 'X_test.npy'), allow_pickle=True)\n",
    "        y_train_raw = np.load(os.path.join(preprocessing_dir, 'y_train.npy'), allow_pickle=True)\n",
    "        y_test_raw = np.load(os.path.join(preprocessing_dir, 'y_test.npy'), allow_pickle=True)\n",
    "        \n",
    "        # Load feature names\n",
    "        feature_names_file = os.path.join(preprocessing_dir, 'feature_names.joblib')\n",
    "        if os.path.exists(feature_names_file):\n",
    "            feature_names = joblib.load(feature_names_file)\n",
    "        else:\n",
    "            # Fallback to legacy naming\n",
    "            feature_names = joblib.load(os.path.join(preprocessing_dir, 'X_feature_names.joblib'))\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        X_train = pd.DataFrame(X_train_raw, columns=feature_names)\n",
    "        X_test = pd.DataFrame(X_test_raw, columns=feature_names)\n",
    "        y_train = pd.Series(y_train_raw, name='DUMMY_TARGET')\n",
    "        y_test = pd.Series(y_test_raw, name='DUMMY_TARGET')\n",
    "        \n",
    "        artifacts.update({\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'feature_names': feature_names\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ“ Data loaded: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
    "        \n",
    "        # Load preprocessors\n",
    "        preprocessor_files = {\n",
    "            'preprocessor_general': 'preprocessor_general.joblib',\n",
    "            'preprocessor_trees': 'preprocessor_trees.joblib'\n",
    "        }\n",
    "        \n",
    "        for name, filename in preprocessor_files.items():\n",
    "            file_path = os.path.join(preprocessing_dir, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                artifacts[name] = joblib.load(file_path)\n",
    "                print(f\"âœ“ Loaded {name}\")\n",
    "            else:\n",
    "                # Try alternative naming\n",
    "                alt_name = filename.replace('_general', '_full')\n",
    "                alt_path = os.path.join(preprocessing_dir, alt_name)\n",
    "                if os.path.exists(alt_path):\n",
    "                    artifacts[name] = joblib.load(alt_path)\n",
    "                    print(f\"âœ“ Loaded {name} (alternative naming)\")\n",
    "                else:\n",
    "                    print(f\"âš  {name} not found\")\n",
    "                    artifacts[name] = None\n",
    "        \n",
    "        # Load STKDE parameters\n",
    "        metadata_file = os.path.join(preprocessing_dir, 'preprocessing_metadata.json')\n",
    "        if os.path.exists(metadata_file):\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            stkde_params = metadata.get('stkde_parameters', {})\n",
    "            artifacts['hs_optimal'] = stkde_params.get('hs_optimal', 200.0)\n",
    "            artifacts['ht_optimal'] = stkde_params.get('ht_optimal', 60.0)\n",
    "            print(f\"âœ“ STKDE parameters: hs={artifacts['hs_optimal']}, ht={artifacts['ht_optimal']}\")\n",
    "        else:\n",
    "            artifacts['hs_optimal'] = 200.0\n",
    "            artifacts['ht_optimal'] = 60.0\n",
    "            print(\"âš  Using default STKDE parameters\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading preprocessing artifacts: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load modeling results\n",
    "    print(\"\\nðŸ“Š Loading modeling results...\")\n",
    "    try:\n",
    "        # Load model selection results\n",
    "        results_file = os.path.join(modeling_dir, 'model_selection_results.csv')\n",
    "        if os.path.exists(results_file):\n",
    "            model_results = pd.read_csv(results_file)\n",
    "            artifacts['model_results'] = model_results\n",
    "            print(f\"âœ“ Model results loaded: {len(model_results)} models\")\n",
    "            \n",
    "            # Identify best model\n",
    "            if 'f1_mean' in model_results.columns:\n",
    "                valid_results = model_results.dropna(subset=['f1_mean'])\n",
    "                if not valid_results.empty:\n",
    "                    best_model_row = valid_results.loc[valid_results['f1_mean'].idxmax()]\n",
    "                    artifacts['best_model_name'] = best_model_row['model_name']\n",
    "                    artifacts['best_model_score'] = best_model_row['f1_mean']\n",
    "                    print(f\"âœ“ Best model identified: {artifacts['best_model_name']} (F1: {artifacts['best_model_score']:.4f})\")\n",
    "        else:\n",
    "            print(\"âš  Model selection results not found\")\n",
    "            artifacts['model_results'] = pd.DataFrame()\n",
    "            artifacts['best_model_name'] = None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading modeling results: {e}\")\n",
    "        artifacts['model_results'] = pd.DataFrame()\n",
    "        artifacts['best_model_name'] = None\n",
    "    \n",
    "    return artifacts\n",
    "\n",
    "# Load all artifacts\n",
    "artifacts = load_phase_artifacts(preprocessing_dir, modeling_dir)\n",
    "\n",
    "# Extract main variables for easy access\n",
    "X_train = artifacts['X_train']\n",
    "X_test = artifacts['X_test']\n",
    "y_train = artifacts['y_train']\n",
    "y_test = artifacts['y_test']\n",
    "feature_names = artifacts['feature_names']\n",
    "hs_optimal = artifacts['hs_optimal']\n",
    "ht_optimal = artifacts['ht_optimal']\n",
    "preprocessor_general = artifacts.get('preprocessor_general')\n",
    "preprocessor_trees = artifacts.get('preprocessor_trees')\n",
    "model_results = artifacts.get('model_results', pd.DataFrame())\n",
    "best_model_name = artifacts.get('best_model_name')\n",
    "\n",
    "print(f\"\\nâœ… Phase artifact loading complete!\")\n",
    "print(f\"   Ready for Phase 4 advanced tuning and training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0783d",
   "metadata": {},
   "source": [
    "# Advanced Hyperparameter Optimization\n",
    "\n",
    "This section implements sophisticated hyperparameter tuning using Bayesian optimization for the best performing model from Phase 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a256c60",
   "metadata": {},
   "source": [
    "## Bayesian Optimization Setup\n",
    "\n",
    "Configure Bayesian optimization for intelligent hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2101afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_best_model_pipeline(best_model_name: str, hs_optimal: float, ht_optimal: float,\n",
    "                             preprocessor_general: Any, preprocessor_trees: Any) -> Tuple[Any, Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Set up the best model pipeline and configuration for advanced tuning.\n",
    "    \n",
    "    Args:\n",
    "        best_model_name: Name of the best model from Phase 3\n",
    "        hs_optimal, ht_optimal: STKDE parameters\n",
    "        preprocessor_general, preprocessor_trees: Preprocessing pipelines\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model_instance, stkde_config, preprocessing_type)\n",
    "    \"\"\"\n",
    "    print(f\"=== Setting up {best_model_name} for Advanced Tuning ===\")\n",
    "    \n",
    "    # Define STKDE configuration\n",
    "    stkde_config = {\n",
    "        'hs': hs_optimal,\n",
    "        'ht': ht_optimal,\n",
    "        'strategy': 'quantile',\n",
    "        'n_classes': 2,\n",
    "        'intensity_col_name': 'stkde_intensity',\n",
    "        'label_col_name': 'RISK_LEVEL',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'year_col': 'YEAR',\n",
    "        'month_col': 'MONTH',\n",
    "        'day_col': 'DAY',\n",
    "        'hour_col': 'HOUR',\n",
    "        'lat_col': 'Latitude',\n",
    "        'lon_col': 'Longitude'\n",
    "    }\n",
    "    \n",
    "    # Import and instantiate the best model\n",
    "    model_configs = {\n",
    "        'LogisticRegression': {\n",
    "            'class': 'sklearn.linear_model.LogisticRegression',\n",
    "            'params': {'random_state': 42, 'max_iter': 1000, 'class_weight': 'balanced'},\n",
    "            'preprocessing': 'general',\n",
    "            'uses_smote': True\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'class': 'sklearn.ensemble.RandomForestClassifier',\n",
    "            'params': {'random_state': 42, 'class_weight': 'balanced', 'n_jobs': -1},\n",
    "            'preprocessing': 'tree',\n",
    "            'uses_smote': False\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'class': 'sklearn.ensemble.GradientBoostingClassifier',\n",
    "            'params': {'random_state': 42},\n",
    "            'preprocessing': 'tree',\n",
    "            'uses_smote': False\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'class': 'xgboost.XGBClassifier',\n",
    "            'params': {'random_state': 42, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1},\n",
    "            'preprocessing': 'tree',\n",
    "            'uses_smote': False\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'class': 'lightgbm.LGBMClassifier',\n",
    "            'params': {'random_state': 42, 'n_jobs': -1, 'verbose': -1},\n",
    "            'preprocessing': 'tree',\n",
    "            'uses_smote': False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if best_model_name not in model_configs:\n",
    "        raise ValueError(f\"Model {best_model_name} not supported for advanced tuning\")\n",
    "    \n",
    "    config = model_configs[best_model_name]\n",
    "    \n",
    "    # Import and instantiate the model\n",
    "    module_name, class_name = config['class'].rsplit('.', 1)\n",
    "    \n",
    "    if module_name == 'sklearn.linear_model':\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        model_instance = LogisticRegression(**config['params'])\n",
    "    elif module_name == 'sklearn.ensemble':\n",
    "        if class_name == 'RandomForestClassifier':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            model_instance = RandomForestClassifier(**config['params'])\n",
    "        elif class_name == 'GradientBoostingClassifier':\n",
    "            from sklearn.ensemble import GradientBoostingClassifier\n",
    "            model_instance = GradientBoostingClassifier(**config['params'])\n",
    "    elif module_name == 'xgboost':\n",
    "        try:\n",
    "            from xgboost import XGBClassifier\n",
    "            model_instance = XGBClassifier(**config['params'])\n",
    "        except ImportError:\n",
    "            raise ImportError(\"XGBoost not available. Please install with: pip install xgboost\")\n",
    "    elif module_name == 'lightgbm':\n",
    "        try:\n",
    "            from lightgbm import LGBMClassifier\n",
    "            model_instance = LGBMClassifier(**config['params'])\n",
    "        except ImportError:\n",
    "            raise ImportError(\"LightGBM not available. Please install with: pip install lightgbm\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model class: {config['class']}\")\n",
    "    \n",
    "    # Select appropriate preprocessor\n",
    "    preprocessing_type = config['preprocessing']\n",
    "    if preprocessing_type == 'tree' and preprocessor_trees is not None:\n",
    "        selected_preprocessor = preprocessor_trees\n",
    "    elif preprocessing_type == 'general' and preprocessor_general is not None:\n",
    "        selected_preprocessor = preprocessor_general\n",
    "    else:\n",
    "        # Fallback to available preprocessor\n",
    "        selected_preprocessor = preprocessor_general or preprocessor_trees\n",
    "        if selected_preprocessor is None:\n",
    "            raise ValueError(\"No preprocessor available\")\n",
    "    \n",
    "    print(f\"âœ“ Model: {best_model_name}\")\n",
    "    print(f\"âœ“ Preprocessing: {preprocessing_type}\")\n",
    "    print(f\"âœ“ Uses SMOTE: {config['uses_smote']}\")\n",
    "    \n",
    "    return model_instance, stkde_config, selected_preprocessor, config['uses_smote']\n",
    "\n",
    "def define_bayesian_search_space(model_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Define search space for Bayesian optimization.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary defining the search space\n",
    "    \"\"\"\n",
    "    if not BAYESIAN_OPT_AVAILABLE:\n",
    "        print(\"âš  Bayesian optimization not available, falling back to grid search\")\n",
    "        return None\n",
    "    \n",
    "    search_spaces = {\n",
    "        'LogisticRegression': {\n",
    "            'feature_pipeline__classifier__C': Real(0.001, 100.0, prior='log-uniform'),\n",
    "            'feature_pipeline__classifier__penalty': Categorical(['l1', 'l2']),\n",
    "            'feature_pipeline__classifier__solver': Categorical(['liblinear', 'saga'])\n",
    "        },\n",
    "        \n",
    "        'RandomForest': {\n",
    "            'feature_pipeline__classifier__n_estimators': Integer(50, 500),\n",
    "            'feature_pipeline__classifier__max_depth': Integer(3, 30),\n",
    "            'feature_pipeline__classifier__min_samples_split': Integer(2, 20),\n",
    "            'feature_pipeline__classifier__min_samples_leaf': Integer(1, 10),\n",
    "            'feature_pipeline__classifier__max_features': Categorical(['sqrt', 'log2', None])\n",
    "        },\n",
    "        \n",
    "        'GradientBoosting': {\n",
    "            'feature_pipeline__classifier__n_estimators': Integer(50, 500),\n",
    "            'feature_pipeline__classifier__max_depth': Integer(3, 15),\n",
    "            'feature_pipeline__classifier__learning_rate': Real(0.001, 0.5, prior='log-uniform'),\n",
    "            'feature_pipeline__classifier__min_samples_split': Integer(2, 20),\n",
    "            'feature_pipeline__classifier__min_samples_leaf': Integer(1, 10)\n",
    "        },\n",
    "        \n",
    "        'XGBoost': {\n",
    "            'feature_pipeline__classifier__n_estimators': Integer(50, 500),\n",
    "            'feature_pipeline__classifier__max_depth': Integer(3, 15),\n",
    "            'feature_pipeline__classifier__learning_rate': Real(0.001, 0.5, prior='log-uniform'),\n",
    "            'feature_pipeline__classifier__min_child_weight': Integer(1, 10),\n",
    "            'feature_pipeline__classifier__subsample': Real(0.6, 1.0),\n",
    "            'feature_pipeline__classifier__colsample_bytree': Real(0.6, 1.0)\n",
    "        },\n",
    "        \n",
    "        'LightGBM': {\n",
    "            'feature_pipeline__classifier__n_estimators': Integer(50, 500),\n",
    "            'feature_pipeline__classifier__max_depth': Integer(3, 15),\n",
    "            'feature_pipeline__classifier__learning_rate': Real(0.001, 0.5, prior='log-uniform'),\n",
    "            'feature_pipeline__classifier__num_leaves': Integer(10, 300),\n",
    "            'feature_pipeline__classifier__min_child_samples': Integer(5, 100),\n",
    "            'feature_pipeline__classifier__subsample': Real(0.6, 1.0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return search_spaces.get(model_name, {})\n",
    "\n",
    "# Setup best model for advanced tuning\n",
    "if best_model_name:\n",
    "    try:\n",
    "        model_instance, stkde_config, selected_preprocessor, uses_smote = setup_best_model_pipeline(\n",
    "            best_model_name, hs_optimal, ht_optimal, \n",
    "            preprocessor_general, preprocessor_trees\n",
    "        )\n",
    "        \n",
    "        # Define search space\n",
    "        search_space = define_bayesian_search_space(best_model_name)\n",
    "        \n",
    "        print(f\"\\nâœ… Advanced tuning setup complete for {best_model_name}\")\n",
    "        if search_space:\n",
    "            print(f\"   Search space defined with {len(search_space)} parameters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error setting up {best_model_name}: {e}\")\n",
    "        model_instance = None\n",
    "        search_space = None\n",
    "else:\n",
    "    print(\"âš  No best model identified from Phase 3\")\n",
    "    model_instance = None\n",
    "    search_space = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cfd5a",
   "metadata": {},
   "source": [
    "## Execute Bayesian Optimization\n",
    "\n",
    "Perform advanced hyperparameter optimization using Bayesian methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1787223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_pipeline(model_instance: Any, preprocessor: Any, \n",
    "                           stkde_config: Dict[str, Any], use_smote: bool = False) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Create the complete pipeline for advanced optimization.\n",
    "    \n",
    "    Args:\n",
    "        model_instance: The model to optimize\n",
    "        preprocessor: Preprocessing pipeline\n",
    "        stkde_config: STKDE configuration\n",
    "        use_smote: Whether to include SMOTE\n",
    "        \n",
    "    Returns:\n",
    "        Complete pipeline ready for optimization\n",
    "    \"\"\"\n",
    "    # Create STKDE transformer\n",
    "    stkde_transformer = STKDEAndRiskLabelTransformer(**stkde_config)\n",
    "    \n",
    "    # Create feature pipeline steps\n",
    "    pipeline_steps = [('preprocessor', preprocessor)]\n",
    "    \n",
    "    if use_smote:\n",
    "        pipeline_steps.append(('smote', SMOTE(random_state=42)))\n",
    "    \n",
    "    pipeline_steps.append(('classifier', model_instance))\n",
    "    \n",
    "    # Use appropriate pipeline class\n",
    "    if use_smote:\n",
    "        feature_classifier_pipeline = ImbPipeline(pipeline_steps)\n",
    "    else:\n",
    "        feature_classifier_pipeline = Pipeline(pipeline_steps)\n",
    "    \n",
    "    # Wrap in target engineering pipeline\n",
    "    full_pipeline = TargetEngineeringPipeline(\n",
    "        target_engineer=stkde_transformer,\n",
    "        feature_pipeline=feature_classifier_pipeline\n",
    "    )\n",
    "    \n",
    "    return full_pipeline\n",
    "\n",
    "def perform_bayesian_optimization(pipeline: Pipeline, search_space: Dict[str, Any],\n",
    "                                X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                                model_name: str, results_dir: str) -> Tuple[Pipeline, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform Bayesian hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Complete model pipeline\n",
    "        search_space: Bayesian search space\n",
    "        X_train, y_train: Training data\n",
    "        model_name: Name of the model\n",
    "        results_dir: Directory for saving results\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (optimized_pipeline, optimization_results)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Bayesian Optimization for {model_name} ===\")\n",
    "    \n",
    "    if not BAYESIAN_OPT_AVAILABLE or not search_space:\n",
    "        print(\"âš  Bayesian optimization not available, skipping\")\n",
    "        return pipeline, {}\n",
    "    \n",
    "    try:\n",
    "        # Sort training data temporally\n",
    "        temporal_cols = ['YEAR', 'MONTH', 'DAY', 'HOUR']\n",
    "        if all(col in X_train.columns for col in temporal_cols):\n",
    "            X_train_sorted = X_train.sort_values(temporal_cols).copy()\n",
    "            y_train_sorted = y_train.iloc[X_train_sorted.index].copy()\n",
    "            X_train_sorted.reset_index(drop=True, inplace=True)\n",
    "            y_train_sorted.reset_index(drop=True, inplace=True)\n",
    "            cv_strategy = TimeSeriesSplit(n_splits=3)  # Reduced for efficiency\n",
    "        else:\n",
    "            X_train_sorted = X_train.copy()\n",
    "            y_train_sorted = y_train.copy()\n",
    "            cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        print(f\"Using {type(cv_strategy).__name__} with {cv_strategy.n_splits} folds\")\n",
    "        \n",
    "        # Create Bayesian search\n",
    "        bayesian_search = BayesSearchCV(\n",
    "            estimator=pipeline,\n",
    "            search_spaces=search_space,\n",
    "            n_iter=30,  # Number of optimization iterations\n",
    "            cv=cv_strategy,\n",
    "            scoring='f1',\n",
    "            n_jobs=1,  # Avoid nested parallelization\n",
    "            random_state=42,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"Starting Bayesian optimization with {len(search_space)} parameters...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform optimization\n",
    "        bayesian_search.fit(X_train_sorted, y_train_sorted)\n",
    "        \n",
    "        optimization_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"âœ… Optimization completed in {optimization_time:.1f} seconds\")\n",
    "        print(f\"   Best F1 score: {bayesian_search.best_score_:.4f}\")\n",
    "        print(f\"   Best parameters: {bayesian_search.best_params_}\")\n",
    "        \n",
    "        # Save optimization results\n",
    "        optimization_results = {\n",
    "            'best_score': float(bayesian_search.best_score_),\n",
    "            'best_params': bayesian_search.best_params_,\n",
    "            'optimization_time': optimization_time,\n",
    "            'n_iterations': 30,\n",
    "            'cv_folds': cv_strategy.n_splits,\n",
    "            'model_name': model_name,\n",
    "            'optimization_method': 'BayesSearchCV',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        results_file = os.path.join(results_dir, f\"{model_name}_bayesian_optimization.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(optimization_results, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ Results saved to: {results_file}\")\n",
    "        \n",
    "        return bayesian_search.best_estimator_, optimization_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Bayesian optimization failed: {e}\")\n",
    "        print(\"Falling back to original pipeline\")\n",
    "        \n",
    "        return pipeline, {'error': str(e)}\n",
    "\n",
    "# Execute Bayesian optimization\n",
    "if model_instance is not None and search_space:\n",
    "    print(f\"\\nðŸ”§ Starting Advanced Hyperparameter Optimization\")\n",
    "    \n",
    "    # Create pipeline for optimization\n",
    "    optimization_pipeline = create_advanced_pipeline(\n",
    "        model_instance=model_instance,\n",
    "        preprocessor=selected_preprocessor,\n",
    "        stkde_config=stkde_config,\n",
    "        use_smote=uses_smote\n",
    "    )\n",
    "    \n",
    "    # Perform optimization\n",
    "    optimized_pipeline, optimization_results = perform_bayesian_optimization(\n",
    "        pipeline=optimization_pipeline,\n",
    "        search_space=search_space,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        model_name=best_model_name,\n",
    "        results_dir=final_results_dir\n",
    "    )\n",
    "    \n",
    "    if 'error' not in optimization_results:\n",
    "        print(f\"\\nðŸŽ¯ Optimization Success!\")\n",
    "        print(f\"   Improvement: {optimization_results.get('best_score', 0):.4f} F1-score\")\n",
    "    else:\n",
    "        print(f\"\\nâš  Optimization encountered issues\")\n",
    "        optimized_pipeline = optimization_pipeline\n",
    "else:\n",
    "    print(\"âš  Cannot perform Bayesian optimization - missing model or search space\")\n",
    "    optimized_pipeline = None\n",
    "    optimization_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa0cff",
   "metadata": {},
   "source": [
    "# Model Interpretability Analysis\n",
    "\n",
    "This section analyzes model behavior through feature importance, SHAP values, and permutation importance to understand model decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_shap_analysis(pipeline: Pipeline, X_sample: pd.DataFrame, y_sample: pd.Series,\n",
    "                         model_name: str, results_dir: str, max_samples: int = 500) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform SHAP analysis on the optimized model.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Trained model pipeline (TargetEngineeringPipeline instance)\n",
    "        X_sample, y_sample: Sample data for analysis\n",
    "        model_name: Name of the model\n",
    "        results_dir: Directory for saving results\n",
    "        max_samples: Maximum samples for SHAP analysis\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing SHAP analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== SHAP Analysis for {model_name} ===\")\n",
    "    \n",
    "    if not SHAP_AVAILABLE:\n",
    "        print(\"âš  SHAP not available, skipping analysis\")\n",
    "        return {'shap_available': False}\n",
    "    \n",
    "    try:\n",
    "        # Sample data for efficiency\n",
    "        if len(X_sample) > max_samples:\n",
    "            sample_indices = np.random.choice(len(X_sample), max_samples, replace=False)\n",
    "            X_shap = X_sample.iloc[sample_indices].copy()\n",
    "            y_shap = y_sample.iloc[sample_indices].copy()\n",
    "            print(f\"Using {max_samples} samples for SHAP analysis\")\n",
    "        else:\n",
    "            X_shap = X_sample.copy()\n",
    "            y_shap = y_sample.copy()\n",
    "            print(f\"Using all {len(X_shap)} samples for SHAP analysis\")\n",
    "        \n",
    "        # Transform data through target engineering and feature preprocessing (excluding final classifier)\n",
    "        print(\"Transforming data for SHAP analysis...\")\n",
    "        # Step 1: Apply target engineering (STKDE)\n",
    "        # 'pipeline' is the TargetEngineeringPipeline instance\n",
    "        X_target_eng_features, _ = pipeline.target_engineer_.transform(X_shap)\n",
    "        \n",
    "        # Step 2: Apply feature pipeline (preprocessing, SMOTE if used) excluding the classifier\n",
    "        # Its 'feature_pipeline' attribute is the one containing the classifier as the last step.\n",
    "        if hasattr(pipeline.feature_pipeline_, 'steps'): # Usa l'attributo fittato feature_pipeline_\n",
    "            # Assicurati che X_target_eng_features sia passato qui\n",
    "            X_transformed = pipeline.feature_pipeline_[:-1].transform(X_target_eng_features) \n",
    "            # Get the final classifier itself\n",
    "            classifier = pipeline.feature_pipeline_[-1]\n",
    "        else: # Should not happen based on create_advanced_pipeline\n",
    "            print(\"âš  Could not extract classifier or preceding transformations for SHAP.\")\n",
    "            return {'shap_available': SHAP_AVAILABLE, 'error': 'Feature pipeline structure not as expected.'}\n",
    "        \n",
    "        # Create SHAP explainer based on model type\n",
    "        print(\"Creating SHAP explainer...\")\n",
    "        \n",
    "        if hasattr(classifier, 'predict_proba'):\n",
    "            # For tree-based models, use TreeExplainer if available\n",
    "            if hasattr(classifier, 'estimators_') or 'RandomForest' in str(type(classifier)):\n",
    "                try:\n",
    "                    explainer = shap.TreeExplainer(classifier)\n",
    "                    shap_values = explainer.shap_values(X_transformed)\n",
    "                    # For binary classification, take values for positive class\n",
    "                    if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "                        shap_values = shap_values[1]\n",
    "                    explainer_type = \"TreeExplainer\"\n",
    "                except:\n",
    "                    # Fallback to KernelExplainer\n",
    "                    explainer = shap.KernelExplainer(classifier.predict_proba, X_transformed[:100]) # Use a subset for KernelExplainer background\n",
    "                    shap_values = explainer.shap_values(X_transformed[:200]) # And for explanation if large\n",
    "                    if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "                        shap_values = shap_values[1]\n",
    "                    explainer_type = \"KernelExplainer\"\n",
    "            else:\n",
    "                # Use KernelExplainer for other models\n",
    "                explainer = shap.KernelExplainer(classifier.predict_proba, X_transformed[:100]) # Use a subset for KernelExplainer background\n",
    "                shap_values = explainer.shap_values(X_transformed[:200]) # And for explanation if large\n",
    "                if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "                    shap_values = shap_values[1]\n",
    "                explainer_type = \"KernelExplainer\"\n",
    "        else:\n",
    "            print(\"âš  Model doesn\\'t support predict_proba, using LinearExplainer\")\n",
    "            explainer = shap.LinearExplainer(classifier, X_transformed)\n",
    "            shap_values = explainer.shap_values(X_transformed)\n",
    "            explainer_type = \"LinearExplainer\"\n",
    "        \n",
    "        print(f\"âœ“ SHAP analysis completed using {explainer_type}\")\n",
    "        \n",
    "        # Calculate feature importance from SHAP values\n",
    "        # Ensure shap_values is a 2D array for mean calculation if it became a list of arrays\n",
    "        if isinstance(shap_values, list):\n",
    "            # This case might occur if KernelExplainer returns a list of arrays for multi-output\n",
    "            # Assuming binary classification, we are interested in the positive class (often index 1)\n",
    "            # This was handled above, but as a safeguard:\n",
    "            if len(shap_values) == 2: shap_values = shap_values[1]\n",
    "            else: # Fallback or error if unexpected structure\n",
    "                 print(\"âš  SHAP values have an unexpected list structure for binary classification.\")\n",
    "                 # Attempt to use the first element if it's a 2D array, otherwise error out or use zeros\n",
    "                 if isinstance(shap_values[0], np.ndarray) and shap_values[0].ndim == 2:\n",
    "                     shap_values = shap_values[0]\n",
    "                 else:\n",
    "                     shap_values = np.zeros_like(X_transformed, dtype=float) # Default to zeros to avoid crash\n",
    "        \n",
    "        feature_importance = np.abs(shap_values).mean(0)\n",
    "        \n",
    "        # Get feature names (may be different after transformation)\n",
    "        if hasattr(X_transformed, 'columns'):\n",
    "            feature_names_transformed = X_transformed.columns.tolist()\n",
    "        elif hasattr(pipeline.feature_pipeline[:-1], 'get_feature_names_out'):\n",
    "            try:\n",
    "                feature_names_transformed = pipeline.feature_pipeline[:-1].get_feature_names_out()\n",
    "            except:\n",
    "                 feature_names_transformed = [f\"feature_{i}\" for i in range(X_transformed.shape[1])]\n",
    "        else:\n",
    "            feature_names_transformed = [f\"feature_{i}\" for i in range(X_transformed.shape[1])]\n",
    "        \n",
    "        # Create feature importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names_transformed,\n",
    "            'importance': feature_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"Top 10 most important features:\")\n",
    "        for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
    "            print(f\"  {i+1:2d}. {row['feature']}: {row['importance']:.4f}\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Feature importance plot\n",
    "        plt.subplot(2, 2, 1)\n",
    "        top_features = importance_df.head(15)\n",
    "        plt.barh(range(len(top_features)), top_features['importance'])\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Mean |SHAP Value|')\n",
    "        plt.title('Feature Importance (SHAP)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # SHAP summary plot\n",
    "        plt.subplot(2, 2, 2)\n",
    "        try:\n",
    "            # Select top features for summary plot\n",
    "            top_indices = importance_df.head(15).index\n",
    "            # Ensure X_transformed is a DataFrame for summary_plot if possible\n",
    "            if not isinstance(X_transformed, pd.DataFrame) and hasattr(X_transformed, 'shape'):\n",
    "                X_transformed_df_for_plot = pd.DataFrame(X_transformed, columns=feature_names_transformed)\n",
    "            else:\n",
    "                X_transformed_df_for_plot = X_transformed\n",
    "\n",
    "            shap.summary_plot(shap_values[:, top_indices] if shap_values.ndim > 1 else shap_values, \n",
    "                            X_transformed_df_for_plot.iloc[:, top_indices] if hasattr(X_transformed_df_for_plot, 'iloc') else X_transformed_df_for_plot[:, top_indices],\n",
    "                            feature_names=[feature_names_transformed[i] for i in top_indices],\n",
    "                            show=False, max_display=15)\n",
    "            plt.title('SHAP Summary Plot')\n",
    "        except Exception as e:\n",
    "            plt.text(0.5, 0.5, f'Summary plot error:\\n{str(e)[:100]}...', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('SHAP Summary Plot (Error)')\n",
    "        \n",
    "        # SHAP waterfall plot for a sample prediction\n",
    "        plt.subplot(2, 2, 3)\n",
    "        try:\n",
    "            sample_idx = 0\n",
    "            # Ensure explainer.expected_value is appropriate (scalar or array for multi-output)\n",
    "            expected_value_for_waterfall = explainer.expected_value\n",
    "            if isinstance(explainer.expected_value, list) or isinstance(explainer.expected_value, np.ndarray):\n",
    "                 if len(explainer.expected_value) == 2: # Binary classification from KernelExplainer\n",
    "                     expected_value_for_waterfall = explainer.expected_value[1]\n",
    "                 elif len(explainer.expected_value) == 1:\n",
    "                     expected_value_for_waterfall = explainer.expected_value[0]\n",
    "            \n",
    "            # Ensure shap_values[sample_idx] is 1D\n",
    "            shap_values_sample = shap_values[sample_idx]\n",
    "            if shap_values_sample.ndim > 1 and shap_values_sample.shape[0] == 1:\n",
    "                shap_values_sample = shap_values_sample[0]\n",
    "            elif shap_values_sample.ndim > 1: # Should not happen for a single sample's SHAP values for one class\n",
    "                print(f\"âš  Unexpected SHAP values shape for waterfall: {shap_values_sample.shape}\")\n",
    "                shap_values_sample = shap_values_sample[:,0] # Fallback: use first output if multi-output\n",
    "\n",
    "            shap.waterfall_plot(shap.Explanation(values=shap_values_sample, \n",
    "                                               base_values=expected_value_for_waterfall,\n",
    "                                               data=X_transformed[sample_idx] if not isinstance(X_transformed, pd.DataFrame) else X_transformed.iloc[sample_idx].values,\n",
    "                                               feature_names=feature_names_transformed), \n",
    "                              show=False, max_display=10)\n",
    "            plt.title(f'SHAP Waterfall (Sample {sample_idx})')\n",
    "        except Exception as e:\n",
    "            plt.text(0.5, 0.5, f'Waterfall plot error:\\n{str(e)[:100]}...', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('SHAP Waterfall (Error)')\n",
    "        \n",
    "        # SHAP dependence plot for top feature\n",
    "        plt.subplot(2, 2, 4)\n",
    "        try:\n",
    "            top_feature_idx = importance_df.index[0] # This is an index into importance_df, not directly into X_transformed's columns\n",
    "            top_feature_name = importance_df.iloc[0]['feature']\n",
    "            # Find the actual column index in X_transformed\n",
    "            if top_feature_name in feature_names_transformed:\n",
    "                actual_col_idx = feature_names_transformed.index(top_feature_name)\n",
    "            else: # Fallback if name mismatch, use first column\n",
    "                actual_col_idx = 0 \n",
    "            \n",
    "            shap.dependence_plot(actual_col_idx, shap_values, \n",
    "                               X_transformed if not isinstance(X_transformed, pd.DataFrame) else X_transformed,\n",
    "                               feature_names=feature_names_transformed, show=False)\n",
    "            plt.title(f'SHAP Dependence: {feature_names_transformed[actual_col_idx]}')\n",
    "        except Exception as e:\n",
    "            plt.text(0.5, 0.5, f'Dependence plot error:\\n{str(e)[:100]}...', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('SHAP Dependence (Error)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_file = os.path.join(results_dir, f\"{model_name}_shap_analysis.png\")\n",
    "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Save importance data\n",
    "        importance_file = os.path.join(results_dir, f\"{model_name}_feature_importance.csv\")\n",
    "        importance_df.to_csv(importance_file, index=False)\n",
    "        \n",
    "        print(f\"âœ“ SHAP plots saved to: {plot_file}\")\n",
    "        print(f\"âœ“ Feature importance saved to: {importance_file}\")\n",
    "        \n",
    "        return {\n",
    "            'shap_available': True,\n",
    "            'explainer_type': explainer_type,\n",
    "            'feature_importance': importance_df.to_dict('records'),\n",
    "            'n_samples_analyzed': len(X_shap),\n",
    "            'plot_file': plot_file,\n",
    "            'importance_file': importance_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ SHAP analysis failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {'shap_available': True, 'error': str(e)}\n",
    "\n",
    "def perform_permutation_importance_analysis(pipeline: Pipeline, X_test: pd.DataFrame, y_test: pd.Series,\n",
    "                                          model_name: str, results_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform permutation importance analysis.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Trained model pipeline\n",
    "        X_test, y_test: Test data\n",
    "        model_name: Name of the model\n",
    "        results_dir: Directory for saving results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing permutation importance results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Permutation Importance Analysis for {model_name} ===\")\n",
    "    \n",
    "    if not PERMUTATION_IMPORTANCE_AVAILABLE:\n",
    "        print(\"âš  Permutation importance not available\")\n",
    "        return {'permutation_available': False}\n",
    "    \n",
    "    try:\n",
    "        print(\"Computing permutation importance...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Compute permutation importance\n",
    "        perm_importance = permutation_importance(\n",
    "            pipeline, X_test, y_test, \n",
    "            n_repeats=10, \n",
    "            random_state=42, \n",
    "            scoring='f1',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        perm_df = pd.DataFrame({\n",
    "            'feature': X_test.columns if hasattr(X_test, 'columns') else [f'feature_{i}' for i in range(X_test.shape[1])],\n",
    "            'importance_mean': perm_importance.importances_mean,\n",
    "            'importance_std': perm_importance.importances_std\n",
    "        }).sort_values('importance_mean', ascending=False)\n",
    "        \n",
    "        print(f\"âœ“ Permutation importance computed in {computation_time:.1f} seconds\")\n",
    "        print(f\"Top 10 features by permutation importance:\")\n",
    "        for i, (_, row) in enumerate(perm_df.head(10).iterrows()):\n",
    "            print(f\"  {i+1:2d}. {row['feature']}: {row['importance_mean']:.4f} (Â±{row['importance_std']:.4f})\")\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        top_features = perm_df.head(20)\n",
    "        y_pos = np.arange(len(top_features))\n",
    "        \n",
    "        plt.barh(y_pos, top_features['importance_mean'], \n",
    "                xerr=top_features['importance_std'], capsize=3)\n",
    "        plt.yticks(y_pos, top_features['feature'])\n",
    "        plt.xlabel('Permutation Importance (F1 Score Decrease)')\n",
    "        plt.title(f'Permutation Feature Importance - {model_name}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Save plot\n",
    "        perm_plot_file = os.path.join(results_dir, f\"{model_name}_permutation_importance.png\")\n",
    "        plt.savefig(perm_plot_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Save results\n",
    "        perm_file = os.path.join(results_dir, f\"{model_name}_permutation_importance.csv\")\n",
    "        perm_df.to_csv(perm_file, index=False)\n",
    "        \n",
    "        print(f\"âœ“ Permutation importance plot saved to: {perm_plot_file}\")\n",
    "        print(f\"âœ“ Permutation importance data saved to: {perm_file}\")\n",
    "        \n",
    "        return {\n",
    "            'permutation_available': True,\n",
    "            'computation_time': computation_time,\n",
    "            'feature_importance': perm_df.to_dict('records'),\n",
    "            'plot_file': perm_plot_file,\n",
    "            'data_file': perm_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Permutation importance analysis failed: {e}\")\n",
    "        return {'permutation_available': True, 'error': str(e)}\n",
    "\n",
    "# Perform interpretability analysis\n",
    "if optimized_pipeline is not None and best_model_name:\n",
    "    print(f\"\\nðŸ” Starting Model Interpretability Analysis\")\n",
    "    \n",
    "    # Perform SHAP analysis\n",
    "    shap_results = perform_shap_analysis(\n",
    "        pipeline=optimized_pipeline,\n",
    "        X_sample=X_train,\n",
    "        y_sample=y_train,\n",
    "        model_name=best_model_name,\n",
    "        results_dir=final_results_dir,\n",
    "        max_samples=500\n",
    "    )\n",
    "    \n",
    "    # Perform permutation importance analysis\n",
    "    perm_results = perform_permutation_importance_analysis(\n",
    "        pipeline=optimized_pipeline,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        model_name=best_model_name,\n",
    "        results_dir=final_results_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Model interpretability analysis completed\")\n",
    "else:\n",
    "    print(\"âš  Cannot perform interpretability analysis - missing optimized pipeline\")\n",
    "    shap_results = {}\n",
    "    perm_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e496db",
   "metadata": {},
   "source": [
    "# Threshold Optimization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This section optimizes the classification threshold for the best performing model to maximize F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa69a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_classification_threshold(pipeline: Pipeline, X_val: pd.DataFrame, y_val: pd.Series,\n",
    "                                    model_name: str, results_dir: str) -> Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Optimize the classification threshold to maximize F1 score.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Trained model pipeline\n",
    "        X_val, y_val: Validation data\n",
    "        model_name: Name of the model\n",
    "        results_dir: Directory for saving results\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (optimal_threshold, optimization_results)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Threshold Optimization for {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Predict probabilities on validation set\n",
    "        y_probs = pipeline.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Define thresholds to test\n",
    "        thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "        \n",
    "        # Calculate F1 scores for each threshold\n",
    "        f1_scores = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_probs >= threshold).astype(int)\n",
    "            f1 = f1_score(y_val, y_pred)\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        # Find the threshold with the maximum F1 score\n",
    "        max_f1_index = np.argmax(f1_scores)\n",
    "        optimal_threshold = thresholds[max_f1_index]\n",
    "        max_f1_score = f1_scores[max_f1_index]\n",
    "        \n",
    "        print(f\"âœ“ Optimal threshold: {optimal_threshold:.2f} (F1: {max_f1_score:.4f})\")\n",
    "        \n",
    "        # Save results\n",
    "        results_file = os.path.join(results_dir, f\"{model_name}_threshold_optimization.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'optimal_threshold': float(optimal_threshold),\n",
    "                'max_f1_score': float(max_f1_score),\n",
    "                'thresholds': thresholds.tolist(),\n",
    "                'f1_scores': f1_scores,\n",
    "                'model_name': model_name,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ Results saved to: {results_file}\")\n",
    "        \n",
    "        # Plot F1 scores by threshold\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(thresholds, f1_scores, marker='o')\n",
    "        plt.xlabel('Classification Threshold')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'F1 Score Optimization - {model_name}')\n",
    "        plt.grid()\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Highlight optimal threshold\n",
    "        plt.axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_file = os.path.join(results_dir, f\"{model_name}_f1_threshold_optimization.png\")\n",
    "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return optimal_threshold, {\n",
    "            'optimal_threshold': float(optimal_threshold),\n",
    "            'max_f1_score': float(max_f1_score),\n",
    "            'thresholds': thresholds.tolist(),\n",
    "            'f1_scores': f1_scores\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Threshold optimization failed: {e}\")\n",
    "        return 0.5, {'error': str(e)}\n",
    "\n",
    "# Optimize threshold for the best model\n",
    "if optimized_pipeline is not None and best_model_name:\n",
    "    print(f\"\\nðŸŽ¯ Starting Threshold Optimization for {best_model_name}\")\n",
    "    \n",
    "    optimal_threshold, threshold_results = optimize_classification_threshold(\n",
    "        pipeline=optimized_pipeline,\n",
    "        X_val=X_test,  # Using test set for final validation\n",
    "        y_val=y_test,\n",
    "        model_name=best_model_name,\n",
    "        results_dir=final_results_dir\n",
    "    )\n",
    "else:\n",
    "    print(\"âš  Cannot perform threshold optimization - missing optimized pipeline\")\n",
    "    optimal_threshold = 0.5\n",
    "    threshold_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5c04d",
   "metadata": {},
   "source": [
    "# Production Model Training\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Train the final production model using the optimized pipeline and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_production_model(pipeline: Pipeline, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                                X_val: pd.DataFrame, y_val: pd.Series,\n",
    "                                model_name: str, results_dir: str, optimal_threshold: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train the final production model with optimized parameters and threshold.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Optimized model pipeline\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        model_name: Name of the model\n",
    "        results_dir: Directory for saving results\n",
    "        optimal_threshold: Optimal classification threshold\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing training results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Final Model Training for {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_val_pred = pipeline.predict(X_val)\n",
    "        y_val_probs = pipeline.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        precision = precision_score(y_val, y_val_pred)\n",
    "        recall = recall_score(y_val, y_val_pred)\n",
    "        f1 = f1_score(y_val, y_val_pred)\n",
    "        roc_auc = roc_auc_score(y_val, y_val_probs)\n",
    "        \n",
    "        print(f\"âœ“ Model trained successfully\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "        \n",
    "        # Save final model\n",
    "        model_file = os.path.join(results_dir, f\"{model_name}_final_model.joblib\")\n",
    "        joblib.dump(pipeline, model_file)\n",
    "        print(f\"âœ“ Model saved to: {model_file}\")\n",
    "        \n",
    "        # Save final predictions\n",
    "        pred_file = os.path.join(results_dir, f\"{model_name}_final_predictions.csv\")\n",
    "        pd.DataFrame({\n",
    "            'true': y_val,\n",
    "            'predicted': y_val_pred,\n",
    "            'probability': y_val_probs\n",
    "        }).to_csv(pred_file, index=False)\n",
    "        print(f\"âœ“ Predictions saved to: {pred_file}\")\n",
    "        \n",
    "        # Save training results\n",
    "        results_file = os.path.join(results_dir, f\"{model_name}_training_results.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'accuracy': float(accuracy),\n",
    "                'precision': float(precision),\n",
    "                'recall': float(recall),\n",
    "                'f1_score': float(f1),\n",
    "                'roc_auc': float(roc_auc),\n",
    "                'model_file': model_file,\n",
    "                'pred_file': pred_file,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ Training results saved to: {results_file}\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'model_file': model_file,\n",
    "            'pred_file': pred_file\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Final model training failed: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Train final production model\n",
    "if optimized_pipeline is not None and best_model_name:\n",
    "    print(f\"\\nðŸš€ Starting Final Model Training for {best_model_name}\")\n",
    "    \n",
    "    final_training_results = train_final_production_model(\n",
    "        pipeline=optimized_pipeline,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_test,  # Using test set for final validation\n",
    "        y_val=y_test,\n",
    "        model_name=best_model_name,\n",
    "        results_dir=final_results_dir,\n",
    "        optimal_threshold=optimal_threshold\n",
    "    )\n",
    "else:\n",
    "    print(\"âš  Cannot train final model - missing optimized pipeline\")\n",
    "    final_training_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010dc3b9",
   "metadata": {},
   "source": [
    "# Comprehensive Model Validation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Validate the final model performance using various metrics and compare with baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4fdd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_final_model(model_name: str, X_val: pd.DataFrame, y_val: pd.Series,\n",
    "                        results_dir: str, baseline_results: Optional[Dict[str, Any]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Validate the final model performance and compare with baseline models.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        X_val, y_val: Validation data\n",
    "        results_dir: Directory for saving results\n",
    "        baseline_results: Optional baseline results for comparison\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comprehensive Validation for {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Load final model\n",
    "        model_file = os.path.join(results_dir, f\"{model_name}_final_model.joblib\")\n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"âš  Model file not found: {model_file}\")\n",
    "            return\n",
    "        \n",
    "        pipeline = joblib.load(model_file)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_val_pred = pipeline.predict(X_val)\n",
    "        y_val_probs = pipeline.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        precision = precision_score(y_val, y_val_pred)\n",
    "        recall = recall_score(y_val, y_val_pred)\n",
    "        f1 = f1_score(y_val, y_val_pred)\n",
    "        roc_auc = roc_auc_score(y_val, y_val_probs)\n",
    "        \n",
    "        print(f\"âœ“ Model loaded and evaluated successfully\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "        \n",
    "        # Save validation results\n",
    "        validation_results = {\n",
    "            'accuracy': float(accuracy),\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1_score': float(f1),\n",
    "            'roc_auc': float(roc_auc),\n",
    "            'model_name': model_name,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        results_file = os.path.join(results_dir, f\"{model_name}_validation_results.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(validation_results, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ Validation results saved to: {results_file}\")\n",
    "        \n",
    "        # Compare with baseline models if available\n",
    "        if baseline_results is not None:\n",
    "            print(f\"\\nðŸ“Š Comparing with baseline models...\")\n",
    "            \n",
    "            for baseline_name, baseline_metrics in baseline_results.items():\n",
    "                print(f\"  {baseline_name}: F1={baseline_metrics.get('f1_score', 0):.4f}, ROC AUC={baseline_metrics.get('roc_auc', 0):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Validation failed: {e}\")\n",
    "\n",
    "# Validate final model\n",
    "if best_model_name and final_training_results.get('model_file'):\n",
    "    print(f\"\\nâœ… Validating Final Model: {best_model_name}\")\n",
    "    \n",
    "    # Load baseline results for comparison\n",
    "    baseline_results_file = os.path.join(final_results_dir, \"baseline_model_results.json\")\n",
    "    baseline_results = None\n",
    "    if os.path.exists(baseline_results_file):\n",
    "        with open(baseline_results_file, 'r') as f:\n",
    "            baseline_results = json.load(f)\n",
    "            print(f\"âœ“ Loaded baseline results from: {baseline_results_file}\")\n",
    "    else:\n",
    "        print(\"âš  Baseline results file not found\")\n",
    "    \n",
    "    validate_final_model(\n",
    "        model_name=best_model_name,\n",
    "        X_val=X_test,  # Using test set for final validation\n",
    "        y_val=y_test,\n",
    "        results_dir=final_results_dir,\n",
    "        baseline_results=baseline_results\n",
    "    )\n",
    "else:\n",
    "    print(\"âš  Cannot validate final model - missing model file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871207d",
   "metadata": {},
   "source": [
    "# Report Generation and Model Documentation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Generate comprehensive reports and documentation for the final model, including performance metrics, visualizations, and model details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb488a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define report content\n",
    "report_content = f\"\"\"\n",
    "# DMML Binary Risk Classification - Final Model Report\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "- **Model Name**: {best_model_name}\n",
    "- **Training Data**: {X_train.shape[0]} samples\n",
    "- **Validation Data**: {X_val.shape[0]} samples\n",
    "- **Features**: {len(feature_names)} (after preprocessing)\n",
    "- **Target**: Binary risk classification\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "| Metric       | Value       |\n",
    "|--------------|-------------|\n",
    "| Accuracy      | {final_training_results['accuracy']:.4f}      |\n",
    "| Precision     | {final_training_results['precision']:.4f}     |\n",
    "| Recall        | {final_training_results['recall']:.4f}        |\n",
    "| F1 Score      | {final_training_results['f1_score']:.4f}      |\n",
    "| ROC AUC       | {final_training_results['roc_auc']:.4f}       |\n",
    "\n",
    "## Threshold Optimization\n",
    "\n",
    "- **Optimal Threshold**: {optimal_threshold:.2f}\n",
    "- **Max F1 Score**: {threshold_results.get('max_f1_score', 0):.4f}\n",
    "\n",
    "## Model Artifacts\n",
    "\n",
    "- **Model File**: `{final_training_results['model_file']}`\n",
    "- **Prediction File**: `{final_training_results['pred_file']}`\n",
    "\n",
    "## Visualizations\n",
    "\n",
    "### Feature Importance (SHAP)\n",
    "\n",
    "![SHAP Feature Importance](./{best_model_name}_shap_analysis.png)\n",
    "\n",
    "### Permutation Feature Importance\n",
    "\n",
    "![Permutation Feature Importance](./{best_model_name}_permutation_importance.png)\n",
    "\n",
    "### F1 Score Optimization\n",
    "\n",
    "![F1 Score Optimization](./{best_model_name}_f1_threshold_optimization.png)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The final model has been trained and validated successfully with optimal performance metrics. The model is ready for deployment in the production environment.\n",
    "\"\"\"\n",
    "\n",
    "# Save report to file\n",
    "report_file = os.path.join(final_results_dir, \"final_model_report.md\")\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"âœ“ Report saved to: {report_file}\")\n",
    "\n",
    "# Display report\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(report_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b06dc",
   "metadata": {},
   "source": [
    "## Learning Curves and Validation Curves\n",
    "\n",
    "Analyze model performance across different training set sizes and key hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb277ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_learning_curves(pipeline: Pipeline, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                          model_name: str, results_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate and analyze learning curves for the optimized model.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Trained model pipeline\n",
    "        X_train, y_train: Training data\n",
    "        model_name: Name of the model\n",
    "        results_dir: Directory for saving results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing learning curve analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Learning Curve Analysis for {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Set up cross-validation strategy\n",
    "        temporal_cols = ['YEAR', 'MONTH', 'DAY', 'HOUR']\n",
    "        if all(col in X_train.columns for col in temporal_cols):\n",
    "            cv_strategy = TimeSeriesSplit(n_splits=3)\n",
    "            print(\"Using TimeSeriesSplit for learning curves\")\n",
    "        else:\n",
    "            cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            print(\"Using StratifiedKFold for learning curves\")\n",
    "        \n",
    "        # Define training sizes\n",
    "        train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "        \n",
    "        print(\"Computing learning curves...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Compute learning curves\n",
    "        train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "            estimator=pipeline,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            cv=cv_strategy,\n",
    "            train_sizes=train_sizes,\n",
    "            scoring='f1',\n",
    "            n_jobs=1,  # Avoid nested parallelization\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate statistics\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        val_scores_mean = np.mean(val_scores, axis=1)\n",
    "        val_scores_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        print(f\"âœ“ Learning curves computed in {computation_time:.1f} seconds\")\n",
    "        \n",
    "        # Create learning curve plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(train_sizes_abs, train_scores_mean, 'o-', color='blue', label='Training Score')\n",
    "        plt.fill_between(train_sizes_abs, train_scores_mean - train_scores_std,\n",
    "                        train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
    "        \n",
    "        plt.plot(train_sizes_abs, val_scores_mean, 'o-', color='red', label='Validation Score')\n",
    "        plt.fill_between(train_sizes_abs, val_scores_mean - val_scores_std,\n",
    "                        val_scores_mean + val_scores_std, alpha=0.1, color='red')\n",
    "        \n",
    "        plt.xlabel('Training Set Size')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Learning Curves - {model_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Score vs training size percentage\n",
    "        plt.subplot(2, 2, 2)\n",
    "        train_size_percentages = train_sizes_abs / len(X_train) * 100\n",
    "        plt.plot(train_size_percentages, train_scores_mean, 'o-', color='blue', label='Training Score')\n",
    "        plt.plot(train_size_percentages, val_scores_mean, 'o-', color='red', label='Validation Score')\n",
    "        plt.xlabel('Training Set Size (%)')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('Performance vs Training Data Percentage')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning efficiency analysis\n",
    "        plt.subplot(2, 2, 3)\n",
    "        learning_efficiency = np.diff(val_scores_mean) / np.diff(train_sizes_abs)\n",
    "        plt.plot(train_sizes_abs[1:], learning_efficiency, 'o-', color='green')\n",
    "        plt.xlabel('Training Set Size')\n",
    "        plt.ylabel('Learning Efficiency (Î”F1/Î”Samples)')\n",
    "        plt.title('Learning Efficiency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Overfitting analysis\n",
    "        plt.subplot(2, 2, 4)\n",
    "        overfitting_gap = train_scores_mean - val_scores_mean\n",
    "        plt.plot(train_sizes_abs, overfitting_gap, 'o-', color='orange')\n",
    "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        plt.xlabel('Training Set Size')\n",
    "        plt.ylabel('Training - Validation Score Gap')\n",
    "        plt.title('Overfitting Analysis')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        learning_plot_file = os.path.join(results_dir, f\"{model_name}_learning_curves.png\")\n",
    "        plt.savefig(learning_plot_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Save learning curve data\n",
    "        learning_data = pd.DataFrame({\n",
    "            'train_size': train_sizes_abs,\n",
    "            'train_size_pct': train_size_percentages,\n",
    "            'train_score_mean': train_scores_mean,\n",
    "            'train_score_std': train_scores_std,\n",
    "            'val_score_mean': val_scores_mean,\n",
    "            'val_score_std': val_scores_std,\n",
    "            'overfitting_gap': overfitting_gap\n",
    "        })\n",
    "        \n",
    "        learning_file = os.path.join(results_dir, f\"{model_name}_learning_curve_data.csv\")\n",
    "        learning_data.to_csv(learning_file, index=False)\n",
    "        \n",
    "        # Analyze results\n",
    "        final_train_score = train_scores_mean[-1]\n",
    "        final_val_score = val_scores_mean[-1]\n",
    "        final_gap = overfitting_gap[-1]\n",
    "        \n",
    "        # Performance recommendations\n",
    "        recommendations = []\n",
    "        if final_gap > 0.1:\n",
    "            recommendations.append(\"High overfitting detected - consider regularization or more data\")\n",
    "        elif final_gap < 0.02:\n",
    "            recommendations.append(\"Good bias-variance balance achieved\")\n",
    "        \n",
    "        if val_scores_mean[-1] > val_scores_mean[-2]:\n",
    "            recommendations.append(\"Model still improving - could benefit from more training data\")\n",
    "        \n",
    "        print(f\"Learning Curve Analysis Results:\")\n",
    "        print(f\"  Final training score: {final_train_score:.4f}\")\n",
    "        print(f\"  Final validation score: {final_val_score:.4f}\")\n",
    "        print(f\"  Overfitting gap: {final_gap:.4f}\")\n",
    "        print(f\"  Recommendations: {'; '.join(recommendations) if recommendations else 'Model performance looks good'}\")\n",
    "        \n",
    "        print(f\"âœ“ Learning curves saved to: {learning_plot_file}\")\n",
    "        print(f\"âœ“ Learning data saved to: {learning_file}\")\n",
    "        \n",
    "        return {\n",
    "            'computation_time': computation_time,\n",
    "            'final_train_score': float(final_train_score),\n",
    "            'final_val_score': float(final_val_score),\n",
    "            'overfitting_gap': float(final_gap),\n",
    "            'recommendations': recommendations,\n",
    "            'learning_data': learning_data.to_dict('records'),\n",
    "            'plot_file': learning_plot_file,\n",
    "            'data_file': learning_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Learning curve analysis failed: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def analyze_validation_curves(pipeline: Pipeline, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                            model_name: str, results_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate validation curves for key hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Model pipeline\n",
    "        X_train, y_train: Training data\n",
    "        model_name: Name of the model\n",
    "        results_dir: Directory for saving results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing validation curve results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Validation Curve Analysis for {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Set up cross-validation\n",
    "        temporal_cols = ['YEAR', 'MONTH', 'DAY', 'HOUR']\n",
    "        if all(col in X_train.columns for col in temporal_cols):\n",
    "            cv_strategy = TimeSeriesSplit(n_splits=3)\n",
    "        else:\n",
    "            cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Define validation parameters based on model type\n",
    "        validation_params = {}\n",
    "        \n",
    "        if 'RandomForest' in model_name:\n",
    "            validation_params = {\n",
    "                'feature_pipeline__classifier__n_estimators': [10, 25, 50, 100, 200, 300],\n",
    "                'feature_pipeline__classifier__max_depth': [3, 5, 10, 15, 20, None]\n",
    "            }\n",
    "        elif 'GradientBoosting' in model_name or 'XGB' in model_name or 'LightGBM' in model_name:\n",
    "            validation_params = {\n",
    "                'feature_pipeline__classifier__n_estimators': [10, 25, 50, 100, 200, 300],\n",
    "                'feature_pipeline__classifier__learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "            }\n",
    "        elif 'LogisticRegression' in model_name:\n",
    "            validation_params = {\n",
    "                'feature_pipeline__classifier__C': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "            }\n",
    "        \n",
    "        if not validation_params:\n",
    "            print(\"âš  No validation parameters defined for this model type\")\n",
    "            return {'validation_available': False}\n",
    "        \n",
    "        print(f\"Analyzing {len(validation_params)} hyperparameters...\")\n",
    "        \n",
    "        validation_results = {}\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(validation_params), figsize=(6*len(validation_params), 6))\n",
    "        if len(validation_params) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (param_name, param_range) in enumerate(validation_params.items()):\n",
    "            print(f\"  Computing validation curve for {param_name}...\")\n",
    "            \n",
    "            try:\n",
    "                train_scores, val_scores = validation_curve(\n",
    "                    estimator=pipeline,\n",
    "                    X=X_train,\n",
    "                    y=y_train,\n",
    "                    param_name=param_name,\n",
    "                    param_range=param_range,\n",
    "                    cv=cv_strategy,\n",
    "                    scoring='f1',\n",
    "                    n_jobs=1\n",
    "                )\n",
    "                \n",
    "                train_mean = np.mean(train_scores, axis=1)\n",
    "                train_std = np.std(train_scores, axis=1)\n",
    "                val_mean = np.mean(val_scores, axis=1)\n",
    "                val_std = np.std(val_scores, axis=1)\n",
    "                \n",
    "                # Plot validation curve\n",
    "                axes[i].plot(param_range, train_mean, 'o-', color='blue', label='Training Score')\n",
    "                axes[i].fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "                \n",
    "                axes[i].plot(param_range, val_mean, 'o-', color='red', label='Validation Score')\n",
    "                axes[i].fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "                \n",
    "                axes[i].set_xlabel(param_name.split('__')[-1])\n",
    "                axes[i].set_ylabel('F1 Score')\n",
    "                axes[i].set_title(f'Validation Curve: {param_name.split(\"__\")[-1]}')\n",
    "                axes[i].legend()\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Set log scale if parameter values span orders of magnitude\n",
    "                if param_name.endswith('C') or param_name.endswith('learning_rate'):\n",
    "                    axes[i].set_xscale('log')\n",
    "                \n",
    "                # Store results\n",
    "                validation_results[param_name] = {\n",
    "                    'param_range': param_range,\n",
    "                    'train_scores_mean': train_mean.tolist(),\n",
    "                    'train_scores_std': train_std.tolist(),\n",
    "                    'val_scores_mean': val_mean.tolist(),\n",
    "                    'val_scores_std': val_std.tolist(),\n",
    "                    'best_param': param_range[np.argmax(val_mean)],\n",
    "                    'best_score': float(np.max(val_mean))\n",
    "                }\n",
    "                \n",
    "                print(f\"    Best {param_name.split('__')[-1]}: {validation_results[param_name]['best_param']} (F1: {validation_results[param_name]['best_score']:.4f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ Failed to compute validation curve for {param_name}: {e}\")\n",
    "                axes[i].text(0.5, 0.5, f'Error:\\n{str(e)[:50]}...', ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].set_title(f'Validation Curve: {param_name.split(\"__\")[-1]} (Error)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        validation_plot_file = os.path.join(results_dir, f\"{model_name}_validation_curves.png\")\n",
    "        plt.savefig(validation_plot_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Save validation results\n",
    "        validation_file = os.path.join(results_dir, f\"{model_name}_validation_curves.json\")\n",
    "        with open(validation_file, 'w') as f:\n",
    "            json.dump(validation_results, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ Validation curves saved to: {validation_plot_file}\")\n",
    "        print(f\"âœ“ Validation data saved to: {validation_file}\")\n",
    "        \n",
    "        return {\n",
    "            'validation_available': True,\n",
    "            'validation_results': validation_results,\n",
    "            'plot_file': validation_plot_file,\n",
    "            'data_file': validation_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Validation curve analysis failed: {e}\")\n",
    "        return {'validation_available': True, 'error': str(e)}\n",
    "\n",
    "# Perform learning curve analysis\n",
    "if optimized_pipeline is not None and best_model_name:\n",
    "    print(f\"\\nðŸ“ˆ Starting Learning Curve Analysis\")\n",
    "    \n",
    "    learning_results = analyze_learning_curves(\n",
    "        pipeline=optimized_pipeline,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        model_name=best_model_name,\n",
    "        results_dir=final_results_dir\n",
    "    )\n",
    "    \n",
    "    # Perform validation curve analysis\n",
    "    validation_results = analyze_validation_curves(\n",
    "        pipeline=optimized_pipeline,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        model_name=best_model_name,\n",
    "        results_dir=final_results_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Learning and validation curve analysis completed\")\n",
    "else:\n",
    "    print(\"âš  Cannot perform curve analysis - missing optimized pipeline\")\n",
    "    learning_results = {}\n",
    "    validation_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c7aa8",
   "metadata": {},
   "source": [
    "## Final Model Training and Test Set Evaluation\n",
    "\n",
    "Train the final production model and evaluate on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4145628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_production_model(pipeline: Pipeline, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                                X_test: pd.DataFrame, y_test: pd.Series, threshold: float,\n",
    "                                model_name: str, results_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train the final production model and evaluate on test set.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Optimized model pipeline\n",
    "        X_train, y_train: Training data\n",
    "        X_test, y_test: Test data\n",
    "        threshold: Optimized classification threshold\n",
    "        model_name: Name of the model\n",
    "        results_dir: Directory for saving results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing final model results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Final Production Model Training and Evaluation ===\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Classification Threshold: {threshold:.3f}\")\n",
    "    \n",
    "    try:\n",
    "        # Train on full training set\n",
    "        print(\"Training final model on complete training set...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"âœ“ Training completed in {training_time:.1f} seconds\")\n",
    "        \n",
    "        # Predictions on test set\n",
    "        print(\"Evaluating on test set...\")\n",
    "        y_test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "        y_test_pred_default = pipeline.predict(X_test) # Uses default 0.5 threshold from model's predict\n",
    "        y_test_pred_optimized = (y_test_proba >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        def calculate_metrics(y_true, y_pred, y_proba):\n",
    "            return {\n",
    "                'accuracy': accuracy_score(y_true, y_pred),\n",
    "                'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "                'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "                'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "                'roc_auc': roc_auc_score(y_true, y_proba),\n",
    "                'average_precision': average_precision_score(y_true, y_proba),\n",
    "                'matthews_corrcoef': matthews_corrcoef(y_true, y_pred)\n",
    "            }\n",
    "        \n",
    "        # Metrics with default threshold (0.5)\n",
    "        metrics_default = calculate_metrics(y_test, y_test_pred_default, y_test_proba)\n",
    "        \n",
    "        # Metrics with optimized threshold\n",
    "        metrics_optimized = calculate_metrics(y_test, y_test_pred_optimized, y_test_proba)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Test Set Performance:\")\n",
    "        print(f\"Default Threshold (0.5):\")\n",
    "        for metric, value in metrics_default.items():\n",
    "            print(f\"  {metric:20}: {value:.4f}\")\n",
    "        \n",
    "        print(f\"\\nOptimized Threshold ({threshold:.3f}):\")\n",
    "        for metric, value in metrics_optimized.items():\n",
    "            print(f\"  {metric:20}: {value:.4f}\")\n",
    "        \n",
    "        # Generate classification report\n",
    "        print(f\"\\nðŸ“‹ Detailed Classification Report (Optimized Threshold):\")\n",
    "        class_report_text = classification_report(y_test, y_test_pred_optimized)\n",
    "        print(class_report_text)\n",
    "        class_report_dict = classification_report(y_test, y_test_pred_optimized, output_dict=True)\n",
    "        \n",
    "        # Create comprehensive evaluation plots\n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        \n",
    "        # 1. Confusion Matrix (Default)\n",
    "        plt.subplot(3, 4, 1)\n",
    "        cm_default = confusion_matrix(y_test, y_test_pred_default)\n",
    "        ConfusionMatrixDisplay(cm_default, display_labels=['Low Risk', 'High Risk']).plot(ax=plt.gca())\n",
    "        plt.title(f'Confusion Matrix (Threshold: 0.5)')\n",
    "        \n",
    "        # 2. Confusion Matrix (Optimized)\n",
    "        plt.subplot(3, 4, 2)\n",
    "        cm_optimized = confusion_matrix(y_test, y_test_pred_optimized)\n",
    "        ConfusionMatrixDisplay(cm_optimized, display_labels=['Low Risk', 'High Risk']).plot(ax=plt.gca())\n",
    "        plt.title(f'Confusion Matrix (Threshold: {threshold:.3f})')\n",
    "        \n",
    "        # 3. ROC Curve\n",
    "        plt.subplot(3, 4, 3)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "        plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC: {metrics_optimized[\"roc_auc\"]:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Precision-Recall Curve\n",
    "        plt.subplot(3, 4, 4)\n",
    "        precision_curve_vals, recall_curve_vals, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "        plt.plot(recall_curve_vals, precision_curve_vals, linewidth=2, \n",
    "                label=f'PR Curve (AP: {metrics_optimized[\"average_precision\"]:.4f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Prediction Probability Distribution\n",
    "        plt.subplot(3, 4, 5)\n",
    "        plt.hist(y_test_proba[y_test == 0], bins=30, alpha=0.7, label='Low Risk', density=True)\n",
    "        plt.hist(y_test_proba[y_test == 1], bins=30, alpha=0.7, label='High Risk', density=True)\n",
    "        plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.3f}')\n",
    "        plt.xlabel('Prediction Probability')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Prediction Probability Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Feature Importance (if available)\n",
    "        plt.subplot(3, 4, 6)\n",
    "        try:\n",
    "            # Access feature importances from the classifier step of the feature_pipeline\n",
    "            final_classifier_step = pipeline.feature_pipeline[-1]\n",
    "            if hasattr(final_classifier_step, 'feature_importances_'):\n",
    "                importances = final_classifier_step.feature_importances_\n",
    "                # Get feature names from the step preceding the classifier in feature_pipeline\n",
    "                preprocessor_step = pipeline.feature_pipeline[:-1]\n",
    "                if hasattr(preprocessor_step, 'get_feature_names_out'):\n",
    "                    feature_names_final = preprocessor_step.get_feature_names_out()\n",
    "                else: # Fallback if get_feature_names_out is not available or X was numpy array\n",
    "                    feature_names_final = [f'feature_{i}' for i in range(len(importances))]\n",
    "                \n",
    "                # Top 15 features\n",
    "                indices = np.argsort(importances)[::-1][:15]\n",
    "                # Ensure we don't try to plot more features than available\n",
    "                num_features_to_plot = min(15, len(importances))\n",
    "                plt.barh(range(num_features_to_plot), importances[indices][:num_features_to_plot])\n",
    "                plt.yticks(range(num_features_to_plot), [feature_names_final[i] for i in indices][:num_features_to_plot])\n",
    "                plt.xlabel('Feature Importance')\n",
    "                plt.title('Top Feature Importances')\n",
    "                plt.gca().invert_yaxis()\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Feature importance\\nnot available for this classifier', \n",
    "                        ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                plt.title('Feature Importance (N/A)')\n",
    "        except Exception as e:\n",
    "            plt.text(0.5, 0.5, f'Feature importance\\nerror: {str(e)[:30]}...', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Feature Importance (Error)')\n",
    "        \n",
    "        # 7. Metrics Comparison\n",
    "        plt.subplot(3, 4, 7)\n",
    "        metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1', 'MCC']\n",
    "        default_values = [metrics_default['accuracy'], metrics_default['precision'], \n",
    "                         metrics_default['recall'], metrics_default['f1'], \n",
    "                         metrics_default['matthews_corrcoef']]\n",
    "        optimized_values = [metrics_optimized['accuracy'], metrics_optimized['precision'], \n",
    "                           metrics_optimized['recall'], metrics_optimized['f1'], \n",
    "                           metrics_optimized['matthews_corrcoef']]\n",
    "        \n",
    "        x_bar = np.arange(len(metrics_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x_bar - width/2, default_values, width, label='Default (0.5)', alpha=0.7)\n",
    "        plt.bar(x_bar + width/2, optimized_values, width, label=f'Optimized ({threshold:.3f})', alpha=0.7)\n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Threshold Comparison')\n",
    "        plt.xticks(x_bar, metrics_names)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 8. Residual Analysis (for probabilities)\n",
    "        plt.subplot(3, 4, 8)\n",
    "        residuals = y_test - y_test_proba\n",
    "        plt.scatter(y_test_proba, residuals, alpha=0.6)\n",
    "        plt.axhline(y=0, color='red', linestyle='--')\n",
    "        plt.xlabel('Predicted Probability')\n",
    "        plt.ylabel('Residuals (Actual - Predicted)')\n",
    "        plt.title('Residual Analysis')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 9-12. Performance by class and other analyses\n",
    "        plt.subplot(3, 4, 9)\n",
    "        class_performance_data = []\n",
    "        for label, metrics in class_report_dict.items():\n",
    "            if label in ['0', '1']:\n",
    "                class_performance_data.append({\n",
    "                    'Class': f\"{'Low Risk' if label == '0' else 'High Risk'} ({label})\",\n",
    "                    'Precision': metrics['precision'],\n",
    "                    'Recall': metrics['recall'],\n",
    "                    'F1-Score': metrics['f1-score']\n",
    "                })\n",
    "        class_performance_df = pd.DataFrame(class_performance_data)\n",
    "        \n",
    "        x_bar_class = np.arange(len(class_performance_df))\n",
    "        width_class = 0.25\n",
    "        \n",
    "        plt.bar(x_bar_class - width_class, class_performance_df['Precision'], width_class, label='Precision', alpha=0.7)\n",
    "        plt.bar(x_bar_class, class_performance_df['Recall'], width_class, label='Recall', alpha=0.7)\n",
    "        plt.bar(x_bar_class + width_class, class_performance_df['F1-Score'], width_class, label='F1-Score', alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Per-Class Performance (Optimized)')\n",
    "        plt.xticks(x_bar_class, class_performance_df['Class'])\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 10. Error Analysis\n",
    "        plt.subplot(3, 4, 10)\n",
    "        errors = {\n",
    "            'True Positives': int(cm_optimized[1, 1]),\n",
    "            'False Positives': int(cm_optimized[0, 1]),\n",
    "            'False Negatives': int(cm_optimized[1, 0]),\n",
    "            'True Negatives': int(cm_optimized[0, 0])\n",
    "        }\n",
    "        \n",
    "        plt.pie(list(errors.values()), labels=list(errors.keys()), autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Prediction Breakdown (Optimized)')\n",
    "        \n",
    "        # 11. Calibration Plot\n",
    "        plt.subplot(3, 4, 11)\n",
    "        from sklearn.calibration import calibration_curve\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_proba, n_bins=10, strategy='uniform')\n",
    "        plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=f\"{model_name}\")\n",
    "        plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "        plt.xlabel('Mean Predicted Probability')\n",
    "        plt.ylabel('Fraction of Positives')\n",
    "        plt.title('Calibration Plot')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 12. Summary Statistics\n",
    "        plt.subplot(3, 4, 12)\n",
    "        summary_stats = {\n",
    "            'Total Test Samples': len(y_test),\n",
    "            'Positive Samples': int(y_test.sum()),\n",
    "            'Negative Samples': int(len(y_test) - y_test.sum()),\n",
    "            'Positive Rate': f\"{y_test.mean():.3f}\",\n",
    "            'Training Time': f\"{training_time:.1f}s\"\n",
    "        }\n",
    "        \n",
    "        plt.axis('off')\n",
    "        text_str = '\\n'.join([f'{k}: {v}' for k, v in summary_stats.items()])\n",
    "        plt.text(0.05, 0.5, text_str, transform=plt.gca().transAxes, fontsize=10,\n",
    "                verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.7))\n",
    "        plt.title('Summary Statistics (Test Set)')\n",
    "        \n",
    "        plt.tight_layout(pad=2.0)\n",
    "        \n",
    "        # Save comprehensive evaluation plot\n",
    "        final_eval_plot = os.path.join(results_dir, f\"{model_name}_final_evaluation.png\")\n",
    "        plt.savefig(final_eval_plot, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Save final model\n",
    "        final_model_file = os.path.join(results_dir, f\"{model_name}_final_model.joblib\")\n",
    "        joblib.dump(pipeline, final_model_file)\n",
    "        \n",
    "        # Save final results\n",
    "        final_results_data = {\n",
    "            'model_name': model_name,\n",
    "            'training_time_seconds': training_time,\n",
    "            'test_samples_count': len(y_test),\n",
    "            'positive_samples_test_count': int(y_test.sum()),\n",
    "            'default_threshold_metrics': {\n",
    "                'threshold': 0.5,\n",
    "                'metrics': metrics_default\n",
    "            },\n",
    "            'optimized_threshold_metrics': {\n",
    "                'threshold': float(threshold),\n",
    "                'metrics': metrics_optimized\n",
    "            },\n",
    "            'classification_report_optimized': class_report_dict,\n",
    "            'confusion_matrix_default_tn_fp_fn_tp': cm_default.tolist(),\n",
    "            'confusion_matrix_optimized_tn_fp_fn_tp': cm_optimized.tolist(),\n",
    "            'summary_statistics_test_set': summary_stats,\n",
    "            'plot_file_final_evaluation': final_eval_plot,\n",
    "            'model_file_final': final_model_file,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        final_results_file = os.path.join(results_dir, f\"{model_name}_final_results.json\")\n",
    "        with open(final_results_file, 'w') as f:\n",
    "            json.dump(final_results_data, f, indent=2, default=str) # Use default=str for non-serializable like numpy floats\n",
    "        \n",
    "        print(f\"\\nâœ… Final Model Training and Evaluation Complete!\")\n",
    "        print(f\"âœ“ Model saved to: {final_model_file}\")\n",
    "        print(f\"âœ“ Evaluation plot saved to: {final_eval_plot}\")\n",
    "        print(f\"âœ“ Results saved to: {final_results_file}\")\n",
    "        \n",
    "        return final_results_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Final model training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Train and evaluate final production model\n",
    "if optimized_pipeline is not None and best_model_name:\n",
    "    print(f\"\\nðŸ† Starting Final Production Model Training\")\n",
    "    \n",
    "    # Ensure optimal_threshold is available from the threshold optimization step\n",
    "    # If threshold_results contains an error or optimal_threshold is not set, use a default (e.g., 0.5)\n",
    "    current_optimal_threshold = threshold_results.get('optimal_threshold', 0.5) if isinstance(threshold_results, dict) else 0.5\n",
    "    if 'error' in threshold_results:\n",
    "        print(f\"âš  Using default threshold 0.5 due to error in threshold optimization: {threshold_results['error']}\")\n",
    "        current_optimal_threshold = 0.5\n",
    "    elif not threshold_results: # If threshold_results is empty\n",
    "        print(f\"âš  Using default threshold 0.5 as threshold optimization results are empty.\")\n",
    "        current_optimal_threshold = 0.5\n",
    "\n",
    "    final_results = train_final_production_model(\n",
    "        pipeline=optimized_pipeline,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        threshold=current_optimal_threshold, # Corrected: use optimal_threshold (or a safe default)\n",
    "        model_name=best_model_name,\n",
    "        results_dir=final_results_dir\n",
    "    )\n",
    "    \n",
    "    if 'error' not in final_results and final_results:\n",
    "        final_f1_score = final_results.get('optimized_threshold_metrics', {}).get('metrics', {}).get('f1', 0)\n",
    "        final_precision = final_results.get('optimized_threshold_metrics', {}).get('metrics', {}).get('precision', 0)\n",
    "        final_recall = final_results.get('optimized_threshold_metrics', {}).get('metrics', {}).get('recall', 0)\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Final Production Model Performance (Threshold: {current_optimal_threshold:.3f}):\")\n",
    "        print(f\"   F1-Score: {final_f1_score:.4f}\")\n",
    "        print(f\"   Precision: {final_precision:.4f}\")\n",
    "        print(f\"   Recall: {final_recall:.4f}\")\n",
    "    elif not final_results:\n",
    "        print(f\"\\nâš  Final model training did not produce results.\")\n",
    "    else: # error in final_results\n",
    "        print(f\"\\nâš  Final model training encountered issues: {final_results.get('error')}\")\n",
    "        # final_results might be {'error': '...'}, so accessing metrics would fail\n",
    "else:\n",
    "    print(\"âš  Cannot train final model - missing optimized pipeline or best_model_name\")\n",
    "    final_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8b2ad",
   "metadata": {},
   "source": [
    "# Comprehensive Final Report\n",
    "\n",
    "This section generates a comprehensive final report summarizing all phases of the DMML project and provides recommendations for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a1eff",
   "metadata": {},
   "source": [
    "## Generate Comprehensive Final Report\n",
    "\n",
    "Create a complete summary of the entire DMML project with recommendations and deployment guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8973a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_final_report(artifacts: Dict[str, Any], optimization_results: Dict[str, Any],\n",
    "                                      shap_results: Dict[str, Any], perm_results: Dict[str, Any],\n",
    "                                      threshold_results: Dict[str, Any], learning_results: Dict[str, Any],\n",
    "                                      validation_results: Dict[str, Any], final_results: Dict[str, Any],\n",
    "                                      model_name: str, results_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive final report for the entire DMML project.\n",
    "    \n",
    "    Args:\n",
    "        artifacts: Loaded artifacts from previous phases\n",
    "        optimization_results: Bayesian optimization results\n",
    "        shap_results: SHAP analysis results\n",
    "        perm_results: Permutation importance results\n",
    "        threshold_results: Threshold optimization results\n",
    "        learning_results: Learning curve results\n",
    "        validation_results: Validation curve results\n",
    "        final_results: Final model evaluation results\n",
    "        model_name: Name of the best model\n",
    "        results_dir: Directory for saving results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the comprehensive report\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Generating Comprehensive Final Report ===\")\n",
    "    \n",
    "    try:\n",
    "        # Compile comprehensive report\n",
    "        report = {\n",
    "            'project_overview': {\n",
    "                'title': 'DMML Binary Risk Classification Project - Final Report',\n",
    "                'objective': 'Develop a robust binary classification model for risk assessment using temporal-spatial data',\n",
    "                'methodology': 'End-to-end machine learning pipeline with custom transformers, comprehensive evaluation, and advanced optimization',\n",
    "                'best_model': model_name,\n",
    "                'completion_date': datetime.now().isoformat(),\n",
    "                'total_phases': 4\n",
    "            },\n",
    "            \n",
    "            'data_summary': {\n",
    "                'training_samples': len(artifacts.get('X_train', [])),\n",
    "                'test_samples': len(artifacts.get('X_test', [])),\n",
    "                'total_features': len(artifacts.get('feature_names', [])),\n",
    "                'class_distribution': {\n",
    "                    'positive_class_rate': float(artifacts.get('y_train', pd.Series()).mean()) if 'y_train' in artifacts else 0.0,\n",
    "                    'balanced': 'No' if float(artifacts.get('y_train', pd.Series()).mean()) < 0.4 or float(artifacts.get('y_train', pd.Series()).mean()) > 0.6 else 'Yes'\n",
    "                },\n",
    "                'temporal_features': ['YEAR', 'MONTH', 'DAY', 'HOUR'],\n",
    "                'spatial_features': ['Latitude', 'Longitude']\n",
    "            },\n",
    "            \n",
    "            'preprocessing_summary': {\n",
    "                'stkde_parameters': {\n",
    "                    'hs_optimal': artifacts.get('hs_optimal', 0),\n",
    "                    'ht_optimal': artifacts.get('ht_optimal', 0)\n",
    "                },\n",
    "                'feature_engineering': ['STKDE intensity calculation', 'Cyclical encoding', 'Risk label transformation'],\n",
    "                'preprocessing_pipelines': ['General preprocessing', 'Tree-based preprocessing'],\n",
    "                'data_leakage_prevention': 'Temporal train/test split implemented'\n",
    "            },\n",
    "            \n",
    "            'model_selection_summary': {\n",
    "                'models_evaluated': len(artifacts.get('model_results', pd.DataFrame())),\n",
    "                'best_model': model_name,\n",
    "                'selection_metric': 'F1-Score',\n",
    "                'cross_validation': 'TimeSeriesSplit or StratifiedKFold',\n",
    "                'best_cv_score': artifacts.get('best_model_score', 0.0)\n",
    "            },\n",
    "            \n",
    "            'optimization_summary': optimization_results if optimization_results else {\n",
    "                'method': 'Not performed',\n",
    "                'reason': 'Bayesian optimization not available or failed'\n",
    "            },\n",
    "            \n",
    "            'interpretability_summary': {\n",
    "                'shap_analysis': shap_results.get('shap_available', False),\n",
    "                'permutation_importance': perm_results.get('permutation_available', False),\n",
    "                'feature_importance_available': shap_results.get('shap_available', False) or perm_results.get('permutation_available', False)\n",
    "            },\n",
    "            \n",
    "            'threshold_optimization_summary': {\n",
    "                'performed': 'error' not in threshold_results,\n",
    "                'recommended_threshold': threshold_results.get('recommended_threshold', 0.5),\n",
    "                'expected_improvement': threshold_results.get('recommended_f1', 0.0) - 0.5 if 'recommended_f1' in threshold_results else 0.0\n",
    "            },\n",
    "            \n",
    "            'learning_analysis_summary': {\n",
    "                'learning_curves': 'error' not in learning_results,\n",
    "                'validation_curves': validation_results.get('validation_available', False),\n",
    "                'overfitting_detected': learning_results.get('overfitting_gap', 0.0) > 0.1,\n",
    "                'recommendations': learning_results.get('recommendations', [])\n",
    "            },\n",
    "            \n",
    "            'final_performance': final_results.get('optimized_threshold', {}).get('metrics', {}) if final_results else {},\n",
    "            \n",
    "            'model_artifacts': {\n",
    "                'final_model_file': f\"{model_name}_final_model.joblib\",\n",
    "                'preprocessing_artifacts': ['preprocessor_general.joblib', 'preprocessor_trees.joblib'],\n",
    "                'stkde_parameters': 'preprocessing_metadata.json',\n",
    "                'feature_names': 'feature_names.joblib'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = []\n",
    "        \n",
    "        # 1. Deployment Recommendations\n",
    "        recommendations.append(\"Deploy the model using the saved artifacts and ensure the runtime environment matches the development environment.\")\n",
    "        recommendations.append(\"Monitor model performance and retrain with new data if performance degrades.\")\n",
    "        \n",
    "        # 2. Data Management Recommendations\n",
    "        if artifacts.get('y_train') is not None and len(artifacts['y_train']) > 0:\n",
    "            positive_rate = artifacts['y_train'].mean()\n",
    "            if positive_rate < 0.1:\n",
    "                recommendations.append(\"Consider using anomaly detection methods due to low positive class rate.\")\n",
    "            elif positive_rate > 0.9:\n",
    "                recommendations.append(\"Consider using specialized methods for imbalanced classes.\")\n",
    "        \n",
    "        # 3. Feature Engineering Recommendations\n",
    "        if artifacts.get('hs_optimal') and artifacts.get('ht_optimal'):\n",
    "            recommendations.append(f\"HS and HT parameters for STKDE are set to optimal values: HS={artifacts['hs_optimal']}, HT={artifacts['ht_optimal']}.\")\n",
    "        else:\n",
    "            recommendations.append(\"Review STKDE parameters (HS, HT) for potential optimization.\")\n",
    "        \n",
    "        # 4. Model Monitoring Recommendations\n",
    "        recommendations.append(\"Implement monitoring for model drift and data quality.\")\n",
    "        recommendations.append(\"Schedule regular intervals for model evaluation and retraining.\")\n",
    "        \n",
    "        # 5. Performance Optimization Recommendations\n",
    "        if final_results.get('optimized_threshold', {}).get('metrics', {}).get('f1', 0) < 0.7:\n",
    "            recommendations.append(\"F1 score is below 0.7, consider further tuning or using a different model.\")\n",
    "        if final_results.get('optimized_threshold', {}).get('metrics', {}).get('roc_auc', 0) < 0.7:\n",
    "            recommendations.append(\"ROC AUC score is below 0.7, consider improving feature engineering or model selection.\")\n",
    "        \n",
    "        report['recommendations'] = recommendations\n",
    "        \n",
    "        # Save comprehensive report to file\n",
    "        report_file = os.path.join(results_dir, \"comprehensive_final_report.json\")\n",
    "        with open(report_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"âœ“ Comprehensive final report saved to: {report_file}\")\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to generate comprehensive report: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Generate comprehensive final report\n",
    "if best_model_name and final_results:\n",
    "    print(f\"\\nðŸ“Š Generating Comprehensive Final Report\")\n",
    "    \n",
    "    comprehensive_report = generate_comprehensive_final_report(\n",
    "        artifacts=artifacts,\n",
    "        optimization_results=optimization_results,\n",
    "        shap_results=shap_results,\n",
    "        perm_results=perm_results,\n",
    "        threshold_results=threshold_results,\n",
    "        learning_results=learning_results,\n",
    "        validation_results=validation_results,\n",
    "        final_results=final_results,\n",
    "        model_name=best_model_name,\n",
    "        results_dir=final_results_dir\n",
    "    )\n",
    "    \n",
    "    if 'error' not in comprehensive_report:\n",
    "        print(f\"\\nðŸ† DMML Project Completion Summary:\")\n",
    "        print(f\"   âœ… All 4 phases completed successfully\")\n",
    "        print(f\"   âœ… Best model: {best_model_name}\")\n",
    "        print(f\"   âœ… Final performance achieved\")\n",
    "        print(f\"   âœ… Model artifacts saved and ready for deployment\")\n",
    "        print(f\"   âœ… Comprehensive documentation generated\")\n",
    "    else:\n",
    "        print(f\"\\nâš  Report generation encountered issues\")\n",
    "else:\n",
    "    print(\"âš  Cannot generate final report - missing required results\")\n",
    "    comprehensive_report = {}\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Phase 4: Advanced Tuning and Final Training - COMPLETED!\")\n",
    "print(f\"ðŸŽ‰ DMML Binary Risk Classification Project - FULLY COMPLETED!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
