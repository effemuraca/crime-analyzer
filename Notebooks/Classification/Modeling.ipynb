{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0890abf3",
   "metadata": {
    "id": "0890abf3"
   },
   "source": [
    "# Modeling Phase\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook outlines the modeling phase, utilizing preprocessed data and pipelines from `AlternativePreprocessing.ipynb`. All feature engineering and label engineering, including the generation of `RISK_LEVEL` via Spatio-Temporal Kernel Density Estimation (STKDE), are conducted in a manner that prevents data leakage. This is achieved by performing these operations, including the determination of STKDE intensity thresholds using Jenks natural breaks, only after the initial train/test split and subsequently within each cross-validation fold. The classification task is to distinguish between two risk categories: \"LOW RISK\" and \"HIGH RISK\".\n",
    "\n",
    "**Dependencies:**\n",
    "- Artifacts from `Classification (Preprocessing)` (including data, pipelines, STKDE parameters, and the `scoring_dict`).\n",
    "- Custom transformers and modular pipelines defined in `custom_transformers.py`.\n",
    "\n",
    "**Objectives:**\n",
    "- Select the optimal model for 2-class risk classification through rigorous cross-validation.\n",
    "- Perform hyperparameter tuning for the selected model.\n",
    "- Evaluate the model\\'s generalization performance on the unseen test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276296db",
   "metadata": {
    "id": "276296db"
   },
   "source": [
    "# Setup\n",
    "\n",
    "This section handles the initial setup, including importing necessary libraries, defining file paths, and loading the preprocessed data. All custom functions are imported from `custom_transformers.py` to maintain modularity and facilitate code reuse for the 2-class risk classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f49aec",
   "metadata": {
    "id": "b7f49aec"
   },
   "source": [
    "## Optional: Run on Google Colab\n",
    "\n",
    "This cell is for users working in a Google Colab environment. If you are running this notebook locally, you can safely ignore this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508ec24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f508ec24",
    "outputId": "95f6ff87-3c99-4ae0-81e1-1baba0113116"
   },
   "outputs": [],
   "source": [
    "# Run on Google Colab (optional)\n",
    "#from google.colab import drive\n",
    "#drive.mount('/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c631771",
   "metadata": {
    "id": "1c631771"
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all libraries required for the modeling process and for generating visualizations. Custom functions, critical for specific transformations and operations for the 2-class problem, are imported from the `custom_transformers.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd20df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6fd20df",
    "outputId": "7ef36419-8505-494c-d3e0-523c50f67275"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, RandomizedSearchCV, GroupKFold, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (make_scorer, f1_score, accuracy_score, precision_score, recall_score,\n",
    "                               confusion_matrix, ConfusionMatrixDisplay, roc_auc_score,\n",
    "                               average_precision_score, matthews_corrcoef, classification_report)\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from IPython.display import display\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import EasyEnsembleClassifier, BalancedRandomForestClassifier, BalancedBaggingClassifier\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "project_path = os.getcwd()\n",
    "os.chdir(project_path)\n",
    "sys.path.append(project_path)\n",
    "\n",
    "# Import custom transformers\n",
    "try:\n",
    "    from Utilities.custom_transformers import STKDEAndRiskLabelTransformer, CustomModelPipeline \n",
    "except ImportError as e:\n",
    "    print(f\"Error importing custom transformers: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    XGBClassifier = None\n",
    "    print(\"XGBoost not installed. Skipping.\")\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    LGBMClassifier = None\n",
    "    print(\"LightGBM not installed. Skipping.\")\n",
    "\n",
    "try:\n",
    "    import jenkspy\n",
    "except ImportError:\n",
    "    print(\"jenkspy not installed. Please install it using 'pip install jenkspy'.\")\n",
    "    jenkspy = None\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "import random\n",
    "\n",
    "# Set seeds and styles\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "#warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5223af",
   "metadata": {
    "id": "2c5223af"
   },
   "source": [
    "## Define Paths\n",
    "\n",
    "Establish and define the directory paths for loading preprocessed data and for saving the results of the modeling phase, such as trained models, performance metrics, and generated plots for the 2-class problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c38cc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1c38cc3",
    "outputId": "e3e48060-2654-4ee7-deb5-7ab88beb1964"
   },
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "\n",
    "preprocessing_dir = os.path.join(base_dir, \"Classification (Preprocessing)\")\n",
    "modeling_results_dir = os.path.join(base_dir, \"Classification (Modeling)\")\n",
    "os.makedirs(modeling_results_dir, exist_ok=True)\n",
    "print(f\"Preprocessing artifacts will be loaded from: {preprocessing_dir}\")\n",
    "print(f\"Modeling results will be saved to: {modeling_results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1d59",
   "metadata": {
    "id": "aa0b1d59"
   },
   "source": [
    "## Load Data and Pipelines\n",
    "\n",
    "Load the unprocessed data splits (`X_train`, `y_train`, `X_test`, `y_test`), the unfitted preprocessing pipelines, the list of feature names, and the scoring dictionary. These artifacts were previously generated and saved by the `AlternativePreprocessing.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OrQEsN0WRIMQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrQEsN0WRIMQ",
    "outputId": "44ba9c8c-2bcb-40b0-9845-979b8403de0c"
   },
   "outputs": [],
   "source": [
    "print(\"=== Loading data and pipelines ===\")\n",
    "\n",
    "# (Keep loading the full pipelines for legacy/compat, but do NOT use them as steps)\n",
    "preprocessing_pipeline_general = joblib.load(os.path.join(preprocessing_dir, 'preprocessing_pipeline_general.joblib'))\n",
    "preprocessing_pipeline_trees = joblib.load(os.path.join(preprocessing_dir, 'preprocessing_pipeline_trees.joblib'))\n",
    "\n",
    "print(f\"Loaded preprocessing pipelines: general, trees\")\n",
    "\n",
    "def load_preprocessing_artifacts(preprocessing_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load all preprocessing artifacts with comprehensive validation.\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_dir: Directory containing preprocessing artifacts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing loaded artifacts\n",
    "    \"\"\"\n",
    "    print(\"=== Loading Preprocessing Artifacts ===\")\n",
    "    \n",
    "    if not os.path.exists(preprocessing_dir):\n",
    "        raise FileNotFoundError(f\"Preprocessing directory not found: {preprocessing_dir}\")\n",
    "    \n",
    "    artifacts = {}\n",
    "    \n",
    "    try:\n",
    "        # Load raw data\n",
    "        X_train_path = os.path.join(preprocessing_dir, 'X_train.npy')\n",
    "        X_test_path = os.path.join(preprocessing_dir, 'X_test.npy')\n",
    "        y_train_path = os.path.join(preprocessing_dir, 'y_train.npy')\n",
    "        y_test_path = os.path.join(preprocessing_dir, 'y_test.npy')\n",
    "        \n",
    "        if not all(os.path.exists(p) for p in [X_train_path, X_test_path, y_train_path, y_test_path]):\n",
    "            raise FileNotFoundError(\"One or more data files missing\")\n",
    "        \n",
    "        X_train_raw = np.load(X_train_path, allow_pickle=True)\n",
    "        X_test_raw = np.load(X_test_path, allow_pickle=True)\n",
    "        y_train_raw = np.load(y_train_path, allow_pickle=True)\n",
    "        y_test_raw = np.load(y_test_path, allow_pickle=True)\n",
    "        \n",
    "        # Load feature names\n",
    "        feature_names_path = os.path.join(preprocessing_dir, 'feature_names.joblib')\n",
    "        if os.path.exists(feature_names_path):\n",
    "            feature_names = joblib.load(feature_names_path)\n",
    "        else:\n",
    "            # Fallback to legacy name\n",
    "            legacy_path = os.path.join(preprocessing_dir, 'X_feature_names.joblib')\n",
    "            if os.path.exists(legacy_path):\n",
    "                feature_names = joblib.load(legacy_path)\n",
    "            else:\n",
    "                raise FileNotFoundError(\"Feature names file not found\")\n",
    "        \n",
    "        # Convert to DataFrames with proper validation\n",
    "        if X_train_raw.ndim != 2 or X_test_raw.ndim != 2:\n",
    "            raise ValueError(\"Training or test data has incorrect dimensions\")\n",
    "        \n",
    "        if len(feature_names) != X_train_raw.shape[1]:\n",
    "            raise ValueError(f\"Feature names length ({len(feature_names)}) != data columns ({X_train_raw.shape[1]})\")\n",
    "        \n",
    "        X_train = pd.DataFrame(X_train_raw, columns=feature_names)\n",
    "        X_test = pd.DataFrame(X_test_raw, columns=feature_names)\n",
    "        y_train = pd.Series(y_train_raw, name='DUMMY_TARGET')\n",
    "        y_test = pd.Series(y_test_raw, name='DUMMY_TARGET')\n",
    "        \n",
    "        artifacts.update({\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'feature_names': feature_names\n",
    "        })\n",
    "        \n",
    "        print(f\"✓ Data loaded: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
    "        \n",
    "        # Load preprocessing pipelines\n",
    "        pipeline_files = {\n",
    "            # Full pipelines (might be used for reference or specific scenarios)\n",
    "            'preprocessing_pipeline_general_with_pca': 'preprocessing_pipeline_general.joblib', # CT + PCA\n",
    "            'preprocessing_pipeline_trees_ct_only': 'preprocessing_pipeline_trees.joblib',   # Just CT for trees\n",
    "\n",
    "            # ColumnTransformers (these are the primary components for building new pipelines)\n",
    "            'preprocessor_general_ct': 'preprocessor_full.joblib', \n",
    "            'preprocessor_trees_ct': 'preprocessor_trees.joblib'\n",
    "        }\n",
    "        \n",
    "        for artifact_name, filename in pipeline_files.items():\n",
    "            file_path = os.path.join(preprocessing_dir, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    artifacts[artifact_name] = joblib.load(file_path)\n",
    "                    print(f\"✓ Loaded {artifact_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠ Failed to load {artifact_name}: {e}\")\n",
    "                    artifacts[artifact_name] = None\n",
    "            else:\n",
    "                print(f\"⚠ {artifact_name} not found\")\n",
    "                artifacts[artifact_name] = None\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(preprocessing_dir, 'preprocessing_metadata.json')\n",
    "        if os.path.exists(metadata_path):\n",
    "            try:\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                artifacts['metadata'] = metadata\n",
    "                \n",
    "                # Extract STKDE parameters\n",
    "                stkde_params = metadata.get('stkde_parameters', {})\n",
    "                artifacts['hs_optimal'] = stkde_params.get('hs_optimal', 200.0)\n",
    "                artifacts['ht_optimal'] = stkde_params.get('ht_optimal', 60.0)\n",
    "                \n",
    "                print(f\"✓ Loaded metadata with STKDE params: hs={artifacts['hs_optimal']}, ht={artifacts['ht_optimal']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Failed to load metadata: {e}\")\n",
    "                artifacts['metadata'] = None\n",
    "                artifacts['hs_optimal'] = 200.0\n",
    "                artifacts['ht_optimal'] = 60.0\n",
    "        else:\n",
    "            print(\"⚠ Metadata not found, using default STKDE parameters\")\n",
    "            artifacts['hs_optimal'] = 200.0\n",
    "            artifacts['ht_optimal'] = 60.0\n",
    "            \n",
    "        # Load legacy STKDE parameters if metadata not available\n",
    "        if artifacts['hs_optimal'] == 200.0 and artifacts['ht_optimal'] == 60.0:\n",
    "            legacy_stkde_path = os.path.join(preprocessing_dir, 'stkde_optimal_params.json')\n",
    "            if os.path.exists(legacy_stkde_path):\n",
    "                try:\n",
    "                    with open(legacy_stkde_path, 'r') as f:\n",
    "                        legacy_params = json.load(f)\n",
    "                    artifacts['hs_optimal'] = legacy_params.get('hs_opt', 200.0)\n",
    "                    artifacts['ht_optimal'] = legacy_params.get('ht_opt', 60.0)\n",
    "                    print(f\"✓ Loaded legacy STKDE params: hs={artifacts['hs_optimal']}, ht={artifacts['ht_optimal']}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠ Failed to load legacy STKDE params: {e}\")\n",
    "        \n",
    "        return artifacts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading preprocessing artifacts: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_data_quality(artifacts: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Validate the quality and consistency of loaded data.\n",
    "    \n",
    "    Args:\n",
    "        artifacts: Dictionary containing loaded artifacts\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Data Quality Validation ===\")\n",
    "    \n",
    "    X_train, X_test = artifacts['X_train'], artifacts['X_test']\n",
    "    y_train, y_test = artifacts['y_train'], artifacts['y_test']\n",
    "    \n",
    "    # Basic shape validation\n",
    "    if X_train.empty or X_test.empty:\n",
    "        raise ValueError(\"Training or test data is empty\")\n",
    "    \n",
    "    # Column consistency\n",
    "    if not X_train.columns.equals(X_test.columns):\n",
    "        raise ValueError(\"Training and test data have different columns\")\n",
    "    \n",
    "    # Required columns for STKDE\n",
    "    required_cols = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'Latitude', 'Longitude']\n",
    "    missing_cols = [col for col in required_cols if col not in X_train.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Data type validation\n",
    "    for col in required_cols:\n",
    "        if X_train[col].dtype == 'object':\n",
    "            print(f\"⚠ Column '{col}' has object dtype, may need conversion\")\n",
    "    \n",
    "    # Missing value check\n",
    "    train_missing = X_train.isnull().sum().sum()\n",
    "    test_missing = X_test.isnull().sum().sum()\n",
    "    \n",
    "    if train_missing > 0:\n",
    "        print(f\"⚠ Training data has {train_missing} missing values\")\n",
    "    if test_missing > 0:\n",
    "        print(f\"⚠ Test data has {test_missing} missing values\")\n",
    "    \n",
    "    print(f\"✓ Data validation completed\")\n",
    "    print(f\"  Training: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"  Test: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "    print(f\"  Feature types: {X_train.dtypes.value_counts().to_dict()}\")\n",
    "    print(f\"  Feature names: {', '.join(X_train.columns)}\")\n",
    "\n",
    "# Load and validate data\n",
    "try:\n",
    "    artifacts = load_preprocessing_artifacts(preprocessing_dir)\n",
    "    validate_data_quality(artifacts)\n",
    "    \n",
    "    # Extract main variables\n",
    "    X_train = artifacts['X_train']\n",
    "    X_test = artifacts['X_test']\n",
    "    y_train = artifacts['y_train']\n",
    "    y_test = artifacts['y_test']\n",
    "    feature_names = artifacts['feature_names']\n",
    "    hs_optimal = artifacts['hs_optimal']\n",
    "    ht_optimal = artifacts['ht_optimal']\n",
    "    \n",
    "    # Extract preprocessing components\n",
    "    preprocessor_general_ct = artifacts.get('preprocessor_general_ct')\n",
    "    preprocessor_trees_ct = artifacts.get('preprocessor_trees_ct')\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load preprocessing artifacts: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548506e",
   "metadata": {
    "id": "3548506e"
   },
   "source": [
    "## Target Variable Selection\n",
    "\n",
    "The primary target variable for classification in this notebook is `RISK_LEVEL`. This variable is not directly present in the initial dataset but is *engineered* using STKDE to represent two classes: \"LOW RISK\" and \"HIGH RISK\". This engineering process is integrated into a leakage-free pipeline, ensuring that the target is generated based only on information available at each stage (e.g., within each CV fold using only training data for that fold)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_5TmHqaYlkfP",
   "metadata": {
    "id": "_5TmHqaYlkfP"
   },
   "source": [
    "## Update Scoring Dictionary for Binary Classification\n",
    "\n",
    "This section updates the `scoring` dictionary to include metrics suitable for binary classification tasks. Ordinal-specific metrics that are not applicable to a 2-class problem (like `severe_error_rate`) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HmC7ZBqplmGS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmC7ZBqplmGS",
    "outputId": "f9303027-e324-49ff-8229-33a75ef8443c"
   },
   "outputs": [],
   "source": [
    "# Update the scoring dictionary\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef, make_scorer\n",
    "\n",
    "# Ordinal MAE can still be computed for binary (0/1) labels, representing the proportion of misclassified instances if labels are 0 and 1.\n",
    "# However, its interpretation is less direct than accuracy for binary. We keep it for now but primary focus will be on standard binary metrics.\n",
    "def ordinal_mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "ordinal_mae_scorer = make_scorer(ordinal_mae, greater_is_better=False)\n",
    "# severe_error_rate is not applicable for 2 classes, so it's removed.\n",
    "\n",
    "# Define the scoring dictionary with appropriate averages for binary/multiclass (some metrics adapt automatically)\n",
    "# For binary classification, 'weighted' average for F1, precision, recall is equivalent to the binary metric for the positive class if pos_label=1.\n",
    "# 'macro' average for binary is the unweighted average of the metric for each class.\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1': make_scorer(f1_score, average='binary', zero_division=0), # Standard F1 for binary\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0), # Weighted F1\n",
    "    'f1_macro': make_scorer(f1_score, average='macro', zero_division=0), # Macro F1\n",
    "    'precision': make_scorer(precision_score, average='binary', zero_division=0), # Standard Precision\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='binary', zero_division=0), # Standard Recall\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0),\n",
    "    'mcc': make_scorer(matthews_corrcoef), # MCC handles binary and multiclass\n",
    "\n",
    "    # Ordinal MAE (retained, but less primary for binary)\n",
    "    'ordinal_mae': ordinal_mae_scorer,\n",
    "\n",
    "    # Probability-based metrics\n",
    "    # For binary, roc_auc_score doesn't need multi_class or average.\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True), # Standard ROC AUC for binary\n",
    "    # For PR AUC, average='weighted' or 'macro' can be used, or simply calculate for positive class.\n",
    "    # average_precision_score defaults to average of PR curve for binary.\n",
    "    'pr_auc': make_scorer(average_precision_score, needs_proba=True),\n",
    "    'neg_log_loss': make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "}\n",
    "\n",
    "print(f\"Updated scoring dictionary for 2-class problem with keys: {list(scoring.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b675f",
   "metadata": {
    "id": "b90b675f"
   },
   "source": [
    "## Data Verification\n",
    "\n",
    "Perform a quick check on the loaded data to verify dimensions and the class distribution of the original target variable (before pipeline-based engineering for the 2-class problem). This helps ensure that the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd939f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfd939f6",
    "outputId": "095875fb-dd20-4801-a174-64d60ab13446"
   },
   "outputs": [],
   "source": [
    "print(\"=== Data verification ===\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Scoring dict: {scoring}\")\n",
    "\n",
    "print(\"\\nClass distribution in y_train (dummy, the real label will be engineered in the pipeline):\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "print(\"Proportions:\")\n",
    "print(dict(zip(unique, counts / len(y_train))))\n",
    "print(\"\\nNote: the real target variable (2 classes) will be created in the modeling pipeline.\")\n",
    "\n",
    "def perform_data_verification(X_train: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                             y_train: pd.Series, y_test: pd.Series,\n",
    "                             hs_optimal: float, ht_optimal: float) -> None:\n",
    "    \"\"\"\n",
    "    Perform comprehensive data verification for modeling phase.\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test: Feature data\n",
    "        y_train, y_test: Target data (dummy targets)\n",
    "        hs_optimal, ht_optimal: STKDE parameters\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Data Verification for Modeling ===\")\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"Training data: {X_train.shape}\")\n",
    "    print(f\"Test data: {X_test.shape}\")\n",
    "    print(f\"Target variables: y_train {y_train.shape}, y_test {y_test.shape}\")\n",
    "    print(f\"STKDE parameters: hs={hs_optimal}, ht={ht_optimal}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'Latitude', 'Longitude']\n",
    "    available_required = [col for col in required_cols if col in X_train.columns]\n",
    "    print(f\"Required STKDE columns available: {len(available_required)}/{len(required_cols)}\")\n",
    "    \n",
    "    if len(available_required) != len(required_cols):\n",
    "        missing = [col for col in required_cols if col not in X_train.columns]\n",
    "        print(f\"⚠ Missing required columns: {missing}\")\n",
    "    \n",
    "    # Dummy target distribution (will be replaced by engineered targets)\n",
    "    print(f\"\\nDummy target distribution (y_train):\")\n",
    "    if not y_train.empty:\n",
    "        target_counts = y_train.value_counts()\n",
    "        target_props = y_train.value_counts(normalize=True)\n",
    "        for val in target_counts.index:\n",
    "            print(f\"  {val}: {target_counts[val]} ({target_props[val]:.3f})\")\n",
    "    else:\n",
    "        print(\"  y_train is empty\")\n",
    "    \n",
    "    print(\"\\nNote: Real binary targets (LOW/HIGH RISK) will be engineered using STKDE in the modeling pipeline\")\n",
    "    \n",
    "    # Data quality summary\n",
    "    quality_issues = []\n",
    "    \n",
    "    # Check for missing values\n",
    "    train_missing = X_train.isnull().sum().sum()\n",
    "    test_missing = X_test.isnull().sum().sum()\n",
    "    \n",
    "    if train_missing > 0:\n",
    "        quality_issues.append(f\"{train_missing} missing values in training data\")\n",
    "    if test_missing > 0:\n",
    "        quality_issues.append(f\"{test_missing} missing values in test data\")\n",
    "    \n",
    "    # Check data types\n",
    "    object_cols = X_train.select_dtypes(include=['object']).columns\n",
    "    if len(object_cols) > 0:\n",
    "        quality_issues.append(f\"{len(object_cols)} columns with object dtype\")\n",
    "    \n",
    "    if quality_issues:\n",
    "        print(f\"\\n⚠ Data quality issues detected:\")\n",
    "        for issue in quality_issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No major data quality issues detected\")\n",
    "    \n",
    "    # Sample preview\n",
    "    print(f\"\\nSample data preview (first 3 rows):\")\n",
    "    sample_cols = X_train.columns[:8].tolist()  # Show first 8 columns\n",
    "    print(X_train[sample_cols].head(3).to_string())\n",
    "    if len(X_train.columns) > 8:\n",
    "        print(f\"... and {len(X_train.columns) - 8} more columns\")\n",
    "\n",
    "# Perform verification\n",
    "perform_data_verification(X_train, X_test, y_train, y_test, hs_optimal, ht_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc089b0",
   "metadata": {
    "id": "dcc089b0"
   },
   "source": [
    "## Model Definitions and Preprocessing Pipelines\n",
    "\n",
    "This section defines the various classification models that will be evaluated for the 2-class risk problem. It also specifies the appropriate preprocessing pipelines to be used with different types of models (e.g., tree-based models, linear models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98802b",
   "metadata": {
    "id": "6d98802b"
   },
   "source": [
    "### Define Models for Comparison\n",
    "\n",
    "A dictionary, `models_to_evaluate`, is created to hold the instances of the classification models that will be compared. Each model will be integrated into a pipeline that includes the necessary preprocessing steps tailored to its requirements for the 2-class problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17f4d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae17f4d1",
    "outputId": "e347ff20-a43d-4770-89c2-24f930079d39"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def define_core_models() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Define core classification models for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of model name -> model instance\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Defining Core Models for Binary Classification ===\")\n",
    "    \n",
    "    models = {\n",
    "    # --- Baseline ---\n",
    "    'Dummy': DummyClassifier(strategy='stratified', random_state=42),\n",
    "\n",
    "    # --- Linear Models ---\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced', n_jobs=-1),\n",
    "\n",
    "    # --- K-Nearest Neighbors ---\n",
    "    'KNN': KNeighborsClassifier(n_jobs=-1),\n",
    "\n",
    "    # --- Naive Bayes ---\n",
    "    'GaussianNB': GaussianNB(),\n",
    "\n",
    "    # --- Tree-Based Models ---\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "\n",
    "    # --- Ensemble Methods ---\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'Bagging': BaggingClassifier(random_state=42, n_jobs=-1),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "\n",
    "    # --- Support Vector Machines ---\n",
    "    'SVC_linear': LinearSVC(C=1.0, penalty='l2', loss='squared_hinge', dual='auto', max_iter=5000, class_weight='balanced', random_state=42),\n",
    "\n",
    "    # --- Advanced Ensemble Methods (from imblearn) ---\n",
    "    'EasyEnsemble': EasyEnsembleClassifier(random_state=42, n_jobs=-1),\n",
    "    'BalancedRandomForest': BalancedRandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'BalancedBagging': BalancedBaggingClassifier(random_state=42, n_jobs=-1),\n",
    "    }\n",
    "    \n",
    "    # Add XGBoost if available\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "        models['XGBoost'] = XGBClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        print(\"✓ Added XGBoost\")\n",
    "    except ImportError:\n",
    "        print(\"⚠ XGBoost not available\")\n",
    "    \n",
    "    # Add LightGBM if available\n",
    "    try:\n",
    "        from lightgbm import LGBMClassifier\n",
    "        models['LightGBM'] = LGBMClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1,  # Suppress warnings\n",
    "        )\n",
    "        print(\"✓ Added LightGBM\")\n",
    "    except ImportError:\n",
    "        print(\"⚠ LightGBM not available\")\n",
    "    \n",
    "    print(f\"\\nDefined {len(models)} models: {list(models.keys())}\")\n",
    "    return models\n",
    "\n",
    "def categorize_models_for_preprocessing(models: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Categorize models based on their preprocessing requirements.\n",
    "    \n",
    "    Args:\n",
    "        models: Dictionary of model instances\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with model categories\n",
    "    \"\"\"\n",
    "    # Models that work better with tree preprocessing (no scaling, ordinal encoding)\n",
    "    tree_models = [\n",
    "        'DecisionTree', 'RandomForest', 'GradientBoosting', 'ExtraTrees',\n",
    "        'Bagging', 'AdaBoost', 'BalancedRandomForest', 'XGBoost', 'LightGBM',\n",
    "        'EasyEnsemble', 'BalancedBagging'\n",
    "    ]\n",
    "    \n",
    "    # Models that work better with general preprocessing (scaling, one-hot encoding, PCA)\n",
    "    general_models = [\n",
    "        'Dummy', 'LogisticRegression', 'KNN', 'GaussianNB', 'SVC_linear'\n",
    "    ]\n",
    "    \n",
    "    # Models that handle imbalance internally (don't need SMOTE)\n",
    "    balanced_models = [\n",
    "        'Dummy',  # Due to strategy='stratified'\n",
    "        'LogisticRegression',  # Due to class_weight='balanced'\n",
    "        'DecisionTree',  # Due to class_weight='balanced'\n",
    "        'RandomForest',  # Due to class_weight='balanced'\n",
    "        'ExtraTrees',  # Due to class_weight='balanced'\n",
    "        'SVC_linear',  # Due to class_weight='balanced'\n",
    "        'BalancedRandomForest',  # Inherently balanced\n",
    "        'EasyEnsemble',  # Inherently balanced\n",
    "        'BalancedBagging',  # Inherently balanced\n",
    "        'XGBoost',  # Can handle imbalance (e.g., scale_pos_weight)\n",
    "        'LightGBM',  # Can handle imbalance (e.g., is_unbalance=True)\n",
    "        'GradientBoosting' # Generally robust, user included it previously\n",
    "    ]\n",
    "    \n",
    "    # Filter by actually available models\n",
    "    available_tree_models = [name for name in tree_models if name in models]\n",
    "    available_general_models = [name for name in general_models if name in models]\n",
    "    available_balanced_models = [name for name in balanced_models if name in models]\n",
    "    \n",
    "    categorization = {\n",
    "        'tree_preprocessing': available_tree_models,\n",
    "        'general_preprocessing': available_general_models,\n",
    "        'needs_smote': [name for name in models.keys() if name not in available_balanced_models],\n",
    "        'balanced_internally': available_balanced_models\n",
    "    }\n",
    "    \n",
    "    print(\"\\nModel categorization:\")\n",
    "    for category, model_list in categorization.items():\n",
    "        print(f\"  {category}: {model_list}\")\n",
    "    \n",
    "    return categorization\n",
    "\n",
    "# Define models and categorization\n",
    "models_to_evaluate = define_core_models()\n",
    "model_categories = categorize_models_for_preprocessing(models_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb590b",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) for Threshold Definition (No Longer Performed Here)\n",
    "\n",
    "Previously, this section was dedicated to performing Exploratory Data Analysis (EDA) on STKDE intensities to identify a fixed threshold for classifying risk levels using Jenks' natural breaks. **This approach has been revised to prevent data leakage.**\n",
    "\n",
    "**The calculation of Jenks' natural breaks (and thus the definition of thresholds for the 'fixed' strategy) is now integrated directly into the `STKDEAndRiskLabelTransformer` and is performed *within each cross-validation fold* using only the training data of that specific fold.** This ensures that information from the validation set (or the test set, in the case of final model training) does not influence the threshold selection process for the training data of that fold.\n",
    "\n",
    "Therefore, the EDA and global Jenks calculation previously done here are no longer necessary and have been removed. The `CHOSEN_LABELING_STRATEGY` will determine if 'fixed' (with per-fold Jenks) or 'quantile' labeling is used within the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2001646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION: PERFORM_EDA is no longer used for global Jenks calculation.\n",
    "# Jenks calculation for 'fixed' strategy is now done within STKDEAndRiskLabelTransformer per fold.\n",
    "PERFORM_EDA = False # This flag is kept for compatibility but does not trigger global Jenks.\n",
    "\n",
    "# Initialize stkde_intensities_train_for_eda - this variable is no longer populated globally.\n",
    "stkde_intensities_train_for_eda = None\n",
    "\n",
    "print(\"Global EDA for Jenks threshold definition is no longer performed here.\")\n",
    "print(\"If CHOSEN_LABELING_STRATEGY is 'fixed', Jenks breaks will be calculated within each CV fold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da17992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CHOSEN_LABELING_STRATEGY and CHOSEN_FIXED_THRESHOLDS with defaults\n",
    "# These will be used by the STKDEAndRiskLabelTransformer.\n",
    "# If 'fixed' is chosen, the transformer will calculate Jenks per fold.\n",
    "# If 'quantile' is chosen, it will use quantiles per fold.\n",
    "\n",
    "CHOSEN_LABELING_STRATEGY = 'fixed' # Default strategy, can be 'fixed' or 'quantile'\n",
    "CHOSEN_FIXED_THRESHOLDS = None       # This is now effectively a placeholder if strategy is 'fixed',\n",
    "                                     # as actual thresholds are determined per-fold.\n",
    "                                     # If strategy is 'fixed' and this is None, STKDE transformer will calculate them.\n",
    "                                     # If strategy is 'fixed' and this is a list, it might be used as a fallback\n",
    "                                     # or override IF the transformer is designed to accept pre-defined global thresholds\n",
    "                                     # (current implementation calculates per-fold for 'fixed').\n",
    "\n",
    "# The PERFORM_EDA block that calculated global Jenks thresholds is removed.\n",
    "# The STKDEAndRiskLabelTransformer is now responsible for handling 'fixed' strategy by calculating Jenks per fold.\n",
    "\n",
    "print(\"\\\\nSTKDE Labeling Strategy Configuration:\")\n",
    "print(f\"Chosen strategy for STKDEAndRiskLabelTransformer: {CHOSEN_LABELING_STRATEGY}\")\n",
    "if CHOSEN_LABELING_STRATEGY == 'fixed':\n",
    "    print(\"If 'fixed' strategy is used, STKDEAndRiskLabelTransformer will calculate Jenks breaks per fold.\")\n",
    "    if CHOSEN_FIXED_THRESHOLDS is not None:\n",
    "        print(f\"Global CHOSEN_FIXED_THRESHOLDS is set to: {CHOSEN_FIXED_THRESHOLDS}, but per-fold calculation will take precedence for 'fixed' strategy.\")\n",
    "elif CHOSEN_LABELING_STRATEGY == 'quantile':\n",
    "    print(\"If 'quantile' strategy is used, STKDEAndRiskLabelTransformer will use quantiles per fold.\")\n",
    "else:\n",
    "    print(f\"Warning: Unknown CHOSEN_LABELING_STRATEGY: {CHOSEN_LABELING_STRATEGY}. Behavior might be undefined.\")\n",
    "\n",
    "\n",
    "# --- Results Analysis ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_results(results_df: pd.DataFrame, primary_metric: str = 'f1_mean') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze model selection results and identify best performers.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with model results\n",
    "        primary_metric: Primary metric for ranking\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Analyzing Model Results (Primary Metric: {primary_metric}) ===\")\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"⚠ No results to analyze\")\n",
    "        return {}\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # Filter out failed models (those with NaN scores)\n",
    "    valid_results = results_df.dropna(subset=[primary_metric])\n",
    "    \n",
    "    if valid_results.empty:\n",
    "        print(\"❌ No valid results found\")\n",
    "        return {'error': 'No valid results'}\n",
    "    \n",
    "    # Sort by primary metric\n",
    "    valid_results_sorted = valid_results.sort_values(primary_metric, ascending=False)\n",
    "    \n",
    "    # Top performers\n",
    "    top_5 = valid_results_sorted.head(5)\n",
    "    analysis['top_performers'] = top_5[['model_name', primary_metric]].to_dict('records')\n",
    "    \n",
    "    print(f\"\\nTop 5 performers by {primary_metric}:\")\n",
    "    for i, row in top_5.iterrows():\n",
    "        score = row[primary_metric]\n",
    "        std_metric = primary_metric.replace('_mean', '_std')\n",
    "        std_score = row.get(std_metric, 0)\n",
    "        print(f\"  {row['model_name']}: {score:.4f} ± {std_score:.4f}\")\n",
    "    \n",
    "    # Best model\n",
    "    best_model = valid_results_sorted.iloc[0]\n",
    "    analysis['best_model'] = {\n",
    "        'name': best_model['model_name'],\n",
    "        'score': best_model[primary_metric],\n",
    "        'preprocessing': best_model['preprocessing_type'],\n",
    "        'uses_smote': best_model['uses_smote']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✓ Best model: {best_model['model_name']} ({best_model[primary_metric]:.4f})\")\n",
    "    \n",
    "    # Performance by preprocessing type\n",
    "    preprocessing_analysis = valid_results.groupby('preprocessing_type')[primary_metric].agg(['mean', 'std', 'count'])\n",
    "    analysis['preprocessing_analysis'] = preprocessing_analysis.to_dict('index')\n",
    "    \n",
    "    print(f\"\\nPerformance by preprocessing type:\")\n",
    "    for prep_type, stats in preprocessing_analysis.iterrows():\n",
    "        print(f\"  {prep_type}: {stats['mean']:.4f} ± {stats['std']:.4f} (n={stats['count']})\")\n",
    "    \n",
    "    # SMOTE analysis\n",
    "    if 'uses_smote' in valid_results.columns:\n",
    "        smote_analysis = valid_results.groupby('uses_smote')[primary_metric].agg(['mean', 'std', 'count'])\n",
    "        analysis['smote_analysis'] = smote_analysis.to_dict('index')\n",
    "        \n",
    "        print(f\"\\nPerformance by SMOTE usage:\")\n",
    "        for uses_smote, stats in smote_analysis.iterrows():\n",
    "            smote_label = \"With SMOTE\" if uses_smote else \"Without SMOTE\"\n",
    "            print(f\"  {smote_label}: {stats['mean']:.4f} ± {stats['std']:.4f} (n={stats['count']})\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def create_results_visualization(results_df: pd.DataFrame, analysis: Dict[str, Any], \n",
    "                               primary_metric: str = 'f1_mean') -> None:\n",
    "    \"\"\"\n",
    "    Create visualizations for model selection results.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with model results\n",
    "        analysis: Analysis results\n",
    "        primary_metric: Primary metric for visualization\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Creating Results Visualization ===\")\n",
    "    \n",
    "    if results_df.empty or not analysis:\n",
    "        print(\"⚠ No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Filter valid results\n",
    "    valid_results = results_df.dropna(subset=[primary_metric])\n",
    "    \n",
    "    if valid_results.empty:\n",
    "        print(\"⚠ No valid results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Selection Results', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Model performance comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    sorted_results = valid_results.sort_values(primary_metric, ascending=True)\n",
    "    \n",
    "    y_pos = np.arange(len(sorted_results))\n",
    "    scores = sorted_results[primary_metric]\n",
    "    \n",
    "    bars = ax1.barh(y_pos, scores, alpha=0.7)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(sorted_results['model_name'])\n",
    "    ax1.set_xlabel(f'{primary_metric.replace(\"_\", \" \").title()}')\n",
    "    ax1.set_title('Model Performance Comparison')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color top 3 performers differently\n",
    "    for i, bar in enumerate(bars):\n",
    "        if i >= len(bars) - 3:  # Top 3\n",
    "            bar.set_color('darkgreen')\n",
    "            bar.set_alpha(0.8)\n",
    "    \n",
    "    # Plot 2: Performance by preprocessing type\n",
    "    ax2 = axes[0, 1]\n",
    "    if 'preprocessing_analysis' in analysis:\n",
    "        prep_data = analysis['preprocessing_analysis']\n",
    "        prep_types = list(prep_data.keys())\n",
    "        prep_means = [prep_data[pt]['mean'] for pt in prep_types]\n",
    "        prep_stds = [prep_data[pt]['std'] for pt in prep_types]\n",
    "        \n",
    "        ax2.bar(prep_types, prep_means, yerr=prep_stds, alpha=0.7, capsize=5)\n",
    "        ax2.set_ylabel(f'{primary_metric.replace(\"_\", \" \").title()}')\n",
    "        ax2.set_title('Performance by Preprocessing Type')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: SMOTE impact\n",
    "    ax3 = axes[1, 0]\n",
    "    if 'smote_analysis' in analysis:\n",
    "        smote_data = analysis['smote_analysis']\n",
    "        smote_labels = ['Without SMOTE' if not k else 'With SMOTE' for k in smote_data.keys()]\n",
    "        smote_means = [smote_data[k]['mean'] for k in smote_data.keys()]\n",
    "        smote_stds = [smote_data[k]['std'] for k in smote_data.keys()]\n",
    "        \n",
    "        ax3.bar(smote_labels, smote_means, yerr=smote_stds, alpha=0.7, capsize=5)\n",
    "        ax3.set_ylabel(f'{primary_metric.replace(\"_\", \" \").title()}')\n",
    "        ax3.set_title('Impact of SMOTE')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Top models detailed comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'top_performers' in analysis:\n",
    "        top_models = analysis['top_performers'][:5]\n",
    "        model_names = [m['model_name'] for m in top_models]\n",
    "        model_scores = [m[primary_metric] for m in top_models]\n",
    "        \n",
    "        bars = ax4.bar(range(len(model_names)), model_scores, alpha=0.7)\n",
    "        ax4.set_xticks(range(len(model_names)))\n",
    "        ax4.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        ax4.set_ylabel(f'{primary_metric.replace(\"_\", \" \").title()}')\n",
    "        ax4.set_title('Top 5 Models')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight best model\n",
    "        bars[0].set_color('gold')\n",
    "        bars[0].set_alpha(0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Visualization created\")\n",
    "\n",
    "print(\"✓ Analysis and visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468ad2e",
   "metadata": {},
   "source": [
    "## Model Selection and Hyperparameter Tuning\n",
    "\n",
    "Based on the cross-validation results, we select the best performing model and perform hyperparameter tuning to optimize its performance. The tuning process uses the same leakage-free approach with proper target engineering within each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hyperparameter_tuning(model_name: str, base_model: Any,\n",
    "                                  preprocessor: Any, stkde_config: Dict[str, Any],\n",
    "                                  cv_config: Dict[str, Any], use_smote: bool,\n",
    "                                  results_dir: str) -> Tuple[Pipeline, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for the selected model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        base_model: Base model instance\n",
    "        preprocessor: Preprocessing pipeline\n",
    "        stkde_config: STKDE configuration\n",
    "        cv_config: CV configuration\n",
    "        use_smote: Whether to use SMOTE\n",
    "        results_dir: Results directory\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (best_pipeline, best_params)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Hyperparameter Tuning for {model_name} ===\")\n",
    "    \n",
    "    # Define parameter grids for different models\n",
    "    param_grids = {\n",
    "        'RandomForest': {\n",
    "            'feature_pipeline__classifier__n_estimators': [50, 100, 200],\n",
    "            'feature_pipeline__classifier__max_depth': [5, 10, None],\n",
    "            'feature_pipeline__classifier__min_samples_split': [2, 5, 10],\n",
    "            'feature_pipeline__classifier__min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'feature_pipeline__classifier__n_estimators': [50, 100, 200],\n",
    "            'feature_pipeline__classifier__max_depth': [3, 6, 9],\n",
    "            'feature_pipeline__classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'feature_pipeline__classifier__subsample': [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'feature_pipeline__classifier__C': [0.1, 1.0, 10.0],\n",
    "            'feature_pipeline__classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'feature_pipeline__classifier__solver': ['liblinear', 'saga']\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'feature_pipeline__classifier__n_estimators': [50, 100, 200],\n",
    "            'feature_pipeline__classifier__max_depth': [3, 6, 9],\n",
    "            'feature_pipeline__classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'feature_pipeline__classifier__subsample': [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'feature_pipeline__classifier__n_estimators': [50, 100, 200],\n",
    "            'feature_pipeline__classifier__max_depth': [3, 6, 9],\n",
    "            'feature_pipeline__classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'feature_pipeline__classifier__subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Get parameter grid for the model\n",
    "    if model_name not in param_grids:\n",
    "        print(f\"⚠ No parameter grid defined for {model_name}, using default parameters\")\n",
    "        # Create pipeline with default parameters\n",
    "        pipeline = create_model_pipeline(\n",
    "            model_name=model_name,\n",
    "            model_instance=base_model,\n",
    "            preprocessor=preprocessor,\n",
    "            stkde_config=stkde_config,\n",
    "            use_smote=use_smote\n",
    "        )\n",
    "        return pipeline, {}\n",
    "    \n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    # Adjust parameter names if SMOTE is used\n",
    "    if use_smote:\n",
    "        adjusted_param_grid = {}\n",
    "        for key, value in param_grid.items():\n",
    "            # Parameters come after SMOTE in the pipeline\n",
    "            if 'feature_pipeline__classifier__' in key:\n",
    "                new_key = key.replace('feature_pipeline__classifier__', 'feature_pipeline__classifier__')\n",
    "                adjusted_param_grid[new_key] = value\n",
    "            else:\n",
    "                adjusted_param_grid[key] = value\n",
    "        param_grid = adjusted_param_grid\n",
    "    \n",
    "    # Create base pipeline\n",
    "    pipeline = create_model_pipeline(\n",
    "        model_name=model_name,\n",
    "        model_instance=base_model,\n",
    "        preprocessor=preprocessor,\n",
    "        stkde_config=stkde_config,\n",
    "        use_smote=use_smote\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Perform randomized search\n",
    "        print(f\"Performing randomized search with {len(param_grid)} parameter ranges...\")\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=20,  # Limit iterations for efficiency\n",
    "            cv=cv_config['cv_strategy'],\n",
    "            scoring='f1',  # Use F1 as primary metric\n",
    "            n_jobs=1,  # Avoid nested parallelization\n",
    "            random_state=42,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit the search\n",
    "        random_search.fit(cv_config['X_train_sorted'], cv_config['y_train_sorted'])\n",
    "        \n",
    "        print(f\"✓ Best F1 score: {random_search.best_score_:.4f}\")\n",
    "        print(f\"✓ Best parameters: {random_search.best_params_}\")\n",
    "        \n",
    "        # Save tuning results\n",
    "        tuning_results_file = os.path.join(results_dir, f\"{model_name}_tuning_results.json\")\n",
    "        try:\n",
    "            with open(tuning_results_file, 'w') as f:\n",
    "                json.dump(tuning_results, f, indent=2)\n",
    "            print(f\"✓ Tuning results saved to: {tuning_results_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error saving tuning results: {e}\")\n",
    "        \n",
    "        return random_search.best_estimator_, tuning_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during hyperparameter tuning: {e}\")\n",
    "        \n",
    "        # Return the base pipeline as fallback\n",
    "        base_pipeline = create_model_pipeline(\n",
    "            model_name=model_name,\n",
    "            model_instance=base_model,\n",
    "            preprocessor=preprocessor,\n",
    "            stkde_config=stkde_config,\n",
    "            use_smote=use_smote\n",
    "        )\n",
    "        \n",
    "        fallback_results = {\n",
    "            'best_score': np.nan,\n",
    "            'best_params': {},\n",
    "            'model_name': model_name,\n",
    "            'error': str(e)\n",
    "        }\n",
    "        \n",
    "        return base_pipeline, fallback_results\n",
    "\n",
    "def define_parameter_grids() -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Define parameter grids for hyperparameter tuning.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping model names to parameter grids\n",
    "    \"\"\"\n",
    "    param_grids = {\n",
    "        'LogisticRegression': {\n",
    "            'feature_pipeline__classifier__C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "            'feature_pipeline__classifier__penalty': ['l1', 'l2'],\n",
    "            'feature_pipeline__classifier__solver': ['liblinear', 'saga']\n",
    "        },\n",
    "        \n",
    "        'DecisionTree': {\n",
    "            'feature_pipeline__classifier__max_depth': [3, 5, 7, 10, None],\n",
    "            'feature_pipeline__classifier__min_samples_split': [2, 5, 10],\n",
    "            'feature_pipeline__classifier__min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        \n",
    "        'RandomForest': {\n",
    "            'feature_pipeline__classifier__n_estimators': [50, 100, 200],\n",
    "            'feature_pipeline__classifier__max_depth': [5, 10, 15, None],\n",
    "            'feature_pipeline__classifier__min_samples_split': [2, 5],\n",
    "            'feature_pipeline__classifier__min_samples_leaf': [1, 2]\n",
    "        },\n",
    "        \n",
    "        'GradientBoosting': {\n",
    "            'feature_pipeline__classifier__n_estimators': [50, 100, 200],\n",
    "            'feature_pipeline__classifier__max_depth': [3, 5, 7],\n",
    "            'feature_pipeline__classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "        },\n",
    "        \n",
    "        'XGBoost': {\n",
    "            'feature_pipeline__classifier__n_estimators': [50, 100, 200],\n",
    "            'feature_pipeline__classifier__max_depth': [3, 5, 7],\n",
    "            'feature_pipeline__classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "        },\n",
    "        \n",
    "        'LightGBM': {\n",
    "            'feature_pipeline__classifier__n_estimators': [50, 100, 200],\n",
    "            'feature_pipeline__classifier__max_depth': [3, 5, 7],\n",
    "            'feature_pipeline__classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "        },\n",
    "        \n",
    "        'KNN': {\n",
    "            'feature_pipeline__classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "            'feature_pipeline__classifier__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return param_grids\n",
    "\n",
    "# Hyperparameter tuning execution\n",
    "if 'best_model_name' in locals() and best_model_name is not None:\n",
    "    print(f\"\\n=== Hyperparameter Tuning for {best_model_name} ===\")\n",
    "    \n",
    "    # Define parameter grids\n",
    "    param_grids = define_parameter_grids()\n",
    "    \n",
    "    if best_model_name in param_grids:\n",
    "        # Get model configuration\n",
    "        best_model_instance = models_to_evaluate[best_model_name]\n",
    "        \n",
    "        # Determine preprocessing and SMOTE settings\n",
    "        if best_model_name in model_categories['tree_preprocessing']:\n",
    "            best_preprocessor = preprocessors['preprocessor_trees']\n",
    "            preprocessing_type = 'tree'\n",
    "        else:\n",
    "            best_preprocessor = preprocessors['preprocessor_general']\n",
    "            preprocessing_type = 'general'\n",
    "        \n",
    "        uses_smote = best_model_name in model_categories.get('needs_smote', [])\n",
    "        \n",
    "        print(f\"Tuning {best_model_name} with {preprocessing_type} preprocessing, SMOTE: {uses_smote}\")\n",
    "        \n",
    "        # Perform tuning\n",
    "        try:\n",
    "            tuned_pipeline, tuning_results = perform_hyperparameter_tuning(\n",
    "                model_name=best_model_name,\n",
    "                base_model=best_model_instance,\n",
    "                preprocessor=best_preprocessor,\n",
    "                stkde_config=stkde_config,\n",
    "                cv_config=cv_config,\n",
    "                use_smote=uses_smote,\n",
    "                results_dir=modeling_results_dir\n",
    "            )\n",
    "            \n",
    "            print(f\"✓ Hyperparameter tuning completed for {best_model_name}\")\n",
    "            print(f\"  Best CV F1 score: {tuning_results.get('best_score', 'N/A')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Hyperparameter tuning failed: {e}\")\n",
    "            tuned_pipeline = None\n",
    "            tuning_results = {}\n",
    "            \n",
    "    else:\n",
    "        print(f\"⚠ No parameter grid defined for {best_model_name}\")\n",
    "        tuned_pipeline = None\n",
    "        tuning_results = {}\n",
    "        \n",
    "else:\n",
    "    print(\"⚠ No best model identified for hyperparameter tuning\")\n",
    "    tuned_pipeline = None\n",
    "    tuning_results = {}\n",
    "\n",
    "def evaluate_final_model_on_test_set(pipeline: Pipeline, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                                   X_test: pd.DataFrame, y_test: pd.Series, \n",
    "                                   model_name: str, results_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate the final tuned model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Tuned model pipeline\n",
    "        X_train, y_train: Training data for fitting\n",
    "        X_test, y_test: Test data for evaluation\n",
    "        model_name: Name of the model\n",
    "        results_dir: Directory for saving results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with test set evaluation results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Final Test Set Evaluation for {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Fit the pipeline on full training data\n",
    "        print(\"Fitting final model on complete training data...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        print(\"Making predictions on test set...\")\n",
    "        y_pred_test = pipeline.predict(X_test)\n",
    "        \n",
    "        # Get probability predictions if available\n",
    "        try:\n",
    "            y_proba_test = pipeline.predict_proba(X_test)\n",
    "            if y_proba_test.shape[1] == 2:\n",
    "                y_proba_test_pos = y_proba_test[:, 1]  # Probability of positive class\n",
    "            else:\n",
    "                y_proba_test_pos = y_proba_test.ravel()\n",
    "        except:\n",
    "            y_proba_test_pos = None\n",
    "            print(\"⚠ Probability predictions not available\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_results = {}\n",
    "        \n",
    "        # Basic classification metrics\n",
    "        test_results['accuracy'] = accuracy_score(y_test, y_pred_test)\n",
    "        test_results['f1'] = f1_score(y_test, y_pred_test, average='binary')\n",
    "        test_results['precision'] = precision_score(y_test, y_pred_test, average='binary', zero_division=0)\n",
    "        test_results['recall'] = recall_score(y_test, y_pred_test, average='binary', zero_division=0)\n",
    "        test_results['mcc'] = matthews_corrcoef(y_test, y_pred_test)\n",
    "        \n",
    "        # Probability-based metrics\n",
    "        if y_proba_test_pos is not None:\n",
    "            test_results['roc_auc'] = roc_auc_score(y_test, y_proba_test_pos)\n",
    "            test_results['pr_auc'] = average_precision_score(y_test, y_proba_test_pos)\n",
    "        \n",
    "        # Class distribution\n",
    "        unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "        unique_pred, counts_pred = np.unique(y_pred_test, return_counts=True)\n",
    "        \n",
    "        test_results['test_class_distribution'] = dict(zip(unique_test.astype(str), counts_test.tolist()))\n",
    "        test_results['pred_class_distribution'] = dict(zip(unique_pred.astype(str), counts_pred.tolist()))\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nTest Set Performance for {model_name}:\")\n",
    "        print(f\"  Accuracy: {test_results['accuracy']:.4f}\")\n",
    "        print(f\"  F1-score: {test_results['f1']:.4f}\")\n",
    "        print(f\"  Precision: {test_results['precision']:.4f}\")\n",
    "        print(f\"  Recall: {test_results['recall']:.4f}\")\n",
    "        print(f\"  MCC: {test_results['mcc']:.4f}\")\n",
    "        \n",
    "        if 'roc_auc' in test_results:\n",
    "            print(f\"  ROC AUC: {test_results['roc_auc']:.4f}\")\n",
    "        if 'pr_auc' in test_results:\n",
    "            print(f\"  PR AUC: {test_results['pr_auc']:.4f}\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        test_results_file = os.path.join(results_dir, f\"{model_name}_test_results.json\")\n",
    "        try:\n",
    "            # Convert numpy types to native Python types for JSON serialization\n",
    "            serializable_results = {}\n",
    "            for key, value in test_results.items():\n",
    "                if isinstance(value, (np.integer, np.floating)):\n",
    "                    serializable_results[key] = float(value)\n",
    "                else:\n",
    "                    serializable_results[key] = value\n",
    "            \n",
    "            with open(test_results_file, 'w') as f:\n",
    "                json.dump(serializable_results, f, indent=2)\n",
    "            print(f\"✓ Test results saved to: {test_results_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error saving test results: {e}\")\n",
    "        \n",
    "        # Save predictions\n",
    "        predictions_file = os.path.join(results_dir, f\"{model_name}_test_predictions.csv\")\n",
    "        try:\n",
    "            pred_df = pd.DataFrame({\n",
    "                'y_true': y_test,\n",
    "                'y_pred': y_pred_test\n",
    "            })\n",
    "            \n",
    "            if y_proba_test_pos is not None:\n",
    "                pred_df['y_proba'] = y_proba_test_pos\n",
    "            \n",
    "            pred_df.to_csv(predictions_file, index=False)\n",
    "            print(f\"✓ Predictions saved to: {predictions_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error saving predictions: {e}\")\n",
    "        \n",
    "        return test_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in test set evaluation: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test set evaluation\n",
    "if tuned_pipeline is not None and best_model_name is not None:\n",
    "    print(\"\\n=== Final Test Set Evaluation ===\")\n",
    "    \n",
    "    final_test_results = evaluate_final_model_on_test_set(\n",
    "        pipeline=tuned_pipeline,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        model_name=best_model_name,\n",
    "        results_dir=modeling_results_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Phase 3 - Modeling Complete!\")\n",
    "    print(f\"   Best Model: {best_model_name}\")\n",
    "    if 'f1' in final_test_results:\n",
    "        print(f\"   Test F1-score: {final_test_results['f1']:.4f}\")\n",
    "    if 'accuracy' in final_test_results:\n",
    "        print(f\"   Test Accuracy: {final_test_results['accuracy']:.4f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠ Cannot perform test set evaluation - no tuned pipeline available\")\n",
    "    final_test_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_summary_report(model_results_df: pd.DataFrame, \n",
    "                               final_test_results: Dict[str, Any],\n",
    "                               best_model_name: str, \n",
    "                               results_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a comprehensive final summary report of the modeling phase.\n",
    "    \n",
    "    Args:\n",
    "        model_results_df: DataFrame with cross-validation results\n",
    "        final_test_results: Test set evaluation results\n",
    "        best_model_name: Name of the best performing model\n",
    "        results_dir: Directory for saving the report\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 DMML PROJECT - MODELING PHASE SUMMARY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase completion status\n",
    "    print(\"\\n📋 PHASE COMPLETION STATUS:\")\n",
    "    print(\"✅ Phase 1 - Custom Transformers: COMPLETED\")\n",
    "    print(\"✅ Phase 2 - Alternative Preprocessing: COMPLETED\") \n",
    "    print(\"✅ Phase 3 - Modeling & Evaluation: COMPLETED\")\n",
    "    print(\"📋 Phase 4 - Final Tuning & Training: READY\")\n",
    "    \n",
    "    # Model evaluation summary\n",
    "    print(f\"\\n🔍 MODEL EVALUATION SUMMARY:\")\n",
    "    if not model_results_df.empty:\n",
    "        total_models = len(model_results_df)\n",
    "        successful_models = len(model_results_df.dropna(subset=['f1_mean'] if 'f1_mean' in model_results_df.columns else []))\n",
    "        print(f\"   Total models evaluated: {total_models}\")\n",
    "        print(f\"   Successful evaluations: {successful_models}\")\n",
    "        print(f\"   Success rate: {successful_models/total_models*100:.1f}%\")\n",
    "        \n",
    "        # Top 3 performers\n",
    "        if 'f1_mean' in model_results_df.columns:\n",
    "            valid_results = model_results_df.dropna(subset=['f1_mean'])\n",
    "            if not valid_results.empty:\n",
    "                top_3 = valid_results.nlargest(3, 'f1_mean')\n",
    "                print(f\"\\n   🏆 TOP 3 PERFORMERS (by F1-score):\")\n",
    "                for i, (_, row) in enumerate(top_3.iterrows(), 1):\n",
    "                    print(f\"      {i}. {row['model_name']}: {row['f1_mean']:.4f} ± {row.get('f1_std', 0):.4f}\")\n",
    "    \n",
    "    # Best model details\n",
    "    print(f\"\\n🏅 BEST MODEL DETAILS:\")\n",
    "    if best_model_name:\n",
    "        print(f\"   Model: {best_model_name}\")\n",
    "        \n",
    "        if not model_results_df.empty:\n",
    "            best_cv_row = model_results_df[model_results_df['model_name'] == best_model_name]\n",
    "            if not best_cv_row.empty:\n",
    "                row = best_cv_row.iloc[0]\n",
    "                print(f\"   Preprocessing: {row.get('preprocessing_type', 'unknown')}\")\n",
    "                print(f\"   Uses SMOTE: {row.get('uses_smote', False)}\")\n",
    "                print(f\"   Internal balancing: {row.get('uses_balancing', False)}\")\n",
    "                \n",
    "                # CV performance\n",
    "                print(f\"\\n   📊 CROSS-VALIDATION PERFORMANCE:\")\n",
    "                cv_metrics = ['accuracy_mean', 'f1_mean', 'precision_mean', 'recall_mean', 'roc_auc_mean']\n",
    "                for metric in cv_metrics:\n",
    "                    if metric in row:\n",
    "                        std_metric = metric.replace('_mean', '_std')\n",
    "                        std_val = row.get(std_metric, 0)\n",
    "                        print(f\"      {metric.replace('_mean', '').upper()}: {row[metric]:.4f} ± {std_val:.4f}\")\n",
    "        \n",
    "        # Test set performance\n",
    "        if final_test_results and 'error' not in final_test_results:\n",
    "            print(f\"\\n   🎯 TEST SET PERFORMANCE:\")\n",
    "            test_metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc', 'pr_auc', 'mcc']\n",
    "            for metric in test_metrics:\n",
    "                if metric in final_test_results:\n",
    "                    print(f\"      {metric.upper()}: {final_test_results[metric]:.4f}\")\n",
    "    \n",
    "    # Technical configuration\n",
    "    print(f\"\\n⚙️  TECHNICAL CONFIGURATION:\")\n",
    "    print(f\"   Target engineering: STKDE-based binary risk labeling\")\n",
    "    print(f\"   Cross-validation: TimeSeriesSplit (5 folds)\")\n",
    "    print(f\"   Data leakage prevention: ✅ Implemented\")\n",
    "    print(f\"   Class imbalance handling: SMOTE + balanced models\")\n",
    "    print(f\"   Hyperparameter tuning: RandomizedSearchCV\")\n",
    "    \n",
    "    # File outputs\n",
    "    print(f\"\\n📁 OUTPUT FILES GENERATED:\")\n",
    "    print(f\"   📊 Model comparison: model_selection_results.csv\")\n",
    "    print(f\"   🔧 Hyperparameter tuning: {best_model_name}_tuning_results.json\")\n",
    "    print(f\"   🎯 Test set evaluation: {best_model_name}_test_results.json\")\n",
    "    print(f\"   📈 Test predictions: {best_model_name}_test_predictions.csv\")\n",
    "    print(f\"   📂 All files saved to: {results_dir}\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    if final_test_results and 'error' not in final_test_results:\n",
    "        f1_score = final_test_results.get('f1', 0)\n",
    "        accuracy = final_test_results.get('accuracy', 0)\n",
    "        \n",
    "        print(f\"\\n⭐ QUALITY ASSESSMENT:\")\n",
    "        if f1_score >= 0.8:\n",
    "            print(\"   🟢 Excellent performance (F1 ≥ 0.80)\")\n",
    "        elif f1_score >= 0.7:\n",
    "            print(\"   🟡 Good performance (F1 ≥ 0.70)\")\n",
    "        elif f1_score >= 0.6:\n",
    "            print(\"   🟠 Moderate performance (F1 ≥ 0.60)\")\n",
    "        else:\n",
    "            print(\"   🔴 Low performance (F1 < 0.60)\")\n",
    "        \n",
    "        print(f\"   Model reliability: {'High' if f1_score > 0.75 and accuracy > 0.75 else 'Moderate' if f1_score > 0.6 else 'Low'}\")\n",
    "    \n",
    "    # Next steps\n",
    "    print(f\"\\n🚀 NEXT STEPS:\")\n",
    "    print(\"   1. Proceed to Phase 4 - Final Tuning & Training\")\n",
    "    print(\"   2. Advanced hyperparameter optimization\")\n",
    "    print(\"   3. Model interpretability analysis\")\n",
    "    print(\"   4. Production deployment preparation\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📝 End of Modeling Phase Summary\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Generate final summary report\n",
    "try:\n",
    "    create_final_summary_report(\n",
    "        model_results_df=model_results_df if 'model_results_df' in locals() else pd.DataFrame(),\n",
    "               final_test_results=final_test_results if 'final_test_results' in locals() else {},\n",
    "        best_model_name=best_model_name if 'best_model_name' in locals() else 'Unknown',\n",
    "        results_dir=modeling_results_dir\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error generating summary report: {e}\")\n",
    "    print(\"\\n✅ Phase 3 - Modeling phase completed with basic functionality\")\n",
    "\n",
    "print(f\"\\n🎉 PHASE 3 COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"The modeling pipeline is now fully functional with:\")\n",
    "print(f\"✓ Leakage-free STKDE target engineering\")\n",
    "print(f\"✓ Comprehensive model evaluation framework\") \n",
    "print(f\"✓ Robust cross-validation with temporal considerations\")\n",
    "print(f\"✓ Hyperparameter tuning capabilities\")\n",
    "print(f\"✓ Final test set evaluation\")\n",
    "print(f\"\\nReady to proceed to Phase 4 - TuningAndTraining.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf23f3",
   "metadata": {},
   "source": [
    "## Workflow Execution\n",
    "\n",
    "This section executes the main modeling workflow: model selection, hyperparameter tuning (if a best model is found), and final evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890703fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Starting Modeling Workflow ===\")\n",
    "\n",
    "# Configuration for the modeling run\n",
    "# These would typically be defined earlier or passed as arguments\n",
    "N_SPLITS_CV = 5  # Number of splits for TimeSeriesSplit\n",
    "PRIMARY_METRIC_MODEL_SELECTION = 'f1_mean'  # Metric to select the best model\n",
    "PRIMARY_METRIC_TUNING = 'f1'  # Metric for hyperparameter tuning scoring\n",
    "N_ITER_RANDOM_SEARCH = 10  # Number of iterations for RandomizedSearchCV\n",
    "\n",
    "# 1. Define STKDE Configuration (used by STKDEAndRiskLabelTransformer)\n",
    "stkde_config = {\n",
    "    'hs': hs_optimal,  # Loaded from preprocessing artifacts\n",
    "    'ht': ht_optimal,  # Loaded from preprocessing artifacts\n",
    "    'strategy': CHOSEN_LABELING_STRATEGY,  # 'fixed' or 'quantile', defined earlier\n",
    "    'fixed_thresholds': CHOSEN_FIXED_THRESHOLDS,  # None if Jenks is computed per fold for 'fixed'\n",
    "    'n_classes': 2,  # For binary classification\n",
    "    'n_jobs': -1,\n",
    "    'intensity_col_name': 'stkde_intensity_engineered',\n",
    "    'label_col_name': 'RISK_LEVEL_engineered'\n",
    "}\n",
    "print(f\"STKDE Configuration: {stkde_config}\")\n",
    "\n",
    "# 2. Define Cross-Validation Strategy\n",
    "X_train_sorted = X_train.sort_values(by=['YEAR', 'MONTH', 'DAY', 'HOUR']).copy()\n",
    "y_train_sorted = y_train.loc[X_train_sorted.index].copy()\n",
    "cv_strategy = TimeSeriesSplit(n_splits=N_SPLITS_CV)\n",
    "\n",
    "cv_config = {\n",
    "    'X_train_sorted': X_train_sorted,\n",
    "    'y_train_sorted': y_train_sorted,  # Placeholder: STKDE transformer will engineer true labels\n",
    "    'cv_strategy': cv_strategy,\n",
    "    'scoring': scoring,  # Defined earlier\n",
    "    'n_jobs_cv': 1\n",
    "}\n",
    "print(f\"CV Configuration: Using TimeSeriesSplit with {N_SPLITS_CV} splits.\")\n",
    "\n",
    "# --- Start Modifications for Score Management ---\n",
    "LOAD_SCORES_IF_EXIST = True # Set to False to force recalculation\n",
    "all_fold_scores = {}\n",
    "fold_scores_path = os.path.join(modeling_results_dir, 'all_models_fold_scores.json')\n",
    "\n",
    "if LOAD_SCORES_IF_EXIST and os.path.exists(fold_scores_path):\n",
    "    print(f\"\\nℹ️ Loading existing scores from: {fold_scores_path}\")\n",
    "    try:\n",
    "        with open(fold_scores_path, 'r') as f:\n",
    "            all_fold_scores = json.load(f)\n",
    "        print(f\"   Loaded scores for {len(all_fold_scores)} models.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Error loading existing scores: {e}. Re-running all models.\")\n",
    "        all_fold_scores = {}\n",
    "# --- End Modifications for Score Management ---\n",
    "\n",
    "# 3. Run Model Selection\n",
    "print(\"\\n--- Running Model Selection ---\")\n",
    "model_results_list = []\n",
    "\n",
    "for model_name, model_instance in models_to_evaluate.items():\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "\n",
    "    # Determine preprocessing type and SMOTE usage\n",
    "    if model_name in model_categories['tree_preprocessing']:\n",
    "        preprocessor_ct = preprocessor_trees_ct\n",
    "        preprocessing_type = 'tree'\n",
    "    else:\n",
    "        preprocessor_ct = preprocessor_general_ct\n",
    "        preprocessing_type = 'general'\n",
    "\n",
    "    if preprocessor_ct is None:\n",
    "        print(f\"⚠ Skipping {model_name} because its required preprocessor ({preprocessing_type}) is not available.\")\n",
    "        model_results_list.append({\n",
    "            'model_name': model_name,\n",
    "            'error': f\"Preprocessor '{preprocessing_type}' not available\",\n",
    "            'preprocessing_type': preprocessing_type,\n",
    "            'uses_smote': False,\n",
    "            'uses_balancing': False\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    needs_smote = model_name in model_categories.get('needs_smote', [])\n",
    "    uses_internal_balancing = model_name in model_categories.get('balanced_internally', [])\n",
    "\n",
    "    print(f\"  Preprocessing: {preprocessing_type}, Needs SMOTE: {needs_smote}, Internal Balancing: {uses_internal_balancing}\")\n",
    "\n",
    "    # --- Start Modifications for Loading Model Scores ---\n",
    "    if LOAD_SCORES_IF_EXIST and model_name in all_fold_scores:\n",
    "        print(f\"  Found existing scores for {model_name}. Using stored scores.\")\n",
    "        cv_results_loaded = all_fold_scores[model_name]\n",
    "        \n",
    "        current_result_entry = {\n",
    "            'model_name': model_name,\n",
    "            'preprocessing_type': preprocessing_type,\n",
    "            'uses_smote': needs_smote,\n",
    "            'uses_balancing': uses_internal_balancing,\n",
    "            'loaded_from_file': True\n",
    "        }\n",
    "        \n",
    "        valid_scores_found = False\n",
    "        # Rebuild means and standard deviations from loaded fold scores\n",
    "        for metric_name_loaded, scores_array_loaded in cv_results_loaded.items():\n",
    "            if metric_name_loaded.startswith('test_'): # These are per-fold score arrays\n",
    "                metric_key_loaded = metric_name_loaded.replace('test_', '')\n",
    "                scores_np_array = np.array(scores_array_loaded) if isinstance(scores_array_loaded, list) else scores_array_loaded\n",
    "                if scores_np_array.size > 0:\n",
    "                    current_result_entry[f\"{metric_key_loaded}_mean\"] = np.mean(scores_np_array)\n",
    "                    current_result_entry[f\"{metric_key_loaded}_std\"] = np.std(scores_np_array)\n",
    "                    valid_scores_found = True\n",
    "            elif metric_name_loaded in ['fit_time', 'score_time']:\n",
    "                scores_np_array = np.array(scores_array_loaded) if isinstance(scores_array_loaded, list) else scores_array_loaded\n",
    "                if scores_np_array.size > 0:\n",
    "                     current_result_entry[f\"{metric_name_loaded}_mean\"] = np.mean(scores_np_array)\n",
    "        \n",
    "        if valid_scores_found:\n",
    "            model_results_list.append(current_result_entry)\n",
    "            # Improved Metrics Display for loaded scores\n",
    "            f1_mean_loaded = current_result_entry.get('f1_mean', np.nan)\n",
    "            f1_std_loaded = current_result_entry.get('f1_std', np.nan)\n",
    "            print(f\"  ✓ {model_name} CV scores loaded. F1: {f1_mean_loaded:.4f} ± {f1_std_loaded:.4f}\")\n",
    "            \n",
    "            accuracy_mean_loaded = current_result_entry.get('accuracy_mean', np.nan)\n",
    "            accuracy_std_loaded = current_result_entry.get('accuracy_std', np.nan)\n",
    "            print(f\"    Accuracy: {accuracy_mean_loaded:.4f} ± {accuracy_std_loaded:.4f}\")\n",
    "            \n",
    "            roc_auc_mean_loaded = current_result_entry.get('roc_auc_mean', np.nan)\n",
    "            roc_auc_std_loaded = current_result_entry.get('roc_auc_std', np.nan)\n",
    "            if not np.isnan(roc_auc_mean_loaded):\n",
    "                print(f\"    ROC AUC: {roc_auc_mean_loaded:.4f} ± {roc_auc_std_loaded:.4f}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"  ⚠️ Loaded scores for {model_name} are invalid or empty. Re-running cross-validation.\")\n",
    "    # --- End Modifications for Loading Model Scores ---\n",
    "\n",
    "    stkde_transformer_instance = STKDEAndRiskLabelTransformer(**stkde_config)\n",
    "\n",
    "    if needs_smote:\n",
    "        feature_processor_pipeline = ImbPipeline([\n",
    "            ('preprocessor', preprocessor_ct),\n",
    "            ('smote', SMOTE(random_state=42))\n",
    "        ])\n",
    "    else:\n",
    "        feature_processor_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor_ct)\n",
    "        ])\n",
    "\n",
    "    full_pipeline = CustomModelPipeline(\n",
    "        stkde_transformer=stkde_transformer_instance,\n",
    "        feature_processor=feature_processor_pipeline,\n",
    "        classifier=clone(model_instance)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cv_results = cross_validate(\n",
    "            full_pipeline,\n",
    "            cv_config['X_train_sorted'],\n",
    "            cv_config['y_train_sorted'],\n",
    "            cv=cv_config['cv_strategy'],\n",
    "            scoring=cv_config['scoring'],\n",
    "            n_jobs=cv_config['n_jobs_cv'],\n",
    "            error_score='raise'\n",
    "        )\n",
    "\n",
    "        # --- Start Modifications for Saving Fold Scores ---\n",
    "        all_fold_scores[model_name] = {k_cv: (v_cv.tolist() if isinstance(v_cv, np.ndarray) else v_cv) for k_cv, v_cv in cv_results.items()}\n",
    "        # --- End Modifications for Saving Fold Scores ---\n",
    "\n",
    "        result_entry = {\n",
    "            'model_name': model_name,\n",
    "            'preprocessing_type': preprocessing_type,\n",
    "            'uses_smote': needs_smote,\n",
    "            'uses_balancing': uses_internal_balancing,\n",
    "            'loaded_from_file': False\n",
    "        }\n",
    "        for metric_name, scores_array in cv_results.items():\n",
    "            if metric_name.startswith('test_'):\n",
    "                metric_key = metric_name.replace('test_', '')\n",
    "                result_entry[f\"{metric_key}_mean\"] = np.mean(scores_array)\n",
    "                result_entry[f\"{metric_key}_std\"] = np.std(scores_array)\n",
    "            elif metric_name in ['fit_time', 'score_time']:\n",
    "                result_entry[f\"{metric_name}_mean\"] = np.mean(scores_array)\n",
    "        \n",
    "        model_results_list.append(result_entry)\n",
    "        \n",
    "        # --- Improved Metrics Display ---\n",
    "        f1_mean = result_entry.get('f1_mean', np.nan)\n",
    "        f1_std = result_entry.get('f1_std', np.nan)\n",
    "        print(f\"  ✓ {model_name} CV completed. F1: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "        \n",
    "        accuracy_mean = result_entry.get('accuracy_mean', np.nan)\n",
    "        accuracy_std = result_entry.get('accuracy_std', np.nan)\n",
    "        print(f\"    Accuracy: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")\n",
    "        \n",
    "        roc_auc_mean = result_entry.get('roc_auc_mean', np.nan)\n",
    "        roc_auc_std = result_entry.get('roc_auc_std', np.nan)\n",
    "        if not np.isnan(roc_auc_mean):\n",
    "             print(f\"    ROC AUC: {roc_auc_mean:.4f} ± {roc_auc_std:.4f}\")\n",
    "        # --- End Improved Metrics Display ---\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error evaluating {model_name}: {e}\")\n",
    "        model_results_list.append({\n",
    "            'model_name': model_name,\n",
    "            'error': str(e),\n",
    "            'preprocessing_type': preprocessing_type,\n",
    "            'uses_smote': needs_smote,\n",
    "            'uses_balancing': uses_internal_balancing,\n",
    "            'loaded_from_file': False\n",
    "        })\n",
    "# --- Start Final Save of All Fold Scores ---\n",
    "if all_fold_scores:\n",
    "    try:\n",
    "        with open(fold_scores_path, 'w') as f:\n",
    "            json.dump(all_fold_scores, f, indent=4)\n",
    "        print(f\"\\n✓ All fold scores have been saved to: {fold_scores_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Error saving all fold scores: {e}\")\n",
    "# --- End Final Save of All Fold Scores ---\n",
    "\n",
    "model_results_df = pd.DataFrame(model_results_list)\n",
    "results_file_path = os.path.join(modeling_results_dir, 'model_selection_results.csv')\n",
    "model_results_df.to_csv(results_file_path, index=False)\n",
    "print(f\"\\n✓ Model selection results saved to: {results_file_path}\")\n",
    "\n",
    "if not model_results_df.empty:\n",
    "    display(model_results_df.sort_values(by=PRIMARY_METRIC_MODEL_SELECTION, ascending=False))\n",
    "else:\n",
    "    print(\"No model results to display.\")\n",
    "\n",
    "if not model_results_df.empty and PRIMARY_METRIC_MODEL_SELECTION in model_results_df.columns:\n",
    "    valid_model_results_df = model_results_df.dropna(subset=[PRIMARY_METRIC_MODEL_SELECTION])\n",
    "    if not valid_model_results_df.empty:\n",
    "        analysis_summary = analyze_model_results(valid_model_results_df, primary_metric=PRIMARY_METRIC_MODEL_SELECTION)\n",
    "        create_results_visualization(valid_model_results_df, analysis_summary, primary_metric=PRIMARY_METRIC_MODEL_SELECTION)\n",
    "\n",
    "        if analysis_summary and 'best_model' in analysis_summary and analysis_summary['best_model']:\n",
    "            best_model_info = analysis_summary['best_model']\n",
    "            best_model_name = best_model_info['name']\n",
    "            print(f\"\\n🏆 Best model from selection: {best_model_name} with {PRIMARY_METRIC_MODEL_SELECTION}: {best_model_info['score']:.4f}\")\n",
    "        else:\n",
    "            best_model_name = None\n",
    "            print(\"\\n⚠ Could not determine best model from selection results (valid_model_results_df was empty or analysis failed).\")\n",
    "    else:\n",
    "        best_model_name = None\n",
    "        analysis_summary = {}\n",
    "        print(f\"\\n⚠ No valid models with primary metric '{PRIMARY_METRIC_MODEL_SELECTION}' found after loading/running. Cannot determine best model.\")\n",
    "elif not model_results_df.empty:\n",
    "    print(f\"\\n⚠ Primary metric '{PRIMARY_METRIC_MODEL_SELECTION}' not found in results. Cannot determine best model.\")\n",
    "    best_model_name = None\n",
    "    analysis_summary = {}\n",
    "else:\n",
    "    print(\"\\n⚠ Model selection produced no results. Cannot determine best model.\")\n",
    "    best_model_name = None\n",
    "    analysis_summary = {}\n",
    "\n",
    "# 5. Hyperparameter Tuning\n",
    "tuned_pipeline = None\n",
    "tuning_results_summary = {}\n",
    "\n",
    "if best_model_name and best_model_name in models_to_evaluate:\n",
    "    print(f\"\\n--- Hyperparameter Tuning for {best_model_name} ---\")\n",
    "    base_model_for_tuning = models_to_evaluate[best_model_name]\n",
    "    param_grids_all = define_parameter_grids()  # Must be defined in a previous cell\n",
    "\n",
    "    if best_model_name not in param_grids_all:\n",
    "        print(f\"⚠ No parameter grid defined for {best_model_name}. Skipping tuning.\")\n",
    "    else:\n",
    "        param_grid_for_model = param_grids_all[best_model_name]\n",
    "\n",
    "        if best_model_name in model_categories['tree_preprocessing']:\n",
    "            preprocessor_ct_tuning = preprocessor_trees_ct\n",
    "        else:\n",
    "            preprocessor_ct_tuning = preprocessor_general_ct\n",
    "\n",
    "        if preprocessor_ct_tuning is None:\n",
    "            print(f\"⚠ Preprocessor for {best_model_name} not available. Skipping tuning.\")\n",
    "        else:\n",
    "            needs_smote_tuning = best_model_name in model_categories.get('needs_smote', [])\n",
    "            stkde_transformer_tuning = STKDEAndRiskLabelTransformer(**stkde_config)\n",
    "\n",
    "            # Adjust parameter grid keys based on whether SMOTE is in the feature_processor pipeline\n",
    "            # The CustomModelPipeline structure is:\n",
    "            # stkde_transformer -> feature_processor (preprocessor_ct -> smote?) -> classifier\n",
    "            # So, RandomizedSearchCV params for classifier need to be prefixed by 'classifier__'\n",
    "            # And params for preprocessor_ct need to be prefixed by 'feature_processor__preprocessor__'\n",
    "            \n",
    "            # Create the feature processing part of the pipeline first\n",
    "            if needs_smote_tuning:\n",
    "                feature_processor_tuning = ImbPipeline([\n",
    "                    ('preprocessor', preprocessor_ct_tuning), # Params: feature_processor__preprocessor__<param>\n",
    "                    ('smote', SMOTE(random_state=42))         # SMOTE usually has no tunable params here\n",
    "                ])\n",
    "            else:\n",
    "                feature_processor_tuning = Pipeline([\n",
    "                    ('preprocessor', preprocessor_ct_tuning) # Params: feature_processor__preprocessor__<param>\n",
    "                ])\n",
    "\n",
    "            # Adjust the keys in param_grid_for_model to match the structure of CustomModelPipeline\n",
    "            # Original param_grids are defined like 'feature_pipeline__classifier__n_estimators'\n",
    "            # We need to change 'feature_pipeline__classifier__' to 'classifier__'\n",
    "            # and 'feature_pipeline__preprocessor__' to 'feature_processor__preprocessor__'\n",
    "            \n",
    "            adjusted_param_grid_for_tuning = {}\n",
    "            for key, value in param_grid_for_model.items():\n",
    "                if key.startswith('feature_pipeline__classifier__'):\n",
    "                    new_key = key.replace('feature_pipeline__classifier__', 'classifier__')\n",
    "                    adjusted_param_grid_for_tuning[new_key] = value\n",
    "                elif key.startswith('feature_pipeline__preprocessor__'): # Assuming preprocessor params might be tuned\n",
    "                    new_key = key.replace('feature_pipeline__preprocessor__', 'feature_processor__preprocessor__')\n",
    "                    adjusted_param_grid_for_tuning[new_key] = value\n",
    "                else:\n",
    "                    # If keys are already simple like 'classifier__n_estimators', keep them\n",
    "                    adjusted_param_grid_for_tuning[key] = value\n",
    "            \n",
    "            print(f\"Adjusted param grid for {best_model_name} tuning: {adjusted_param_grid_for_tuning.keys()}\")\n",
    "\n",
    "\n",
    "            full_tuning_pipeline = CustomModelPipeline(\n",
    "                stkde_transformer=stkde_transformer_tuning,\n",
    "                feature_processor=feature_processor_tuning, # This is now an ImbPipeline or Pipeline\n",
    "                classifier=clone(base_model_for_tuning)   # Params: classifier__<param>\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                print(f\"Starting RandomizedSearchCV for {best_model_name} with {N_ITER_RANDOM_SEARCH} iterations.\")\n",
    "                random_search = RandomizedSearchCV(\n",
    "                    estimator=full_tuning_pipeline,\n",
    "                    param_distributions=adjusted_param_grid_for_tuning, # Use the adjusted grid\n",
    "                    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "                    cv=cv_config['cv_strategy'],\n",
    "                    scoring=PRIMARY_METRIC_TUNING,\n",
    "                    n_jobs=cv_config['n_jobs_cv'], # Use 1 if STKDE is not thread-safe or for debugging\n",
    "                    random_state=42,\n",
    "                    verbose=1,\n",
    "                    error_score='raise' # Make sure errors in CV during search are raised\n",
    "                )\n",
    "                # Fit on y_train_sorted (dummy target), actual target engineered inside pipeline\n",
    "                random_search.fit(cv_config['X_train_sorted'], cv_config['y_train_sorted'])\n",
    "\n",
    "\n",
    "                tuned_pipeline = random_search.best_estimator_\n",
    "                tuning_results_summary = {\n",
    "                    'best_score': random_search.best_score_,\n",
    "                    'best_params': random_search.best_params_,\n",
    "                    'model_name': best_model_name\n",
    "                }\n",
    "\n",
    "                print(f\"✓ Hyperparameter tuning completed for {best_model_name}.\")\n",
    "                print(f\"  Best CV {PRIMARY_METRIC_TUNING} score: {random_search.best_score_:.4f}\")\n",
    "                print(f\"  Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "                tuning_results_file = os.path.join(modeling_results_dir, f\"{best_model_name}_tuning_results.json\")\n",
    "                # Ensure all parts of cv_results_ and best_params_ are JSON serializable\n",
    "                serializable_cv_results = {}\n",
    "                if hasattr(random_search, 'cv_results_'):\n",
    "                    for k_cv, v_cv in random_search.cv_results_.items():\n",
    "                        if isinstance(v_cv, np.ndarray):\n",
    "                            serializable_cv_results[k_cv] = v_cv.tolist()\n",
    "                        elif isinstance(v_cv, list) and v_cv and isinstance(v_cv[0], (np.integer, np.floating, np.bool_)):\n",
    "                             serializable_cv_results[k_cv] = [item.item() if hasattr(item, 'item') else item for item in v_cv] # Convert numpy types in lists\n",
    "                        else:\n",
    "                            serializable_cv_results[k_cv] = v_cv\n",
    "                \n",
    "                serializable_best_params = {}\n",
    "                if random_search.best_params_:\n",
    "                    for k_bp, v_bp in random_search.best_params_.items():\n",
    "                        if isinstance(v_bp, (np.integer, np.floating, np.bool_)):\n",
    "                             serializable_best_params[k_bp] = v_bp.item() # Convert numpy types\n",
    "                        else:\n",
    "                             serializable_best_params[k_bp] = v_bp\n",
    "\n",
    "\n",
    "                with open(tuning_results_file, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'best_score': random_search.best_score_,\n",
    "                        'best_params': serializable_best_params,\n",
    "                        'cv_results': serializable_cv_results\n",
    "                    }, f, indent=2)\n",
    "                print(f\"✓ Tuning results saved to: {tuning_results_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error during hyperparameter tuning for {best_model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc() # Print full traceback for debugging\n",
    "                tuned_pipeline = None # Ensure it's None if tuning fails\n",
    "                tuning_results_summary = {'error': str(e), 'model_name': best_model_name}\n",
    "else:\n",
    "    print(\"\\n⚠ No best model identified or best model not in evaluation list. Skipping hyperparameter tuning.\")\n",
    "\n",
    "# 6. Final Evaluation on Test Set\n",
    "final_test_results = {} # Initialize final_test_results\n",
    "if tuned_pipeline is not None and best_model_name is not None:\n",
    "    print(f\"\\n--- Final Test Set Evaluation for Tuned {best_model_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Refitting the best tuned pipeline on the entire training set (X_train_sorted, y_train_sorted)...\")\n",
    "        # Fit on y_train_sorted (dummy target), actual target engineered inside pipeline\n",
    "        tuned_pipeline.fit(X_train_sorted, y_train_sorted)\n",
    "        print(\"Refit complete.\")\n",
    "\n",
    "        print(\"Making predictions on the test set (X_test)...\")\n",
    "        y_pred_test = tuned_pipeline.predict(X_test)\n",
    "\n",
    "        # To get the actual y_test_engineered, we need to pass X_test through the STKDE transformer part\n",
    "        # of the *fitted* tuned_pipeline.\n",
    "        print(\"Engineering true labels for the test set using the fitted STKDE transformer...\")\n",
    "        if hasattr(tuned_pipeline, 'stkde_transformer') and hasattr(tuned_pipeline.stkde_transformer, 'transform'):\n",
    "            # Ensure X_test is copied to avoid in-place modifications if any\n",
    "            X_test_stkde_aug = tuned_pipeline.stkde_transformer.transform(X_test.copy())\n",
    "            y_test_engineered = X_test_stkde_aug[tuned_pipeline.stkde_transformer.label_col_name]\n",
    "            print(f\"Engineered y_test shape: {y_test_engineered.shape}\")\n",
    "        else:\n",
    "            raise AttributeError(\"Tuned pipeline does not have a 'stkde_transformer' with a 'transform' method.\")\n",
    "\n",
    "        # Ensure y_pred_test and y_test_engineered are aligned and have same length\n",
    "        if len(y_pred_test) != len(y_test_engineered):\n",
    "            print(f\"Warning: Length mismatch between y_pred_test ({len(y_pred_test)}) and y_test_engineered ({len(y_test_engineered)}).\")\n",
    "            # This might indicate that STKDEAndRiskLabelTransformer.transform changed the number of rows in X_test.\n",
    "            # If so, y_pred_test (from the full pipeline) should align with the output of STKDE.\n",
    "            # Assuming y_pred_test is already aligned if the pipeline is consistent.\n",
    "            # If X_test_stkde_aug has a different index, y_pred_test might need re-indexing if it's a Series.\n",
    "            # For now, we proceed, but this is a critical point for data integrity.\n",
    "            # If y_pred_test is a numpy array, it should correspond to X_test_stkde_aug's rows.\n",
    "            if isinstance(y_pred_test, np.ndarray) and isinstance(y_test_engineered, pd.Series):\n",
    "                 if len(y_pred_test) == len(y_test_engineered.index):\n",
    "                     y_pred_test = pd.Series(y_pred_test, index=y_test_engineered.index)\n",
    "                 else:\n",
    "                     print(\"Cannot align y_pred_test (numpy) with y_test_engineered (pandas Series) due to length mismatch after STKDE.\")\n",
    "                     # Handle error or skip evaluation\n",
    "        \n",
    "        y_proba_test_pos = None\n",
    "        if hasattr(tuned_pipeline, \"predict_proba\"):\n",
    "            print(\"Getting probability predictions from the tuned pipeline...\")\n",
    "            y_proba_test_full = tuned_pipeline.predict_proba(X_test) # Probabilities for all classes\n",
    "            if y_proba_test_full.ndim == 2 and y_proba_test_full.shape[1] >= 2:\n",
    "                y_proba_test_pos = y_proba_test_full[:, 1]  # Probability of the positive class (class 1)\n",
    "            elif y_proba_test_full.ndim == 1: # Case for some classifiers or if only one class prob is returned\n",
    "                y_proba_test_pos = y_proba_test_full\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected shape for probability predictions: {y_proba_test_full.shape}\")\n",
    "        else:\n",
    "            print(\"Probability predictions (predict_proba) not available for the tuned pipeline.\")\n",
    "\n",
    "        current_test_metrics = {\n",
    "            'accuracy': accuracy_score(y_test_engineered, y_pred_test),\n",
    "            'f1': f1_score(y_test_engineered, y_pred_test, average='binary', zero_division=0),\n",
    "            'precision': precision_score(y_test_engineered, y_pred_test, average='binary', zero_division=0),\n",
    "            'recall': recall_score(y_test_engineered, y_pred_test, average='binary', zero_division=0),\n",
    "            'mcc': matthews_corrcoef(y_test_engineered, y_pred_test)\n",
    "        }\n",
    "        if y_proba_test_pos is not None:\n",
    "            try:\n",
    "                # Ensure y_proba_test_pos has the same length as y_test_engineered\n",
    "                if len(y_proba_test_pos) == len(y_test_engineered):\n",
    "                    current_test_metrics['roc_auc'] = roc_auc_score(y_test_engineered, y_proba_test_pos)\n",
    "                    current_test_metrics['pr_auc'] = average_precision_score(y_test_engineered, y_proba_test_pos)\n",
    "                    # log_loss needs probabilities for all classes\n",
    "                    current_test_metrics['neg_log_loss'] = log_loss(y_test_engineered, y_proba_test_full)\n",
    "                else:\n",
    "                    print(f\"Warning: Length mismatch for probability scores. y_proba_test_pos: {len(y_proba_test_pos)}, y_test_engineered: {len(y_test_engineered)}\")\n",
    "            except ValueError as e_proba_metrics:\n",
    "                print(f\"Could not calculate probability-based metrics on test set: {e_proba_metrics}\")\n",
    "        \n",
    "        final_test_results.update(current_test_metrics)\n",
    "\n",
    "        print(f\"\\nTest Set Performance for Tuned {best_model_name}:\")\n",
    "        for metric, value in final_test_results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {metric.capitalize()}: {value}\")\n",
    "\n",
    "\n",
    "        test_results_file_path = os.path.join(modeling_results_dir, f\"{best_model_name}_final_test_results.json\")\n",
    "        with open(test_results_file_path, 'w') as f:\n",
    "            # Ensure all values are native Python types for JSON\n",
    "            serializable_final_test_results = {\n",
    "                k_ftr: (v_ftr.item() if hasattr(v_ftr, 'item') else v_ftr) \n",
    "                for k_ftr, v_ftr in final_test_results.items()\n",
    "            }\n",
    "            json.dump(serializable_final_test_results, f, indent=2)\n",
    "        print(f\"✓ Final test results saved to: {test_results_file_path}\")\n",
    "\n",
    "        # Save predictions with original features and engineered target/predictions\n",
    "        # Ensure X_test_stkde_aug (which might have different row count/index than original X_test) is used for joining\n",
    "        # y_test_engineered is already aligned with X_test_stkde_aug\n",
    "        # y_pred_test needs to be aligned if it's not already a Series with the same index\n",
    "        \n",
    "        # Create a base DataFrame from X_test that matches the rows of y_test_engineered\n",
    "        # This assumes y_test_engineered (from X_test_stkde_aug) has an index that can be used to loc original X_test features\n",
    "        if isinstance(y_test_engineered, pd.Series):\n",
    "            aligned_X_test_features = X_test.loc[y_test_engineered.index].copy()\n",
    "        else: # If y_test_engineered is not a Series (e.g. numpy array), this alignment is harder\n",
    "            print(\"Warning: y_test_engineered is not a pandas Series. Cannot reliably align original X_test features for predictions export.\")\n",
    "            aligned_X_test_features = X_test.copy() # Fallback, might not be aligned\n",
    "\n",
    "        predictions_output_df = pd.DataFrame(index=aligned_X_test_features.index)\n",
    "        predictions_output_df['true_risk_level_engineered'] = y_test_engineered\n",
    "        \n",
    "        if isinstance(y_pred_test, pd.Series) and y_pred_test.index.equals(predictions_output_df.index):\n",
    "            predictions_output_df['predicted_risk_level'] = y_pred_test\n",
    "        elif isinstance(y_pred_test, np.ndarray) and len(y_pred_test) == len(predictions_output_df):\n",
    "            predictions_output_df['predicted_risk_level'] = y_pred_test\n",
    "        else:\n",
    "            print(\"Warning: Could not align y_pred_test with the output DataFrame for predictions.\")\n",
    "\n",
    "        if y_proba_test_pos is not None:\n",
    "            if isinstance(y_proba_test_pos, pd.Series) and y_proba_test_pos.index.equals(predictions_output_df.index):\n",
    "                predictions_output_df['predicted_proba_high_risk'] = y_proba_test_pos\n",
    "            elif isinstance(y_proba_test_pos, np.ndarray) and len(y_proba_test_pos) == len(predictions_output_df):\n",
    "                predictions_output_df['predicted_proba_high_risk'] = y_proba_test_pos\n",
    "            else:\n",
    "                 print(\"Warning: Could not align y_proba_test_pos with the output DataFrame for predictions.\")\n",
    "\n",
    "        # Add original features\n",
    "        final_predictions_df_with_features = aligned_X_test_features.join(predictions_output_df)\n",
    "\n",
    "        final_predictions_file_path = os.path.join(modeling_results_dir, f\"{best_model_name}_final_test_predictions_with_features.csv\")\n",
    "        final_predictions_df_with_features.to_csv(final_predictions_file_path, index=True) # Keep index if it's meaningful (e.g. original IDs)\n",
    "        print(f\"✓ Final test predictions with features saved to: {final_predictions_file_path}\")\n",
    "\n",
    "    except Exception as e_final_eval:\n",
    "        print(f\"❌ Error during final test set evaluation for {best_model_name}: {e_final_eval}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # final_test_results might be partially populated or empty\n",
    "        if not final_test_results: # Ensure it's a dict even on error\n",
    "            final_test_results = {'error': str(e_final_eval), 'model_name': best_model_name}\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠ Cannot perform final test set evaluation: tuned pipeline or best model name is not available.\")\n",
    "    if not final_test_results: # Ensure it's a dict\n",
    "         final_test_results = {}\n",
    "\n",
    "\n",
    "# 7. Generate Final Summary Report\n",
    "print(\"\\n--- Generating Final Summary Report ---\")\n",
    "create_final_summary_report(\n",
    "    model_results_df=model_results_df if 'model_results_df' in locals() else pd.DataFrame(),\n",
    "    final_test_results=final_test_results if 'final_test_results' in locals() else {}, # Use the potentially updated final_test_results\n",
    "    best_model_name=best_model_name if 'best_model_name' in locals() and best_model_name is not None else 'Unknown',\n",
    "    results_dir=modeling_results_dir\n",
    ")\n",
    "\n",
    "print(\"\\n=== Modeling Workflow Completed ===\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
