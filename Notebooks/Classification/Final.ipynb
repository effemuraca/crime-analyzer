{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4697545e",
   "metadata": {},
   "source": [
    "# Final Model Training, Evaluation, and Deployment\n",
    "\n",
    "This notebook represents the final phase of the classification project. It takes the best components identified in previous stages (preprocessing, feature selection, hyperparameters) to train a final production-ready model. The process includes a comprehensive evaluation on the test set, diagnostic analysis to understand model behavior, and the saving of all necessary artifacts for deployment and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c7aa1",
   "metadata": {},
   "source": [
    "# Setup and Initialization\n",
    "\n",
    "This section handles the initial setup, imports, and loading of artifacts from previous phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227aa94",
   "metadata": {},
   "source": [
    "## Optional: Google Colab Setup\n",
    "\n",
    "Uncomment and run this cell if working in Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12268949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Google Colab (optional)\n",
    "#from google.colab import drive\n",
    "#drive.mount('/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6bd56d",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all necessary libraries for advanced tuning, model interpretation, and final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257a4e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "# Core ML libraries\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    learning_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, f1_score, accuracy_score, precision_score, recall_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve,\n",
    "    average_precision_score, precision_recall_curve, matthews_corrcoef,\n",
    "    classification_report, log_loss, fbeta_score\n",
    ")\n",
    "\n",
    "# Model interpretation libraries\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Imbalanced learning and ensemble libraries\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.ensemble import (\n",
    "    EasyEnsembleClassifier, BalancedRandomForestClassifier, BalancedBaggingClassifier\n",
    ")\n",
    "\n",
    "# Custom utilities from the project\n",
    "from Utilities.custom_transformers import STKDEAndRiskLabelTransformer, SlidingWindowSplit, CyclicalTransformer\n",
    "\n",
    "# Models\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Set random seeds and display settings for reproducibility and clarity\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e988d",
   "metadata": {},
   "source": [
    "## Define Paths and Directories\n",
    "\n",
    "Set up directory structure for loading previous results and saving final outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47977ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Structure:\n",
      "   Preprocessing artifacts: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Preprocessing)\n",
      "   Modeling results: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Modeling)\n",
      "   Tuning results: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Tuning)\n",
      "   Final outputs: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Final)\n",
      "\n",
      "All required directories found.\n"
     ]
    }
   ],
   "source": [
    "# Define base directories\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"JupyterOutputs\"))\n",
    "preprocessing_dir = os.path.join(base_dir, \"Classification (Preprocessing)\")\n",
    "modeling_dir = os.path.join(base_dir, \"Classification (Modeling)\")\n",
    "tuning_dir = os.path.join(base_dir, \"Classification (Tuning)\")\n",
    "final_dir = os.path.join(base_dir, \"Classification (Final)\")\n",
    "\n",
    "# Create the directory for final results if it doesn't exist\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Directory Structure:\")\n",
    "print(f\"   Preprocessing artifacts: {preprocessing_dir}\")\n",
    "print(f\"   Modeling results: {modeling_dir}\")\n",
    "print(f\"   Tuning results: {tuning_dir}\")\n",
    "print(f\"   Final outputs: {final_dir}\")\n",
    "\n",
    "# Verify that all required input directories exist\n",
    "required_dirs = [preprocessing_dir, modeling_dir, tuning_dir]\n",
    "missing_dirs = [d for d in required_dirs if not os.path.exists(d)]\n",
    "\n",
    "if missing_dirs:\n",
    "    raise FileNotFoundError(f\"Required directories not found: {missing_dirs}. Please run previous notebooks.\")\n",
    "else:\n",
    "    print(\"\\nAll required directories found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f198675",
   "metadata": {},
   "source": [
    "## Load Previous Phase Artifacts\n",
    "\n",
    "Load data, models, and results from previous phases to continue with advanced tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55441b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified best model from tuning phase: LogisticRegression\n",
      "\n",
      "=== Loading Final Phase Artifacts ===\n",
      "Loaded pipeline components from: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Tuning)\\LogisticRegression_final_pipeline.joblib\n",
      "Loaded best_params from: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Tuning)\\LogisticRegression_best_params.json\n",
      "Loaded optimal_threshold from: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Tuning)\\LogisticRegression_optimal_threshold.json\n",
      "Loaded X_train from: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Tuning)\\X_train.pkl\n",
      "Loaded X_test from: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Tuning)\\X_test.pkl\n",
      "Loaded stkde_params from: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Preprocessing)\\stkde_optimal_params.json\n",
      "\n",
      "Successfully loaded all artifacts.\n",
      "X_train shape: (570076, 33), X_test shape: (144344, 33)\n",
      "Optimal threshold: 0.64\n"
     ]
    }
   ],
   "source": [
    "def load_final_phase_artifacts(preprocessing_dir: str, tuning_dir: str, best_model_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load final phase artifacts with comprehensive error handling.\"\"\"\n",
    "    print(\"=== Loading Final Phase Artifacts ===\")\n",
    "    if not os.path.exists(tuning_dir):\n",
    "        raise FileNotFoundError(f\"Tuning results directory not found: {tuning_dir}\")\n",
    "\n",
    "    artifacts = {}\n",
    "    paths = {\n",
    "        'pipeline': os.path.join(tuning_dir, f\"{best_model_name}_final_pipeline.joblib\"),\n",
    "        'best_params': os.path.join(tuning_dir, f\"{best_model_name}_best_params.json\"),\n",
    "        'optimal_threshold': os.path.join(tuning_dir, f\"{best_model_name}_optimal_threshold.json\"),\n",
    "        'X_train': os.path.join(tuning_dir, \"X_train.pkl\"),\n",
    "        'X_test': os.path.join(tuning_dir, \"X_test.pkl\"),\n",
    "        'stkde_params': os.path.join(preprocessing_dir, \"stkde_optimal_params.json\")\n",
    "    }\n",
    "\n",
    "    # Load pipeline components\n",
    "    try:\n",
    "        pipeline_components = joblib.load(paths['pipeline'])\n",
    "        artifacts['preprocessor'] = pipeline_components.get('preprocessor')\n",
    "        artifacts['feature_selector'] = pipeline_components.get('feature_selector')\n",
    "        print(f\"Loaded pipeline components from: {paths['pipeline']}\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Failed to load pipeline components from {paths['pipeline']}: {e}\")\n",
    "\n",
    "    # Load other JSON and Pickle files\n",
    "    for key, path in paths.items():\n",
    "        if key == 'pipeline': continue\n",
    "        try:\n",
    "            if path.endswith('.json'):\n",
    "                with open(path, \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "                    if key == 'optimal_threshold':\n",
    "                        artifacts[key] = data.get('optimal_threshold')\n",
    "                    else:\n",
    "                        artifacts[key] = data\n",
    "            elif path.endswith('.pkl'):\n",
    "                artifacts[key] = pd.read_pickle(path)\n",
    "            print(f\"Loaded {key} from: {path}\")\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Failed to load {key} from {path}: {e}\")\n",
    "            \n",
    "    artifacts['feature_names'] = list(artifacts['X_train'].columns)\n",
    "    return artifacts\n",
    "\n",
    "try:\n",
    "    # Programmatically load the best model's name from the tuning phase\n",
    "    best_model_info_path = os.path.join(tuning_dir, \"best_model_info.json\")\n",
    "    with open(best_model_info_path, \"r\") as f:\n",
    "        best_model_info = json.load(f)\n",
    "        best_model_name = best_model_info.get(\"best_model_name\")\n",
    "    print(f\"Identified best model from tuning phase: {best_model_name}\\n\")\n",
    "\n",
    "    artifacts = load_final_phase_artifacts(preprocessing_dir, tuning_dir, best_model_name)\n",
    "    X_train = artifacts['X_train']\n",
    "    X_test = artifacts['X_test']\n",
    "    feature_names = artifacts['feature_names']\n",
    "    preprocessor = artifacts['preprocessor']\n",
    "    feature_selector = artifacts['feature_selector']\n",
    "    best_params = artifacts['best_params']\n",
    "    optimal_threshold = artifacts['optimal_threshold']\n",
    "    stkde_params = artifacts['stkde_params']\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded all artifacts.\")\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    print(f\"Optimal threshold: {optimal_threshold}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during artifact loading: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ae882",
   "metadata": {},
   "source": [
    "## Final y_train and y_test Calculation\n",
    "Calculate the final labels for training and testing datasets based on the best model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea63b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Target Engineering (Using Sliding Window Logic) ===\n",
      "Loaded training window size: 162878\n",
      "\n",
      "Selecting the last 162878 samples from sorted training data for final training.\n",
      "Initialized STKDEAndRiskLabelTransformer with hs=250, ht=75\n",
      "\n",
      "Fitting transformer on the final training window to determine final risk threshold...\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Final Target Engineering (Using Sliding Window Logic) ===\")\n",
    "\n",
    "# Load the window size determined during the modeling phase\n",
    "window_config_path = os.path.join(modeling_dir, 'window_config.json')\n",
    "if not os.path.exists(window_config_path):\n",
    "    raise FileNotFoundError(f\"Window configuration file not found at {window_config_path}. Please re-run Modeling.ipynb.\")\n",
    "\n",
    "with open(window_config_path, 'r') as f:\n",
    "    window_config = json.load(f)\n",
    "    train_window_size = window_config.get('train_window_size')\n",
    "    print(f\"Loaded training window size: {train_window_size}\")\n",
    "\n",
    "# --- SLIDING WINDOW LOGIC APPLIED HERE (DATA LEAKAGE PREVENTION) ---\n",
    "# 1. Select only the most recent data for the final training set.\n",
    "print(f\"\\nSelecting the last {train_window_size} samples from sorted training data for final training.\")\n",
    "X_train_final_window = X_train.sort_values(by=['YEAR', 'MONTH', 'DAY', 'HOUR']).tail(train_window_size)\n",
    "\n",
    "# 2. Initialize the STKDE transformer with optimal parameters.\n",
    "hs_optimal = stkde_params.get('hs_opt')\n",
    "ht_optimal = stkde_params.get('ht_opt')\n",
    "stkde_transformer = STKDEAndRiskLabelTransformer(\n",
    "    hs=hs_optimal, ht=ht_optimal, threshold_strategy='dynamic_jenks', n_classes=2\n",
    ")\n",
    "print(f\"Initialized STKDEAndRiskLabelTransformer with hs={hs_optimal}, ht={ht_optimal}\")\n",
    "\n",
    "# 3. Fit the transformer ONLY on the final training window and transform it to get final training labels.\n",
    "# This is a critical step to prevent data leakage. The risk threshold is determined solely from this recent data window.\n",
    "print(\"\\nFitting transformer on the final training window to determine final risk threshold...\")\n",
    "dummy_y_train = pd.Series(0, index=X_train_final_window.index) # Dummy y is required by fit_transform signature\n",
    "_, y_train_engineered = stkde_transformer.fit_transform(X_train_final_window, dummy_y_train)\n",
    "\n",
    "print(\"Final threshold determined and y_train_engineered created.\")\n",
    "print(f\"  Shape of y_train_engineered: {y_train_engineered.shape}\")\n",
    "print(f\"  Class distribution in final y_train_engineered:\\n{y_train_engineered.value_counts(normalize=True).to_string()}\")\n",
    "\n",
    "# 4. Use the FITTED transformer to transform the test data.\n",
    "# The transformer now applies the threshold learned from the training window to the test set.\n",
    "print(\"\\nTransforming the test set using the window-fitted transformer...\")\n",
    "dummy_y_test = pd.Series(0, index=X_test.index)\n",
    "_, y_test_engineered = stkde_transformer.transform(X_test, dummy_y_test)\n",
    "\n",
    "print(\"y_test_engineered created.\")\n",
    "print(f\"  Shape of y_test_engineered: {y_test_engineered.shape}\")\n",
    "print(f\"  Class distribution in final y_test_engineered:\\n{y_test_engineered.value_counts(normalize=True).to_string()}\")\n",
    "\n",
    "# IMPORTANT: Overwrite X_train to be used in subsequent cells for model fitting.\n",
    "X_train = X_train_final_window\n",
    "print(f\"\\nUpdated X_train to be the final window of shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54531c0",
   "metadata": {},
   "source": [
    "# 3. Production Model Training and Evaluation\n",
    "\n",
    "**Goal:** Train the final production model and comprehensively evaluate its performance on the unseen test set.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Pipeline Assembly and Training:** The full production pipeline is assembled, combining the preprocessor, feature selector, and the classifier with its optimal hyperparameters. The pipeline is then trained (`.fit()`) using the final training data (`X_train_final_window` and `y_train_engineered`).\n",
    "2.  **Test Set Evaluation:** The trained model predicts probabilities on `X_test`. These are converted to binary classes using the pre-determined `optimal_threshold`. A comprehensive evaluation report is generated, including key metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_production_model(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    preprocessor: Pipeline,\n",
    "    feature_selector: BaseEstimator,\n",
    "    classifier_class: BaseEstimator,\n",
    "    best_params: Dict[str, Any],\n",
    "    optimal_threshold: float\n",
    ") -> Tuple[ImbPipeline, Dict[str, Any], plt.Figure]:\n",
    "    \"\"\"Builds, trains, and evaluates the final production model.\"\"\"\n",
    "    print(\"=== Production Model Training and Evaluation ===\")\n",
    "\n",
    "    # 1. Assemble the full production pipeline\n",
    "    print(\"Assembling the production pipeline...\")\n",
    "    # Define base parameters for Logistic Regression\n",
    "    if classifier_class.__name__ == 'LogisticRegression':\n",
    "        base_params = {\n",
    "            'random_state': 42,\n",
    "            'max_iter': 1000,\n",
    "            'class_weight': 'balanced',\n",
    "            'n_jobs': -1,\n",
    "            'solver': 'liblinear'\n",
    "        }\n",
    "    else:\n",
    "        base_params = {'random_state': 42}\n",
    "\n",
    "    # Clean parameter keys from the tuning phase and update base parameters\n",
    "    tuned_params = {k.split('__', 1)[1]: v for k, v in best_params.items()}\n",
    "    final_params = {**base_params, **tuned_params}\n",
    "    \n",
    "    print(f\"Final classifier parameters: {final_params}\")\n",
    "    classifier = classifier_class(**final_params)\n",
    "    \n",
    "    production_model = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    # 2. Train the production model on the final training window\n",
    "    print(f\"Training the final model on {X_train.shape[0]} samples...\")\n",
    "    start_time = time.time()\n",
    "    production_model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # 3. Evaluate on the test set using the optimal threshold\n",
    "    print(\"\\nEvaluating the model on the test set...\")\n",
    "    y_pred_proba = production_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "    # 4. Generate a comprehensive evaluation report\n",
    "    evaluation_report = {\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"f2_score\": fbeta_score(y_test, y_pred, beta=2), \n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"mcc\": matthews_corrcoef(y_test, y_pred),\n",
    "        \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "        \"optimal_threshold\": optimal_threshold,\n",
    "        \"training_duration_seconds\": end_time - start_time\n",
    "    }\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for metric, value in evaluation_report.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "    # 5. Create a visual evaluation dashboard\n",
    "    print(\"\\nGenerating evaluation dashboard...\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 7))\n",
    "    fig.suptitle('Production Model Evaluation Dashboard', fontsize=16)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm).plot(ax=axes[0], cmap='Blues')\n",
    "    axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    axes[1].plot(fpr, tpr, label=f\"ROC AUC = {evaluation_report['roc_auc']:.4f}\")\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', label='Chance Level')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    ap = average_precision_score(y_test, y_pred_proba)\n",
    "    axes[2].plot(recall, precision, label=f'AP = {ap:.4f}')\n",
    "    axes[2].set_title('Precision-Recall Curve')\n",
    "    axes[2].set_xlabel('Recall')\n",
    "    axes[2].set_ylabel('Precision')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    return production_model, evaluation_report, fig\n",
    "\n",
    "# Execute Model Training and Evaluation\n",
    "model_class_map = {'LogisticRegression': LogisticRegression}\n",
    "classifier_class = model_class_map.get(best_model_name)\n",
    "\n",
    "if classifier_class:\n",
    "    production_model, evaluation_report, evaluation_dashboard_fig = train_and_evaluate_production_model(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train_engineered,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test_engineered,\n",
    "        preprocessor=preprocessor,\n",
    "        feature_selector=feature_selector,\n",
    "        classifier_class=classifier_class,\n",
    "        best_params=best_params,\n",
    "        optimal_threshold=optimal_threshold\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model class for '{best_model_name}' not found. Skipping training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376ba2d",
   "metadata": {},
   "source": [
    "# 4. Diagnostic and Interpretability Analysis\n",
    "\n",
    "**Goal:** Perform an in-depth analysis of the trained model to understand its behavior, limitations, and decision-making process.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Learning Curves:** Analyze model performance as a function of training set size. This helps diagnose high variance (overfitting) or high bias (underfitting) and assess whether more data would be beneficial. A `TimeSeriesSplit` is used for cross-validation to respect the temporal order of the data.\n",
    "2.  **SHAP Analysis:** Use SHAP (SHapley Additive exPlanations) to explain the model's predictions. This reveals the \"why\" behind its decisions by quantifying the impact of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_diagnostic_and_shap_analysis(\n",
    "    production_model: ImbPipeline,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series\n",
    ") -> Tuple[\n",
    "    Optional[plt.Figure], Optional[plt.Figure], Optional[plt.Figure],\n",
    "    Optional[plt.Figure], Optional[plt.Figure]\n",
    "]:\n",
    "    \"\"\"Performs diagnostic (learning curves) and interpretability (SHAP) analysis.\"\"\"\n",
    "    print(\"\\n=== Diagnostic and Interpretability Analysis ===\")\n",
    "\n",
    "    # 1. Diagnostic Analysis: Learning Curves\n",
    "    print(\"Generating learning curves...\")\n",
    "    learning_curve_fig = None\n",
    "    try:\n",
    "        X_combined = pd.concat([X_train, X_test]).sort_index()\n",
    "        y_combined = pd.concat([y_train, y_test]).sort_index()\n",
    "        n_samples_total = len(X_combined)\n",
    "        n_splits_cv = 5\n",
    "        initial_train_fraction = 0.5\n",
    "        lc_train_size = int(n_samples_total * initial_train_fraction)\n",
    "        remaining_samples = n_samples_total - lc_train_size\n",
    "        lc_test_size = remaining_samples // n_splits_cv\n",
    "\n",
    "        print(f\"Total samples for learning curve: {n_samples_total}\")\n",
    "        print(f\"Dynamically calculated sizes for SlidingWindowSplit: train_size={lc_train_size}, test_size={lc_test_size}, n_splits={n_splits_cv}\")\n",
    "\n",
    "        if lc_train_size <= 0 or lc_test_size <= 0:\n",
    "            raise ValueError(\"Calculated train or test size is zero. Not enough data for learning curves.\")\n",
    "\n",
    "        sliding_cv = SlidingWindowSplit(\n",
    "            n_splits=n_splits_cv,\n",
    "            train_size=lc_train_size,\n",
    "            test_size=lc_test_size\n",
    "        )\n",
    "\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            estimator=clone(production_model),\n",
    "            X=X_combined,\n",
    "            y=y_combined,\n",
    "            cv=sliding_cv,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            train_sizes=np.linspace(0.2, 1.0, n_splits_cv)\n",
    "        )\n",
    "\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "        learning_curve_fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        ax.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "        ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        ax.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "        ax.set_title(\"Learning Curves (SlidingWindowSplit)\")\n",
    "        ax.set_xlabel(\"Training examples\")\n",
    "        ax.set_ylabel(\"F1 Score\")\n",
    "        ax.legend(loc=\"best\")\n",
    "        ax.grid(True)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate learning curves: {e}\")\n",
    "\n",
    "    # 2. Interpretability Analysis with SHAP\n",
    "    print(\"\\nPerforming SHAP analysis...\")\n",
    "    shap_importance_fig, shap_summary_fig = None, None\n",
    "    shap_dependence_fig, shap_force_fig = None, None\n",
    "    try:\n",
    "        preprocessor_step = production_model.named_steps['preprocessor']\n",
    "        selector_step = production_model.named_steps['feature_selector']\n",
    "        classifier_step = production_model.named_steps['classifier']\n",
    "\n",
    "        X_train_sample = X_train.sample(n=min(1000, len(X_train)), random_state=42)\n",
    "        X_train_transformed = preprocessor_step.transform(X_train_sample)\n",
    "        X_train_selected = selector_step.transform(X_train_transformed)\n",
    "\n",
    "        transformed_names = preprocessor_step.get_feature_names_out()\n",
    "        selected_indices = selector_step.get_support(indices=True)\n",
    "        selected_feature_names = [transformed_names[i] for i in selected_indices]\n",
    "        X_train_selected_df = pd.DataFrame(X_train_selected, columns=selected_feature_names)\n",
    "\n",
    "        explainer = shap.LinearExplainer(classifier_step, X_train_selected_df)\n",
    "        shap_values = explainer.shap_values(X_train_selected_df)\n",
    "\n",
    "        # SHAP Feature Importance Plot\n",
    "        print(\"Generating SHAP feature importance plot...\")\n",
    "        shap_importance_fig, ax_imp = plt.subplots()\n",
    "        shap.summary_plot(shap_values, X_train_selected_df, plot_type=\"bar\", show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # SHAP Summary (Beeswarm) Plot\n",
    "        print(\"Generating SHAP summary (beeswarm) plot...\")\n",
    "        shap_summary_fig, ax_sum = plt.subplots()\n",
    "        shap.summary_plot(shap_values, X_train_selected_df, show=False)\n",
    "        plt.title(\"SHAP Summary Plot\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # SHAP Dependence Plot (for the most important feature)\n",
    "        print(\"Generating SHAP dependence plot for the top feature...\")\n",
    "        top_feature = X_train_selected_df.columns[np.argmax(np.abs(shap_values).mean(axis=0))]\n",
    "        shap_dependence_fig, ax_dep = plt.subplots()\n",
    "        shap.dependence_plot(top_feature, shap_values, X_train_selected_df, show=False, ax=ax_dep)\n",
    "        plt.title(f\"SHAP Dependence Plot: {top_feature}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # SHAP Force Plot (for the first sample)\n",
    "        print(\"Generating SHAP force plot for the first sample...\")\n",
    "        shap.initjs()\n",
    "        shap_force_fig = shap.force_plot(\n",
    "            explainer.expected_value, shap_values[0, :], X_train_selected_df.iloc[0, :], matplotlib=True, show=False\n",
    "        )\n",
    "        plt.title(\"SHAP Force Plot (First Sample)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform SHAP analysis: {e}\")\n",
    "\n",
    "    return learning_curve_fig, shap_importance_fig, shap_summary_fig, shap_dependence_fig, shap_force_fig\n",
    "\n",
    "# Execute Phase 4\n",
    "if 'production_model' in locals():\n",
    "    learning_curve_fig, shap_importance_fig, shap_summary_fig, shap_dependence_fig, shap_force_fig = perform_diagnostic_and_shap_analysis(\n",
    "        production_model=production_model,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train_engineered,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test_engineered\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping Phase 4 because the production model was not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f2867",
   "metadata": {},
   "source": [
    "# 5. Final Saving of Production Artifacts\n",
    "\n",
    "**Goal:** Save all necessary elements for reproducibility, documentation, and deployment in a structured manner.\n",
    "\n",
    "**Artifacts Saved:**\n",
    "1.  **Production Model (`..._production_model.joblib`):** The complete, trained pipeline object. This is the primary deployment artifact.\n",
    "2.  **Evaluation Report (`..._evaluation_report.json`):** A JSON file containing all test set performance metrics.\n",
    "3.  **Evaluation Dashboard (`..._evaluation_dashboard.png`):** A composite image of the confusion matrix, ROC curve, and Precision-Recall curve.\n",
    "4.  **Diagnostic & SHAP Plots (`..._learning_curve.png`, `..._shap_*.png`):** Visualizations from the diagnostic and interpretability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_production_artifacts(\n",
    "    final_dir: str,\n",
    "    model_name: str,\n",
    "    production_model: ImbPipeline,\n",
    "    evaluation_report: Dict[str, Any],\n",
    "    evaluation_dashboard_fig: plt.Figure,\n",
    "    learning_curve_fig: Optional[plt.Figure],\n",
    "    shap_importance_fig: Optional[plt.Figure],\n",
    "    shap_summary_fig: Optional[plt.Figure],\n",
    "    shap_dependence_fig: Optional[plt.Figure],\n",
    "    shap_force_fig: Optional[plt.Figure]\n",
    "):\n",
    "    \"\"\"Saves all final production artifacts to the specified directory.\"\"\"\n",
    "    print(\"\\n=== Saving Final Production Artifacts ===\")\n",
    "    \n",
    "    # 1. Production Model\n",
    "    model_path = os.path.join(final_dir, f\"{model_name}_production_model.joblib\")\n",
    "    joblib.dump(production_model, model_path)\n",
    "    print(f\"Saved production model to: {model_path}\")\n",
    "\n",
    "    # 2. Evaluation Report\n",
    "    report_path = os.path.join(final_dir, f\"{model_name}_evaluation_report.json\")\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(evaluation_report, f, indent=4)\n",
    "    print(f\"Saved evaluation report to: {report_path}\")\n",
    "\n",
    "    # 3. Evaluation Dashboard\n",
    "    dashboard_path = os.path.join(final_dir, f\"{model_name}_evaluation_dashboard.png\")\n",
    "    evaluation_dashboard_fig.savefig(dashboard_path, bbox_inches='tight')\n",
    "    print(f\"Saved evaluation dashboard to: {dashboard_path}\")\n",
    "\n",
    "    # 4. Diagnostic Reports\n",
    "    if learning_curve_fig:\n",
    "        lc_path = os.path.join(final_dir, f\"{model_name}_learning_curve.png\")\n",
    "        learning_curve_fig.savefig(lc_path, bbox_inches='tight')\n",
    "        print(f\"Saved learning curve plot to: {lc_path}\")\n",
    "\n",
    "    # 5. SHAP Analysis Plots\n",
    "    if shap_importance_fig:\n",
    "        shap_imp_path = os.path.join(final_dir, f\"{model_name}_shap_feature_importance.png\")\n",
    "        shap_importance_fig.savefig(shap_imp_path, bbox_inches='tight')\n",
    "        print(f\"Saved SHAP feature importance plot to: {shap_imp_path}\")\n",
    "\n",
    "    if shap_summary_fig:\n",
    "        shap_sum_path = os.path.join(final_dir, f\"{model_name}_shap_summary.png\")\n",
    "        shap_summary_fig.savefig(shap_sum_path, bbox_inches='tight')\n",
    "        print(f\"Saved SHAP summary plot to: {shap_sum_path}\")\n",
    "\n",
    "    if shap_dependence_fig:\n",
    "        shap_dep_path = os.path.join(final_dir, f\"{model_name}_shap_dependence.png\")\n",
    "        shap_dependence_fig.savefig(shap_dep_path, bbox_inches='tight')\n",
    "        print(f\"Saved SHAP dependence plot to: {shap_dep_path}\")\n",
    "\n",
    "    if shap_force_fig:\n",
    "        shap_force_path = os.path.join(final_dir, f\"{model_name}_shap_force.png\")\n",
    "        shap_force_fig.savefig(shap_force_path, bbox_inches='tight')\n",
    "        print(f\"Saved SHAP force plot to: {shap_force_path}\")\n",
    "\n",
    "    print(\"\\nAll production artifacts have been saved successfully.\")\n",
    "    \n",
    "# Execute Phase 5\n",
    "if 'production_model' in locals():\n",
    "    save_production_artifacts(\n",
    "        final_dir=final_dir,\n",
    "        model_name=best_model_name,\n",
    "        production_model=production_model,\n",
    "        evaluation_report=evaluation_report,\n",
    "        evaluation_dashboard_fig=evaluation_dashboard_fig,\n",
    "        learning_curve_fig=learning_curve_fig,\n",
    "        shap_importance_fig=shap_importance_fig,\n",
    "        shap_summary_fig=shap_summary_fig,\n",
    "        shap_dependence_fig=shap_dependence_fig,\n",
    "        shap_force_fig=shap_force_fig\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping Phase 5 because no production model was trained.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
