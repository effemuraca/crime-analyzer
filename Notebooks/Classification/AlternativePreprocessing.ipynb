{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfb93ca",
   "metadata": {
    "id": "bdfb93ca",
    "jp-MarkdownHeadingCollapsed": true,
    "language": "markdown"
   },
   "source": [
    "# Alternative Preprocessing Strategy for Classification\n",
    "\n",
    "This notebook implements a robust preprocessing strategy for classification, with particular attention to preventing data leakage and ensuring reproducibility.\n",
    "\n",
    "## Key Principles\n",
    "- The train/test split is performed before any transformation.\n",
    "- STKDE parameters are optimized only on the training data.\n",
    "- All feature/label engineering is postponed to the modeling phase.\n",
    "- Preprocessing pipelines are defined here but fitted only on the training data during modeling.\n",
    "\n",
    "**Note:**\n",
    "- All custom functions (e.g., cyclical_transform, BinarizeSinCosTransformer) are defined in `custom_transformers.py` to ensure modularity and reusability.\n",
    "- The produced artifacts (data, pipeline, parameters, scoring_dict) are used by `Modeling.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5576c",
   "metadata": {
    "id": "2eb5576c"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Import libraries, define paths, and prepare for preprocessing. All custom functions are imported from `custom_transformers.py` where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e431564",
   "metadata": {
    "id": "1e431564"
   },
   "source": [
    "## Google Drive Mount (optional)\n",
    "\n",
    "If working locally, this cell can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f45ce99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18195,
     "status": "ok",
     "timestamp": 1747475125719,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "6f45ce99",
    "outputId": "df9beba5-eee7-497c-caff-7fb21e7ae09d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01985d90",
   "metadata": {
    "id": "01985d90"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d12be78",
   "metadata": {
    "executionInfo": {
     "elapsed": 7198,
     "status": "ok",
     "timestamp": 1747475132911,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "0d12be78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and random seeds set.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, FunctionTransformer, StandardScaler, KBinsDiscretizer, Binarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import random\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.neighbors import KDTree\n",
    "from pyproj import Transformer\n",
    "from Utilities.custom_transformers import cyclical_transform\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "print(\"Libraries imported and random seeds set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ac08b",
   "metadata": {
    "id": "fc3ac08b"
   },
   "source": [
    "## Path Definition\n",
    "\n",
    "Define paths for loading data and saving preprocessing artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529f9d74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1747475133393,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "529f9d74",
    "outputId": "d749983b-c4b2-4d4d-ce07-2d9d3a5a4861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\n",
      "Feature engineered file path: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Final\\final_crime_data.csv\n",
      "Save directory: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Preprocessing)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = r\"C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\"\n",
    "feature_engineered_file_path = os.path.join(base_dir, \"Final\", \"final_crime_data.csv\")\n",
    "save_dir = os.path.join(base_dir, \"Classification (Preprocessing)\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Notebook directory: {base_dir}\")\n",
    "print(f\"Feature engineered file path: {feature_engineered_file_path}\")\n",
    "print(f\"Save directory: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228ef6b",
   "metadata": {
    "id": "3228ef6b"
   },
   "source": [
    "### Load and validate feature engineered data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff262e2",
   "metadata": {
    "id": "1ff262e2"
   },
   "source": [
    "Load the dataset produced by the initial feature engineering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4d8f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: 2493835 rows, 44 columns\n",
      "Columns: ['BORO_NM', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT', 'SCHOOLS_COUNT', 'METRO_DISTANCE', 'MIN_POI_DISTANCE', 'AVG_POI_DISTANCE', 'MAX_POI_DISTANCE', 'TOTAL_POI_COUNT', 'POI_DIVERSITY', 'POI_DENSITY_SCORE', 'HOUR', 'DAY', 'WEEKDAY', 'IS_WEEKEND', 'MONTH', 'YEAR', 'SEASON', 'TIME_BUCKET', 'IS_HOLIDAY', 'IS_PAYDAY', 'SAME_AGE_GROUP', 'SAME_SEX', 'TO_CHECK_CITIZENS']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2493835 entries, 0 to 2493834\n",
      "Data columns (total 44 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   BORO_NM             object \n",
      " 1   KY_CD               int64  \n",
      " 2   LAW_CAT_CD          object \n",
      " 3   LOC_OF_OCCUR_DESC   object \n",
      " 4   OFNS_DESC           object \n",
      " 5   PD_CD               int64  \n",
      " 6   PREM_TYP_DESC       object \n",
      " 7   SUSP_AGE_GROUP      object \n",
      " 8   SUSP_RACE           object \n",
      " 9   SUSP_SEX            object \n",
      " 10  VIC_AGE_GROUP       object \n",
      " 11  VIC_RACE            object \n",
      " 12  VIC_SEX             object \n",
      " 13  Latitude            float64\n",
      " 14  Longitude           float64\n",
      " 15  BAR_DISTANCE        float64\n",
      " 16  NIGHTCLUB_DISTANCE  float64\n",
      " 17  ATM_DISTANCE        float64\n",
      " 18  ATMS_COUNT          float64\n",
      " 19  BARS_COUNT          float64\n",
      " 20  BUS_STOPS_COUNT     float64\n",
      " 21  METROS_COUNT        float64\n",
      " 22  NIGHTCLUBS_COUNT    float64\n",
      " 23  SCHOOLS_COUNT       float64\n",
      " 24  METRO_DISTANCE      float64\n",
      " 25  MIN_POI_DISTANCE    float64\n",
      " 26  AVG_POI_DISTANCE    float64\n",
      " 27  MAX_POI_DISTANCE    float64\n",
      " 28  TOTAL_POI_COUNT     float64\n",
      " 29  POI_DIVERSITY       int64  \n",
      " 30  POI_DENSITY_SCORE   float64\n",
      " 31  HOUR                int64  \n",
      " 32  DAY                 int64  \n",
      " 33  WEEKDAY             object \n",
      " 34  IS_WEEKEND          int64  \n",
      " 35  MONTH               int64  \n",
      " 36  YEAR                int64  \n",
      " 37  SEASON              object \n",
      " 38  TIME_BUCKET         object \n",
      " 39  IS_HOLIDAY          int64  \n",
      " 40  IS_PAYDAY           int64  \n",
      " 41  SAME_AGE_GROUP      int64  \n",
      " 42  SAME_SEX            int64  \n",
      " 43  TO_CHECK_CITIZENS   int64  \n",
      "dtypes: float64(17), int64(13), object(14)\n",
      "memory usage: 837.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_basic_data_info(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(df.info())\n",
    "    return df\n",
    "\n",
    "df = load_basic_data_info(feature_engineered_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e37dd",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "- Remove irrelevant rows (`TO_CHECK_CITIZENS` = 0)\n",
    "- Remove rows where `VIC_SEX` is not in ['M', 'F']\n",
    "- Remove rows with unknown location\n",
    "- Remove columns not available at prediction time\n",
    "\n",
    "These steps ensure consistency with operational conditions and reliability of performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5034f239",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1953,
     "status": "ok",
     "timestamp": 1747475156233,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "5034f239",
    "outputId": "b86b8ef8-55e1-4713-afcc-b79adb98144d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Cleaning ===\n",
      "Removed 225707 rows where TO_CHECK_CITIZENS = 0\n",
      "Removed 577470 rows where VIC_SEX is not 'M' or 'F'. Rows remaining: 1690658\n",
      "Removed 21713 rows with UNKNOWN LOC_OF_OCCUR_DESC. Rows remaining: 1668945\n",
      "Dropped specified columns: ['TO_CHECK_CITIZENS', 'KY_CD', 'LAW_CAT_CD', 'OFNS_DESC', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'SAME_AGE_GROUP', 'SAME_SEX']. Columns remaining: 33\n",
      "Data cleaning completed: 2493835 -> 1668945 rows (66.9% retained)\n",
      "Cleaned df shape: (1668945, 33)\n",
      "Cleaned df columns: ['BORO_NM', 'LOC_OF_OCCUR_DESC', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT', 'SCHOOLS_COUNT', 'METRO_DISTANCE', 'MIN_POI_DISTANCE', 'AVG_POI_DISTANCE', 'MAX_POI_DISTANCE', 'TOTAL_POI_COUNT', 'POI_DIVERSITY', 'POI_DENSITY_SCORE', 'HOUR', 'DAY', 'WEEKDAY', 'IS_WEEKEND', 'MONTH', 'YEAR', 'SEASON', 'TIME_BUCKET', 'IS_HOLIDAY', 'IS_PAYDAY']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Data Cleaning ===\")\n",
    "initial_rows = len(df)\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove rows not relevant to citizens\n",
    "if 'TO_CHECK_CITIZENS' in df_cleaned.columns:\n",
    "    citizen_mask = df_cleaned['TO_CHECK_CITIZENS'] != 0\n",
    "    rows_removed = (~citizen_mask).sum()\n",
    "    df_cleaned = df_cleaned[citizen_mask]\n",
    "    print(f\"Removed {rows_removed} rows where TO_CHECK_CITIZENS = 0\") \n",
    "\n",
    "# Remove rows where VIC_SEX is not in ['M', 'F']\n",
    "if 'VIC_SEX' in df_cleaned.columns:\n",
    "    valid_sex_mask = df_cleaned['VIC_SEX'].isin(['M', 'F'])\n",
    "    rows_removed = (~valid_sex_mask).sum()\n",
    "    df_cleaned = df_cleaned[valid_sex_mask]\n",
    "    print(f\"Removed {rows_removed} rows where VIC_SEX is not 'M' or 'F'. Rows remaining: {len(df_cleaned)}\")\n",
    "else:\n",
    "    print(\"'VIC_SEX' column not found, skipping VIC_SEX filter.\")\n",
    "\n",
    "# Remove rows where the location is unknown\n",
    "if 'LOC_OF_OCCUR_DESC' in df_cleaned.columns:\n",
    "    if df_cleaned['LOC_OF_OCCUR_DESC'].dtype == 'object':\n",
    "        location_mask = df_cleaned['LOC_OF_OCCUR_DESC'].str.upper() != 'UNKNOWN'\n",
    "        rows_removed = (~location_mask).sum()\n",
    "        df_cleaned = df_cleaned[location_mask]\n",
    "        print(f\"Removed {rows_removed} rows with UNKNOWN LOC_OF_OCCUR_DESC. Rows remaining: {len(df_cleaned)}\") \n",
    "    else:\n",
    "        print(\"'LOC_OF_OCCUR_DESC' is not of object type, skipping UNKNOWN location filter. It might be OHE.\")\n",
    "else:\n",
    "    print(\"'LOC_OF_OCCUR_DESC' column not found, skipping UNKNOWN location filter.\")\n",
    "\n",
    "cols_to_drop = [\n",
    "    'TO_CHECK_CITIZENS', 'KY_CD', 'LAW_CAT_CD', \n",
    "    'OFNS_DESC', 'PD_CD', 'PREM_TYP_DESC',\n",
    "    'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX',\n",
    "    'SAME_AGE_GROUP', 'SAME_SEX'\n",
    "]\n",
    "\n",
    "actual_cols_to_drop = [col for col in cols_to_drop if col in df_cleaned.columns]\n",
    "if actual_cols_to_drop:\n",
    "    df_cleaned = df_cleaned.drop(columns=actual_cols_to_drop)\n",
    "    print(f\"Dropped specified columns: {actual_cols_to_drop}. Columns remaining: {df_cleaned.shape[1]}\") \n",
    "else:\n",
    "    print(\"No specified columns from 'cols_to_drop' found to remove.\") \n",
    "\n",
    "final_rows = len(df_cleaned)\n",
    "print(f\"Data cleaning completed: {initial_rows} -> {final_rows} rows ({ (f'{final_rows/initial_rows*100:.1f}% retained' if initial_rows > 0 else f'{0.0:.1f}%') })\") \n",
    "\n",
    "# Clean the data\n",
    "df = df_cleaned\n",
    "print(f\"Cleaned df shape: {df.shape}\") \n",
    "if not df.empty:\n",
    "    print(f\"Cleaned df columns: {df.columns.tolist()}\") \n",
    "else:\n",
    "    print(\"Cleaned df is empty after cleaning steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1826737",
   "metadata": {},
   "source": [
    "### Reduce the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ef57ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1747475156402,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "19ef57ca",
    "outputId": "12ebba17-6ee0-4f4d-e1ee-8d13ac081c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Applying Temporal Filter (YEAR >= 2023) ===\n",
      "Filtered data to include only rows from 2023 onwards. Rows before: 1668945, Rows after: 714420\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Applying Temporal Filter (YEAR >= 2023) ===\") \n",
    "if 'YEAR' in df.columns:\n",
    "    initial_rows_temporal_filter = len(df)\n",
    "    df = df[df['YEAR'] >= 2023].copy()\n",
    "    rows_after_temporal_filter = len(df)\n",
    "    print(f\"Filtered data to include only rows from 2023 onwards. Rows before: {initial_rows_temporal_filter}, Rows after: {rows_after_temporal_filter}\") \n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty after temporal filter.\")\n",
    "else:\n",
    "    print(\"'YEAR' column not found. Skipping temporal filter.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68336e8",
   "metadata": {},
   "source": [
    "## STKDE: parameter optimization only\n",
    "\n",
    "The `stkde_intensity` column and the `RISK_LEVEL` label are **not** computed here to avoid leakage. Only parameter optimization is performed on X_train. Actual feature/label engineering will take place in `Modeling.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d920f883",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- `stkde_intensity` and `RISK_LEVEL` are not computed here to prevent data leakage.\n",
    "- Only parameter optimization is performed here; feature and label engineering are postponed to the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7102fff",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1747475156423,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "e7102fff"
   },
   "outputs": [],
   "source": [
    "def calculate_stkde_intensity(\n",
    "    df: pd.DataFrame,\n",
    "    year_col: str,\n",
    "    month_col: str,\n",
    "    day_col: str,\n",
    "    hour_col: str,\n",
    "    lat_col: str,\n",
    "    lon_col: str,\n",
    "    hs: float = 200.0,\n",
    "    ht: float = 60.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate STKDE intensity for each event in the DataFrame.\n",
    "    Spatial coords are projected to EPSG:3857 (meters) and time\n",
    "    is normalized in days relative to the earliest event.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Calculating STKDE Intensity (Projected + Relative Time) ===\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Quick checks\n",
    "    if hs <= 0 or ht <= 0 or df.empty:\n",
    "        print(\"Invalid parameters or empty DataFrame, returning NaN intensities.\")\n",
    "        df_out = df.copy()\n",
    "        df_out['stkde_intensity'] = np.nan\n",
    "        return df_out\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    required = [year_col, month_col, day_col, hour_col, lat_col, lon_col]\n",
    "    missing = [c for c in required if c not in df_copy.columns]\n",
    "    if missing:\n",
    "        print(f\"Missing columns {missing}, returning NaN intensities.\")\n",
    "        df_copy['stkde_intensity'] = np.nan\n",
    "        return df_copy\n",
    "\n",
    "    # 1) build temporary datetime\n",
    "    rename_map = {year_col: 'year', month_col: 'month', day_col: 'day', hour_col: 'hour'}\n",
    "    temp_dt = '__temp_stkde_datetime__'\n",
    "    try:\n",
    "        for c in [year_col, month_col, day_col, hour_col]:\n",
    "            df_copy[c] = pd.to_numeric(df_copy[c], errors='coerce')\n",
    "        df_copy[temp_dt] = pd.to_datetime(\n",
    "            df_copy[[year_col, month_col, day_col, hour_col]].rename(columns=rename_map), errors='coerce'\n",
    "        )\n",
    "        df_copy.dropna(subset=[temp_dt, lat_col, lon_col], inplace=True)\n",
    "        df_copy.reset_index(drop=True, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Datetime construction error: {e}\")\n",
    "        df_copy['stkde_intensity'] = np.nan\n",
    "        return df_copy\n",
    "\n",
    "    if df_copy.empty:\n",
    "        df_copy['stkde_intensity'] = np.nan\n",
    "        return df_copy\n",
    "\n",
    "    # 2) project lat/lon to meters\n",
    "    transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:3857\", always_xy=True)\n",
    "    xs, ys = transformer.transform(df_copy[lon_col].values, df_copy[lat_col].values)\n",
    "    df_copy['x_m'], df_copy['y_m'] = xs, ys\n",
    "\n",
    "    # 3) compute relative time in days\n",
    "    t0 = df_copy[temp_dt].min()\n",
    "    df_copy['t_days'] = (df_copy[temp_dt] - t0) / np.timedelta64(1, 'D')\n",
    "\n",
    "    coords = df_copy[['x_m', 'y_m']].values\n",
    "    times = df_copy['t_days'].values\n",
    "    tree = KDTree(coords, metric='euclidean')\n",
    "\n",
    "    # kernel functions\n",
    "    def spatial_kernel(d, h):\n",
    "        if h <= 0:\n",
    "            return 0.0\n",
    "        return np.exp(-0.5 * (d/h)**2) / (2 * np.pi * h**2)\n",
    "\n",
    "    def temporal_kernel(dt, h):\n",
    "        if h <= 0:\n",
    "            return 0.0\n",
    "        return np.exp(-dt/h) / (2 * h)\n",
    "\n",
    "    def process_event(i):\n",
    "        neigh_idx = tree.query_radius(coords[i:i+1], r=5*hs, return_distance=False)[0]\n",
    "        s = 0.0\n",
    "        for j in neigh_idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "            dt = abs(times[i] - times[j])\n",
    "            if dt>5*ht:\n",
    "                continue\n",
    "            dist = np.linalg.norm(coords[i] - coords[j])\n",
    "            s += spatial_kernel(dist, hs) * temporal_kernel(dt, ht)\n",
    "        return max(s, 1e-12)\n",
    "\n",
    "    # parallel computation\n",
    "    n = len(df_copy)\n",
    "    try:\n",
    "        intensities = Parallel(n_jobs=-1)(\n",
    "            delayed(process_event)(i) for i in range(n)\n",
    "        )\n",
    "        df_copy['stkde_intensity'] = np.array(intensities)\n",
    "    except Exception as e:\n",
    "        print(f\"Parallel STKDE error: {e}\")\n",
    "        df_copy['stkde_intensity'] = np.nan\n",
    "\n",
    "    # clean up\n",
    "    df_copy.drop(columns=[temp_dt, 'x_m', 'y_m', 't_days'], errors='ignore', inplace=True)\n",
    "    end_time = time.time()\n",
    "    print(f\"STKDE completed on {n} events in {end_time-start_time:.2f} seconds.\")\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162a329",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "The creation of the 'RISK_LEVEL' column using qcut on the entire dataset has been disabled to prevent data leakage. This operation is performed only in the modeling pipeline, after the train/test split and within each cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb43f3a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1747475157084,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "fb43f3a9",
    "outputId": "18fbce08-6b46-4c5a-a7fd-7e7f18ae1d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Data Splitting ===\n",
      "Ensured YEAR and MONTH are numeric. Rows after NaN drop (if any): 714420\n",
      "Created 'YearMonth' for sorting and df sorted.\n",
      "Distribution of data across YearMonth:\n",
      "YearMonth\n",
      "202301    28904\n",
      "202302    25824\n",
      "202303    29259\n",
      "202304    28931\n",
      "202305    32076\n",
      "202306    31626\n",
      "202307    32832\n",
      "202308    32030\n",
      "202309    30193\n",
      "202310    31450\n",
      "202311    29141\n",
      "202312    29832\n",
      "202401    28495\n",
      "202402    26824\n",
      "202403    28991\n",
      "202404    28657\n",
      "202405    31861\n",
      "202406    31398\n",
      "202407    31752\n",
      "202408    30787\n",
      "202409    30246\n",
      "202410    30900\n",
      "202411    27822\n",
      "202412    24589\n",
      "Name: count, dtype: int64\n",
      "Attempting temporal split. Total rows: 714420, target test rows (~20%): 128596\n",
      "Temporal split scan: Test set would start from YearMonth 202408, include 5 month(s), accumulating 144344 rows.\n",
      "Temporal split successful: Test set starts from YearMonth 202408.\n",
      "  Test set rows: 144344 (~20.20% of data).\n",
      "  Train set rows: 570076.\n",
      "Training set size (X_train): 570076 rows, 33 columns\n",
      "Test set size (X_test): 144344 rows, 33 columns\n",
      "y_train size: 570076, y_test size: 144344\n",
      "X_train columns: ['BORO_NM', 'LOC_OF_OCCUR_DESC', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT', 'SCHOOLS_COUNT', 'METRO_DISTANCE', 'MIN_POI_DISTANCE', 'AVG_POI_DISTANCE', 'MAX_POI_DISTANCE', 'TOTAL_POI_COUNT', 'POI_DIVERSITY', 'POI_DENSITY_SCORE', 'HOUR', 'DAY', 'WEEKDAY', 'IS_WEEKEND', 'MONTH', 'YEAR', 'SEASON', 'TIME_BUCKET', 'IS_HOLIDAY', 'IS_PAYDAY']\n",
      "X_train will now be used for STKDE parameter LCV and scaler selection.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Starting Data Splitting ===\") \n",
    "\n",
    "# Initialize variables at the beginning of the cell\n",
    "perform_temporal_split_successfully = False\n",
    "unique_year_months = [] \n",
    "X_train, X_test = pd.DataFrame(), pd.DataFrame()\n",
    "y_train, y_test = pd.Series(name='DUMMY_TARGET', dtype='object'), pd.Series(name='DUMMY_TARGET', dtype='object')\n",
    "\n",
    "# Ensure YEAR and MONTH are numeric and sortable for temporal split\n",
    "if not df.empty and 'YEAR' in df.columns and 'MONTH' in df.columns:\n",
    "    df['YEAR'] = pd.to_numeric(df['YEAR'], errors='coerce')\n",
    "    df['MONTH'] = pd.to_numeric(df['MONTH'], errors='coerce')\n",
    "    df.dropna(subset=['YEAR', 'MONTH'], inplace=True)\n",
    "    print(f\"Ensured YEAR and MONTH are numeric. Rows after NaN drop (if any): {len(df)}\") \n",
    "    \n",
    "    if df.empty: # Check if df became empty after dropna\n",
    "           print(\"DataFrame became empty after YEAR/MONTH processing. Cannot perform split.\")\n",
    "    else:\n",
    "        df['YearMonth'] = df['YEAR'] * 100 + df['MONTH']\n",
    "        df.sort_values('YearMonth', inplace=True)\n",
    "        print(\"Created 'YearMonth' for sorting and df sorted.\") \n",
    "        print(f\"Distribution of data across YearMonth:\\n{df['YearMonth'].value_counts().sort_index()}\") \n",
    "        unique_year_months = df['YearMonth'].unique()\n",
    "else:\n",
    "    print(\"DataFrame is empty or YEAR/MONTH columns missing. Cannot perform temporal split based on YearMonth.\")\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty. Cannot perform split.\")\n",
    "\n",
    "# Attempt temporal split\n",
    "if not df.empty and len(unique_year_months) >= 2: # Check df.empty again as it might have become empty\n",
    "    total_rows = len(df)\n",
    "    target_test_rows = total_rows * 0.18\n",
    "    print(f\"Attempting temporal split. Total rows: {total_rows}, target test rows (~20%): {target_test_rows:.0f}\") \n",
    "\n",
    "    accumulated_rows_for_test = 0\n",
    "    num_months_for_test_set = 0\n",
    "    # This will be the OLDEST YearMonth included in the test set\n",
    "    determined_test_set_start_ym = None\n",
    "\n",
    "    # Iterate unique_year_months from newest to oldest\n",
    "    for i in range(len(unique_year_months) - 1, -1, -1):\n",
    "        ym = unique_year_months[i]\n",
    "        rows_this_month = df[df['YearMonth'] == ym].shape[0]\n",
    "\n",
    "        accumulated_rows_for_test += rows_this_month\n",
    "        num_months_for_test_set += 1\n",
    "        determined_test_set_start_ym = ym # This ym is the oldest month in the current selection\n",
    "\n",
    "        if accumulated_rows_for_test >= target_test_rows:\n",
    "            # Found the minimum number of recent months to meet the target\n",
    "            break\n",
    "\n",
    "    print(f\"Temporal split scan: Test set would start from YearMonth {determined_test_set_start_ym}, include {num_months_for_test_set} month(s), accumulating {accumulated_rows_for_test} rows.\") \n",
    "\n",
    "    if determined_test_set_start_ym is not None:\n",
    "        if determined_test_set_start_ym > unique_year_months[0]: # Ensures training set is not empty\n",
    "            X_train = df[df['YearMonth'] < determined_test_set_start_ym].copy()\n",
    "            X_test = df[df['YearMonth'] >= determined_test_set_start_ym].copy()\n",
    "            # Define y_train and y_test for successful temporal split\n",
    "            y_train = pd.Series([0] * len(X_train), name='DUMMY_TARGET', dtype='object')\n",
    "            y_test = pd.Series([0] * len(X_test), name='DUMMY_TARGET', dtype='object')\n",
    "\n",
    "            print(f\"Temporal split successful: Test set starts from YearMonth {determined_test_set_start_ym}.\") \n",
    "            print(f\"  Test set rows: {len(X_test)} (~{len(X_test)/total_rows*100:.2f}% of data).\") \n",
    "            print(f\"  Train set rows: {len(X_train)}.\") \n",
    "            perform_temporal_split_successfully = True\n",
    "        else:\n",
    "            print(\"Temporal split would result in an empty training set. Falling back to random split.\")\n",
    "            # perform_temporal_split_successfully remains False\n",
    "    else:\n",
    "        print(\"Could not determine a temporal cutoff for splitting. Falling back to random split.\")\n",
    "        # perform_temporal_split_successfully remains False\n",
    "\n",
    "elif not df.empty: # If df not empty but unique_year_months < 2\n",
    "    print(f\"Not enough unique YearMonth values ({len(unique_year_months)}) for temporal split. Falling back to random split.\")\n",
    "    # perform_temporal_split_successfully remains False\n",
    "\n",
    "# Fallback to random split if temporal split was not successful and df is not empty\n",
    "if not perform_temporal_split_successfully and not df.empty:\n",
    "    print(\"Performing random split (20% test size, random_state=42).\") \n",
    "    random_state = 42\n",
    "    # Stratify=None because y is a dummy target.\n",
    "    # Ensure df passed to train_test_split does not have 'YearMonth' if it was added\n",
    "    df_for_split = df.copy()\n",
    "    if 'YearMonth' in df_for_split.columns:\n",
    "        df_for_split.drop(columns=['YearMonth'], inplace=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_for_split, pd.Series([0] * len(df_for_split), name='DUMMY_TARGET', dtype='object'), \n",
    "        test_size=0.2, random_state=random_state, stratify=None\n",
    "    )\n",
    "    print(\"Random split performed.\") \n",
    "elif df.empty and not perform_temporal_split_successfully: # df is empty\n",
    "    print(\"DataFrame is empty. Split resulted in empty sets.\")\n",
    "\n",
    "# Drop 'YearMonth' from X_train, X_test, and original X (if it was added to X directly)\n",
    "if 'YearMonth' in X_train.columns: X_train.drop(columns=['YearMonth'], inplace=True)\n",
    "if 'YearMonth' in X_test.columns: X_test.drop(columns=['YearMonth'], inplace=True)\n",
    "if 'YearMonth' in df.columns: df.drop(columns=['YearMonth'], inplace=True) # Clean from original df too\n",
    "\n",
    "print(f\"Training set size (X_train): {X_train.shape[0]} rows, {X_train.shape[1]} columns\") \n",
    "print(f\"Test set size (X_test): {X_test.shape[0]} rows, {X_test.shape[1]} columns\") \n",
    "print(f\"y_train size: {y_train.shape[0]}, y_test size: {y_test.shape[0]}\") \n",
    "\n",
    "if not X_train.empty:\n",
    "    print(f\"X_train columns: {X_train.columns.tolist()}\") \n",
    "print(\"X_train will now be used for STKDE parameter LCV and scaler selection.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa82cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(df_lcv, max_rows=20000):\n",
    "    if len(df_lcv) <= max_rows:\n",
    "        return df_lcv\n",
    "\n",
    "    print(f\"Strategic sampling from {len(df_lcv)} to {max_rows} events...\")\n",
    "\n",
    "    # Create temporal strata to ensure coverage of all months/years\n",
    "    df_temp = df_lcv.copy()\n",
    "\n",
    "    # Temporal strata: Year-Month\n",
    "    df_temp['time_stratum'] = df_temp['YEAR'].astype(str) + '-' + df_temp['MONTH'].astype(str).str.zfill(2)\n",
    "\n",
    "    # Spatial strata: Grid optimized for NYC\n",
    "    # Use grid that captures neighborhoods/precincts\n",
    "    lat_bin_size = 0.01  # about 1.1 km - reasonable size for NYC\n",
    "    lon_bin_size = 0.01  # about 0.85 km in NYC\n",
    "\n",
    "    df_temp['lat_bin'] = (df_temp['Latitude'] // lat_bin_size) * lat_bin_size\n",
    "    df_temp['lon_bin'] = (df_temp['Longitude'] // lon_bin_size) * lon_bin_size\n",
    "    df_temp['spatial_stratum'] = df_temp['lat_bin'].astype(str) + '_' + df_temp['lon_bin'].astype(str)\n",
    "\n",
    "    # Combined spatio-temporal stratum\n",
    "    df_temp['combined_stratum'] = df_temp['time_stratum'] + '_' + df_temp['spatial_stratum']\n",
    "\n",
    "    # Strata analysis\n",
    "    stratum_counts = df_temp['combined_stratum'].value_counts()\n",
    "    n_strata = len(stratum_counts)\n",
    "    target_per_stratum = max(1, max_rows // n_strata)\n",
    "\n",
    "    print(f\"Spatio-temporal strata: {n_strata}\")\n",
    "    print(f\"Target per stratum: {target_per_stratum}\")\n",
    "    print(f\"Strata distribution: min={stratum_counts.min()}, max={stratum_counts.max()}, median={stratum_counts.median()}\")\n",
    "\n",
    "    # Adaptive sampling strategy\n",
    "    sampled_dfs = []\n",
    "\n",
    "    for stratum, group in df_temp.groupby('combined_stratum'):\n",
    "        n_in_stratum = len(group)\n",
    "\n",
    "        if n_in_stratum <= target_per_stratum:\n",
    "            # If the stratum is small, take all\n",
    "            sampled_dfs.append(group)\n",
    "        else:\n",
    "            # Subsampling with density: in very dense areas, sample less densely to avoid overrepresenting hotspots\n",
    "            if n_in_stratum > target_per_stratum * 3:\n",
    "                # For very dense strata, use uniform spatial sampling\n",
    "                sample_size = min(target_per_stratum * 2, n_in_stratum)\n",
    "                sampled = group.sample(n=sample_size, random_state=42)\n",
    "            else:\n",
    "                # For moderately dense strata, sample proportionally\n",
    "                sample_size = target_per_stratum\n",
    "                sampled = group.sample(n=sample_size, random_state=42)\n",
    "\n",
    "            sampled_dfs.append(sampled)\n",
    "\n",
    "    # Combine all samples\n",
    "    df_sampled = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "    # If we are still above the target, do a second round of sampling\n",
    "    if len(df_sampled) > max_rows:\n",
    "        print(f\"Second round: reducing from {len(df_sampled)} to {max_rows}\")\n",
    "        # Final sampling maintaining temporal proportions\n",
    "        time_proportions = df_sampled['time_stratum'].value_counts(normalize=True)\n",
    "        final_samples = []\n",
    "\n",
    "        for time_stratum, proportion in time_proportions.items():\n",
    "            group = df_sampled[df_sampled['time_stratum'] == time_stratum]\n",
    "            n_to_sample = max(1, int(max_rows * proportion))\n",
    "            if len(group) <= n_to_sample:\n",
    "                final_samples.append(group)\n",
    "            else:\n",
    "                final_samples.append(group.sample(n=n_to_sample, random_state=42))\n",
    "\n",
    "        df_sampled = pd.concat(final_samples, ignore_index=True)\n",
    "\n",
    "    # Cleanup temporary columns\n",
    "    columns_to_drop = ['time_stratum', 'lat_bin', 'lon_bin', 'spatial_stratum', 'combined_stratum']\n",
    "    df_sampled = df_sampled.drop(columns=[col for col in columns_to_drop if col in df_sampled.columns])\n",
    "\n",
    "    print(f\"Sampling completed: {len(df_sampled)} events selected\")\n",
    "    return df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0686613",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1747475157447,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "b0686613",
    "outputId": "bc6e72fa-0d7e-418a-8f85-bc1883f7ff25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing STKDE optimal parameters at: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Preprocessing)\\stkde_optimal_params.json\n",
      "No STKDE parameter file found at C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Preprocessing)\\stkde_optimal_params.json; will run LCV.\n",
      "=== Likelihood Cross-Validation for STKDE Parameter Optimization ===\n",
      "Performing LCV on X_train (shape: (570076, 33))\n",
      "Strategic sampling from 570076 to 20000 events...\n",
      "Spatio-temporal strata: 15706\n",
      "Target per stratum: 1\n",
      "Strata distribution: min=1, max=354, median=21.0\n",
      "Second round: reducing from 29222 to 20000\n",
      "Sampling completed: 19991 events selected\n",
      "Starting LCV on 19991 sampled events.\n",
      "[Focused Grid] hs_values=[150, 250, 300, 350, 450], ht_values=[45, 60, 75, 90, 120, 150]\n",
      "[Focused Grid] Best LCV params: hs=250, ht=75, Score=-17.48776529303316\n",
      "--- LCV Optimization Complete ---\n",
      "Optimal parameters: hs_opt=250 m, ht_opt=75 days\n",
      "Saved optimal STKDE parameters to C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Preprocessing)\\stkde_optimal_params.json\n",
      "Using STKDE parameters: hs_opt = 250 m, ht_opt = 75 days\n"
     ]
    }
   ],
   "source": [
    "# Initialize STKDE parameters and related variables at the beginning of the cell\n",
    "hs_opt = None\n",
    "ht_opt = None\n",
    "required_cols_for_stkde = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'Latitude', 'Longitude']\n",
    "params_file_path = os.path.join(save_dir, \"stkde_optimal_params.json\")\n",
    "lcv_needed = True\n",
    "\n",
    "print(f\"Checking for existing STKDE optimal parameters at: {params_file_path}\")\n",
    "if os.path.exists(params_file_path):\n",
    "    try:\n",
    "        with open(params_file_path, 'r') as f:\n",
    "            params = json.load(f)\n",
    "        hs_opt = params.get('hs_opt')\n",
    "        ht_opt = params.get('ht_opt')\n",
    "        if isinstance(hs_opt, (int, float)) and isinstance(ht_opt, (int, float)):\n",
    "            print(f\"Loaded optimal STKDE parameters from {params_file_path}: hs_opt={hs_opt}, ht_opt={ht_opt}\")\n",
    "            lcv_needed = False\n",
    "        else:\n",
    "            print(f\"Invalid or non-numeric parameters in {params_file_path}; will run LCV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parameters from {params_file_path}: {e}; will run LCV.\")\n",
    "else:\n",
    "    print(f\"No STKDE parameter file found at {params_file_path}; will run LCV.\")\n",
    "\n",
    "if lcv_needed:\n",
    "    print(\"=== Likelihood Cross-Validation for STKDE Parameter Optimization ===\")\n",
    "    # Check that X_train has all required columns\n",
    "    if X_train.empty or not all(col in X_train.columns for col in required_cols_for_stkde):\n",
    "        print(f\"X_train is empty or missing required columns {required_cols_for_stkde}. Skipping LCV.\")\n",
    "    else:\n",
    "        print(f\"Performing LCV on X_train (shape: {X_train.shape})\")\n",
    "        # Drop rows with missing STKDE inputs\n",
    "        df_lcv = X_train[required_cols_for_stkde].dropna().reset_index(drop=True)\n",
    "        if df_lcv.empty:\n",
    "            print(\"No data left for LCV after dropping NaNs. Skipping LCV.\")\n",
    "        else:\n",
    "            # Sample to limit size\n",
    "            max_lcv_sample_size = 20000\n",
    "            df_lcv_sampled = sampling(df_lcv, max_lcv_sample_size)\n",
    "            print(f\"Starting LCV on {len(df_lcv_sampled)} sampled events.\")\n",
    "\n",
    "            # Define focused grid for bandwidths\n",
    "            # From this grid we obtain hs = 300 m, ht = 45 days\n",
    "            # hs_values_new = [100, 200, 300, 500, 800, 1000, 1500]\n",
    "            # ht_values_new = [1, 3, 7, 14, 21, 30, 45]\n",
    "            hs_values_new = [150, 250, 300, 350, 450]   \n",
    "            ht_values_new = [45, 60, 75, 90, 120, 150]   \n",
    "            print(f\"[Focused Grid] hs_values={hs_values_new}, ht_values={ht_values_new}\")\n",
    "\n",
    "            def evaluate_lcv_candidate(params_eval):\n",
    "                hs_candidate, ht_candidate = params_eval\n",
    "                # Use the unified calculate_stkde_intensity (which handles datetime and projections)\n",
    "                df_int = calculate_stkde_intensity(\n",
    "                    df_lcv_sampled.copy(),\n",
    "                    year_col='YEAR', month_col='MONTH',\n",
    "                    day_col='DAY', hour_col='HOUR',\n",
    "                    lat_col='Latitude', lon_col='Longitude',\n",
    "                    hs=hs_candidate, ht=ht_candidate\n",
    "                )\n",
    "                vals = df_int['stkde_intensity']\n",
    "                valid = vals[(vals > 1e-12) & np.isfinite(vals)]\n",
    "                if len(valid) == 0:\n",
    "                    return -np.inf, hs_candidate, ht_candidate\n",
    "                return np.mean(np.log(valid)), hs_candidate, ht_candidate\n",
    "\n",
    "            # Prepare all candidate pairs\n",
    "            param_candidates = [(h, t) for h in hs_values_new for t in ht_values_new]\n",
    "            results = Parallel(n_jobs=-1)(\n",
    "                delayed(evaluate_lcv_candidate)(cand) for cand in param_candidates\n",
    "            )\n",
    "\n",
    "            # Select the best candidate\n",
    "            best_score, best_hs, best_ht = max(results, key=lambda x: x[0])\n",
    "            print(f\"[Focused Grid] Best LCV params: hs={best_hs}, ht={best_ht}, Score={best_score}\")\n",
    "\n",
    "            if np.isfinite(best_score):\n",
    "                hs_opt, ht_opt = best_hs, best_ht\n",
    "                print(\"--- LCV Optimization Complete ---\")\n",
    "                print(f\"Optimal parameters: hs_opt={hs_opt} m, ht_opt={ht_opt} days\")\n",
    "                try:\n",
    "                    with open(params_file_path, 'w') as f:\n",
    "                        json.dump({'hs_opt': hs_opt, 'ht_opt': ht_opt}, f)\n",
    "                    print(f\"Saved optimal STKDE parameters to {params_file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving STKDE parameters: {e}\")\n",
    "            else:\n",
    "                print(\"No valid intensities found for any candidate. Parameters file not updated.\")\n",
    "else:\n",
    "    print(\"Skipping STKDE LCV run; existing parameters were loaded.\")\n",
    "\n",
    "# Fallback to defaults if needed\n",
    "if hs_opt is None:\n",
    "    print(\"hs_opt is None after LCV/loading; falling back to default 200.0\")\n",
    "    hs_opt = 200.0\n",
    "if ht_opt is None:\n",
    "    print(\"ht_opt is None after LCV/loading; falling back to default 60.0\")\n",
    "    ht_opt = 60.0\n",
    "\n",
    "print(f\"Using STKDE parameters: hs_opt = {hs_opt} m, ht_opt = {ht_opt} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e530f",
   "metadata": {
    "id": "ff3e530f"
   },
   "source": [
    "# Preprocessing Pipeline (Encoding, Scaling, Feature Selection, PCA)\n",
    "\n",
    "Preprocessing steps are defined for different model types, using scikit-learn's `Pipeline` and `ColumnTransformer` to avoid data leakage. Pipelines are fit only on the training set in the modeling phase.\n",
    "\n",
    "This includes:\n",
    "- **Ordinal encoding** for ordinal categorical features (in general and tree pipelines).\n",
    "- **One-hot encoding** for nominal categorical features.\n",
    "- **Cyclical encoding** for cyclical features (custom transformer).\n",
    "- **Scaling** (RobustScaler) for general pipeline.\n",
    "- **Feature selection** (RandomForest importance) in general and tree pipelines.\n",
    "- **Dimensionality reduction** (PCA) for general pipeline.\n",
    "\n",
    "All column lists are dynamically checked against the columns present in `X_train`.\n",
    "\n",
    "**Best Practice Check:**\n",
    "\n",
    "- The general pipeline (with scaling and PCA) is used for linear models and distance-based models (e.g., LogisticRegression, KNN, SVC).\n",
    "- The tree pipeline (OrdinalEncoder for all categoricals, no scaling/PCA) is used for tree-based models (DecisionTree, RandomForest, GradientBoosting).\n",
    "- All pipelines are fit only on the training set, and then applied to the test set, avoiding data leakage.\n",
    "\n",
    "This structure is correct and follows best practices for model-specific preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fb8e534",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1747475157481,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "5fb8e534",
    "outputId": "603a53cb-5fb7-4cde-8243-595eba264c71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Defining Preprocessing Pipelines ===\n",
      "X_train columns available for pipeline definition: ['BORO_NM', 'LOC_OF_OCCUR_DESC', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT']...\n",
      "Ordinal columns found in X_train: ['VIC_AGE_GROUP']\n",
      "Nominal columns found in X_train: ['BORO_NM', 'LOC_OF_OCCUR_DESC', 'VIC_RACE', 'VIC_SEX', 'SEASON', 'TIME_BUCKET']\n",
      "Cyclical columns found in X_train: ['HOUR', 'DAY', 'WEEKDAY', 'MONTH']\n",
      "Numeric columns found in X_train: ['Latitude', 'Longitude', 'BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT', 'SCHOOLS_COUNT', 'METRO_DISTANCE', 'MIN_POI_DISTANCE', 'AVG_POI_DISTANCE', 'MAX_POI_DISTANCE', 'TOTAL_POI_COUNT', 'POI_DIVERSITY', 'POI_DENSITY_SCORE', 'YEAR']\n",
      "\n",
      "General preprocessing pipeline defined using RobustScaler and PCA.\n",
      "Tree-optimized preprocessing pipeline defined (uses OrdinalEncoder, no scaling/PCA).\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Defining Preprocessing Pipelines ===\") \n",
    "\n",
    "# Define chosen_scaler_class before it is used\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "chosen_scaler_class = RobustScaler # Default scaler class, can be changed later\n",
    "\n",
    "# Ensure X_train exists and has columns before proceeding\n",
    "if 'X_train' not in locals() or X_train.empty:\n",
    "    print(\"X_train is not available or is empty. Cannot define pipelines.\")\n",
    "else:\n",
    "    train_cols = X_train.columns.tolist()\n",
    "    print(f\"X_train columns available for pipeline definition: {train_cols[:15]}...\") \n",
    "\n",
    "    # --- Define column types ---\n",
    "    ordinal_cols_def  = ['VIC_AGE_GROUP']\n",
    "    nominal_cols_def  = ['BORO_NM', 'LOC_OF_OCCUR_DESC', 'VIC_RACE', 'VIC_SEX', 'SEASON', 'TIME_BUCKET']\n",
    "    cyclical_cols_def = ['HOUR', 'DAY', 'WEEKDAY', 'MONTH']\n",
    "    numeric_cols_def  = [\n",
    "        'Latitude','Longitude','BAR_DISTANCE','NIGHTCLUB_DISTANCE','ATM_DISTANCE',\n",
    "        'ATMS_COUNT','BARS_COUNT','BUS_STOPS_COUNT','METROS_COUNT','NIGHTCLUBS_COUNT',\n",
    "        'SCHOOLS_COUNT','METRO_DISTANCE','MIN_POI_DISTANCE','AVG_POI_DISTANCE',\n",
    "        'MAX_POI_DISTANCE','TOTAL_POI_COUNT','POI_DIVERSITY','POI_DENSITY_SCORE',\n",
    "        'YEAR'\n",
    "    ]    \n",
    "    binary_cols_def   = ['IS_WEEKEND','IS_HOLIDAY','IS_PAYDAY']\n",
    "\n",
    "    # --- Dynamically filter columns based on what's available in X_train ---\n",
    "    ordinal_cols = [col for col in ordinal_cols_def if col in train_cols]\n",
    "    nominal_cols = [col for col in nominal_cols_def if col in train_cols]\n",
    "    cyclical_cols = [col for col in cyclical_cols_def if col in train_cols]\n",
    "    numeric_cols = [col for col in numeric_cols_def if col in train_cols]\n",
    "    binary_cols = [col for col in binary_cols_def if col in train_cols] # Binary cols will be passed through\n",
    "\n",
    "    print(f\"Ordinal columns found in X_train: {ordinal_cols}\")\n",
    "    print(f\"Nominal columns found in X_train: {nominal_cols}\")\n",
    "    print(f\"Cyclical columns found in X_train: {cyclical_cols}\")\n",
    "    print(f\"Numeric columns found in X_train: {numeric_cols}\")\n",
    "\n",
    "    # --- Common Transformers ---\n",
    "    cyclical_transformer = FunctionTransformer(cyclical_transform, validate=False)\n",
    "    ordinal_categories = [['<18', '18-24', '25-44', '45-64', '65+', 'UNKNOWN']]\n",
    "    random_state = 42\n",
    "\n",
    "    # --- Pipeline for General Models (Linear, KNN, SVC) ---\n",
    "    transformers_general = []\n",
    "    if ordinal_cols:\n",
    "        transformers_general.append(('ord', OrdinalEncoder(categories=ordinal_categories, handle_unknown='use_encoded_value', unknown_value=-1), ordinal_cols))\n",
    "    if nominal_cols:\n",
    "        transformers_general.append(('nom', OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.uint8), nominal_cols))\n",
    "    if cyclical_cols:\n",
    "        transformers_general.append(('cyc', cyclical_transformer, cyclical_cols))\n",
    "    if numeric_cols: \n",
    "        transformers_general.append(('num', chosen_scaler_class(), numeric_cols))\n",
    "\n",
    "    if not transformers_general:\n",
    "        print(\"Warning: No transformers applicable for the general pipeline. It will be empty.\")\n",
    "        preprocessor_general = ColumnTransformer([], remainder='passthrough')\n",
    "    else:\n",
    "        preprocessor_general = ColumnTransformer(transformers_general, remainder='passthrough', n_jobs=-1)\n",
    "\n",
    "    preprocessing_pipeline_general = Pipeline([\n",
    "        ('preprocessor', preprocessor_general),\n",
    "        ('pca', PCA(n_components=0.95, random_state=random_state))\n",
    "    ])\n",
    "    print(f\"\\nGeneral preprocessing pipeline defined using {chosen_scaler_class.__name__} and PCA.\")\n",
    "\n",
    "    # --- Pipeline for Tree-Based Models (and Naive Bayes) ---\n",
    "    # Trees benefit from OrdinalEncoding for all categoricals, no scaling, and no PCA.\n",
    "    all_categorical_cols_for_trees = ordinal_cols + nominal_cols\n",
    "    \n",
    "    transformers_trees = []\n",
    "    if all_categorical_cols_for_trees:\n",
    "        # We can use a single OrdinalEncoder step. It can handle specified categories for some columns\n",
    "        # and infer them for others, but it's cleaner to separate them.\n",
    "        # Let's use one encoder for all categoricals for simplicity, as trees can handle it.\n",
    "        transformers_trees.append(('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), all_categorical_cols_for_trees))\n",
    "    \n",
    "    if cyclical_cols:\n",
    "        transformers_trees.append(('cyc', cyclical_transformer, cyclical_cols))\n",
    "\n",
    "    if not transformers_trees:\n",
    "        print(\"Warning: No transformers applicable for the tree pipeline. It will be empty.\")\n",
    "        preprocessor_trees = ColumnTransformer([], remainder='passthrough')\n",
    "    else:\n",
    "        preprocessor_trees = ColumnTransformer(transformers_trees, remainder='passthrough', n_jobs=-1)\n",
    "\n",
    "    # This is the full pipeline for tree-based models. It does NOT include scaling or PCA.\n",
    "    preprocessing_pipeline_trees = Pipeline([\n",
    "        ('preprocessor', preprocessor_trees)\n",
    "    ])\n",
    "    print(\"Tree-optimized preprocessing pipeline defined (uses OrdinalEncoder, no scaling/PCA).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7e316",
   "metadata": {},
   "source": [
    "## Saving artifacts\n",
    "\n",
    "Save the unprocessed train/test sets, pipeline definitions, and scoring metrics for use in the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f61a4fac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5389,
     "status": "ok",
     "timestamp": 1747475162930,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "f61a4fac",
    "outputId": "f58f53ba-9608-4494-b5df-656321e8d012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saving Unprocessed Train/Test Sets, Pipeline Definitions, and Other Artifacts ===\n",
      "Artifacts will be saved in: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Classification (Preprocessing)\n",
      "X_train saved as pickle ((570076, 33)).\n",
      "X_test saved as pickle ((144344, 33)).\n",
      "y_train saved as pickle ((570076,)).\n",
      "y_test saved as pickle ((144344,)).\n",
      "Unfitted general preprocessing pipeline saved.\n",
      "Unfitted tree-optimized preprocessing pipeline saved.\n",
      "STKDE optimal parameters saved.\n",
      "Chosen scaler class name saved.\n",
      "\n",
      "Artifact saving process completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Saving Unprocessed Train/Test Sets, Pipeline Definitions, and Other Artifacts ===\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"Artifacts will be saved in: {save_dir}\")\n",
    "\n",
    "# Save unprocessed data as pandas DataFrames for easier column handling in the next notebook\n",
    "if 'X_train' in locals() and not X_train.empty:\n",
    "    try:\n",
    "        X_train.to_pickle(os.path.join(save_dir, 'X_train.pkl'))\n",
    "        print(f\"X_train saved as pickle ({X_train.shape}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving X_train: {e}\")\n",
    "else:\n",
    "    print(\"X_train is not available or empty. Not saving X_train.\")\n",
    "\n",
    "if 'X_test' in locals() and not X_test.empty:\n",
    "    try:\n",
    "        X_test.to_pickle(os.path.join(save_dir, 'X_test.pkl'))\n",
    "        print(f\"X_test saved as pickle ({X_test.shape}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving X_test: {e}\")\n",
    "else:\n",
    "    print(\"X_test is not available or empty. Not saving X_test.\")\n",
    "\n",
    "# y can be saved as numpy array as it's a single series\n",
    "if 'y_train' in locals() and not y_train.empty:\n",
    "    try:\n",
    "        y_train.to_pickle(os.path.join(save_dir, 'y_train.pkl'))\n",
    "        print(f\"y_train saved as pickle ({y_train.shape}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving y_train: {e}\")\n",
    "else:\n",
    "    print(\"y_train is not available or empty. Not saving y_train.\")\n",
    "\n",
    "if 'y_test' in locals() and not y_test.empty:\n",
    "    try:\n",
    "        y_test.to_pickle(os.path.join(save_dir, 'y_test.pkl'))\n",
    "        print(f\"y_test saved as pickle ({y_test.shape}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving y_test: {e}\")\n",
    "else:\n",
    "    print(\"y_test is not available or empty. Not saving y_test.\")\n",
    "\n",
    "# Save unfitted pipelines\n",
    "if 'preprocessing_pipeline_general' in locals():\n",
    "    try:\n",
    "        joblib.dump(preprocessing_pipeline_general, os.path.join(save_dir, 'preprocessing_pipeline_general.joblib'))\n",
    "        print(\"Unfitted general preprocessing pipeline saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving general preprocessing pipeline: {e}\")\n",
    "\n",
    "if 'preprocessing_pipeline_trees' in locals():\n",
    "    try:\n",
    "        joblib.dump(preprocessing_pipeline_trees, os.path.join(save_dir, 'preprocessing_pipeline_trees.joblib'))\n",
    "        print(\"Unfitted tree-optimized preprocessing pipeline saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving tree-optimized preprocessing pipeline: {e}\")\n",
    "\n",
    "# Save parameters\n",
    "if 'hs_opt' in locals() and 'ht_opt' in locals() and hs_opt is not None and ht_opt is not None: \n",
    "    try:\n",
    "        with open(os.path.join(save_dir, \"stkde_optimal_params.json\"), 'w') as f:\n",
    "            json.dump({'hs_opt': hs_opt, 'ht_opt': ht_opt}, f)\n",
    "        print(\"STKDE optimal parameters saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving STKDE optimal parameters: {e}\")\n",
    "\n",
    "if 'chosen_scaler_class' in locals():\n",
    "    try:\n",
    "        with open(os.path.join(save_dir, \"scaler_info.json\"), 'w') as f:\n",
    "            json.dump({'chosen_scaler_class_name': chosen_scaler_class.__name__}, f)\n",
    "        print(\"Chosen scaler class name saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving chosen_scaler_info: {e}\")\n",
    "\n",
    "print(\"\\nArtifact saving process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a2538",
   "metadata": {},
   "source": [
    "# Conclusions and Next Steps\n",
    "\n",
    "- Data loaded, cleaned, and split into train/test sets.\n",
    "- STKDE parameters optimized only on training data.\n",
    "- Preprocessing pipelines defined for different model types.\n",
    "- All feature and label engineering, as well as pipeline fitting, are postponed to the modeling phase to prevent data leakage.\n",
    "\n",
    "Proceed with `Modeling.ipynb` for feature engineering, target creation, pipeline fitting, and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "872dee94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1747475165154,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "872dee94",
    "outputId": "93fc63cc-b330-4f97-f43b-c19a0ddf5a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Data Scan (Unprocessed Training Data) ===\n",
      "X_train shape: (570076, 33)\n",
      "Sample of X_train (first 5 rows):\n",
      "               BORO_NM LOC_OF_OCCUR_DESC VIC_AGE_GROUP  \\\n",
      "2272796         QUEENS            INSIDE         45-64   \n",
      "827898       MANHATTAN             FRONT         25-44   \n",
      "1927854          BRONX            INSIDE         25-44   \n",
      "1927904          BRONX            INSIDE         25-44   \n",
      "382      STATEN ISLAND            INSIDE         45-64   \n",
      "\n",
      "                         VIC_RACE VIC_SEX   Latitude  Longitude  BAR_DISTANCE  \\\n",
      "2272796  ASIAN / PACIFIC ISLANDER       F  40.753403 -73.815551    988.943219   \n",
      "827898             WHITE HISPANIC       M  40.771866 -73.951560    146.973480   \n",
      "1927854  ASIAN / PACIFIC ISLANDER       F  40.828117 -73.871285    806.327770   \n",
      "1927904            WHITE HISPANIC       M  40.828117 -73.871285    806.327770   \n",
      "382                         WHITE       M  40.511804 -74.250034   3126.908322   \n",
      "\n",
      "         NIGHTCLUB_DISTANCE  ATM_DISTANCE  ...  HOUR  DAY   WEEKDAY  \\\n",
      "2272796         1657.176717   2494.398319  ...    23   16    MONDAY   \n",
      "827898          1944.833414     90.329940  ...    13   27    FRIDAY   \n",
      "1927854          749.235897   2337.952881  ...    19   21  SATURDAY   \n",
      "1927904          749.235897   2337.952881  ...    20    1    SUNDAY   \n",
      "382            16282.560058   1503.217655  ...    10   12  THURSDAY   \n",
      "\n",
      "         IS_WEEKEND  MONTH  YEAR  SEASON  TIME_BUCKET  IS_HOLIDAY  IS_PAYDAY  \n",
      "2272796           0      1  2023  WINTER      EVENING           1          0  \n",
      "827898            0      1  2023  WINTER    AFTERNOON           0          0  \n",
      "1927854           1      1  2023  WINTER      EVENING           0          0  \n",
      "1927904           1      1  2023  WINTER      EVENING           1          1  \n",
      "382               0      1  2023  WINTER      MORNING           0          0  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "y_train shape: (570076,) (Note: y_train is currently a dummy target)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Data Scan (Unprocessed Training Data) ===\") \n",
    "if 'X_train' in locals() and not X_train.empty:\n",
    "    print(f\"X_train shape: {X_train.shape}\") \n",
    "    print(f\"Sample of X_train (first 5 rows):\\n{X_train.head()}\") \n",
    "\n",
    "    if 'y_train' in locals() and not y_train.empty:\n",
    "        print(f\"y_train shape: {y_train.shape} (Note: y_train is currently a dummy target)\") \n",
    "    else:\n",
    "       print(\"y_train is not defined or is empty.\")\n",
    "else:\n",
    "    print(\"X_train is not defined or is empty. Cannot perform final data scan.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
