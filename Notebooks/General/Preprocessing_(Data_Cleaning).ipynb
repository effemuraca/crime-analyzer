{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n0b3NPgg5AO"
   },
   "source": [
    "# Crime Analyzer — General — Data Cleaning (Preprocessing)\n",
    "\n",
    "Purpose\n",
    "- Clean and standardize NYPD complaints prior to spatial enrichment and feature engineering\n",
    "- Remove corrupt/irrelevant records while preserving distributions of key variables\n",
    "\n",
    "Inputs\n",
    "- JupyterOutputs/PrePreProcessed/cleaned_crime_data.csv\n",
    "\n",
    "Outputs\n",
    "- JupyterOutputs/Processed/cleaned_crime_data_processed.csv\n",
    "\n",
    "Design decisions (non-obvious)\n",
    "- Coordinates: remove rows with missing lat/lon; if >5% missing, remove stratified by BORO_NM/LAW_CAT_CD/OFNS_DESC to preserve class mix\n",
    "- PD_CD: drop if missing because downstream mapping requires it\n",
    "- Placeholders: normalize common placeholders like (NULL)/UNKNOWN with uppercase casing for consistency\n",
    "- Age groups: enforce patterns (e.g., 25-44, <18, 65+, UNKNOWN); drop invalid formats\n",
    "- Categorical text: uppercase and trim; map borough abbreviations and location variants (e.g., FRONT OF → FRONT)\n",
    "- Duplicates: drop exact duplicates only\n",
    "\n",
    "Reproducibility\n",
    "- All artifacts are written under JupyterOutputs; no in-place mutations of source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGHusCspg5Ab"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Use pandas/numpy for tabular ops, seaborn/matplotlib for quick checks, and sklearn utilities for optional strategies. Warnings are suppressed for signal-to-noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHtqzircTSlx"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Utilities for missingness profiling, placeholder/string anomaly detection, and validation planning that guide cleaning actions without hard-failing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for efficient data analysis\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"Comprehensive missing value analysis\"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # Standard null/NaN analysis\n",
    "    null_counts = df.isna().sum()\n",
    "    null_percentages = (null_counts / len(df)) * 100\n",
    "    result['standard_nulls'] = pd.DataFrame({\n",
    "        'Column': null_counts.index,\n",
    "        'Null_Count': null_counts.values,\n",
    "        'Null_Percentage': null_percentages.values\n",
    "    })[null_counts > 0].sort_values('Null_Count', ascending=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "def detect_placeholder_values(df, placeholder_list=None):\n",
    "    \"\"\"Detect common placeholder values in object columns\"\"\"\n",
    "    if placeholder_list is None:\n",
    "        placeholder_list = [\n",
    "            'NA', 'N/A', 'NONE', 'UNKNOWN', 'NULL', '-', '--', '?', '0', 'MISSING', 'U',\n",
    "            'UNDEFINED', 'NOT AVAILABLE', 'NOT APPLICABLE', 'NOT SPECIFIED', 'UNSPECIFIED',\n",
    "            'NOT DETERMINED', 'NOT REPORTED', 'NR', 'NIL', 'BLANK', 'UNK', '.', '..', '...',\n",
    "            '9999', '999', 'TBD', 'PENDING', '(NULL)'\n",
    "        ]\n",
    "\n",
    "    placeholder_values_upper = [p.upper() for p in placeholder_list]\n",
    "    placeholder_counts = {}\n",
    "\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].notna().any():\n",
    "            try:\n",
    "                col_upper = df[col].astype(str).str.upper()\n",
    "                found_placeholders = col_upper.isin(placeholder_values_upper)\n",
    "                if found_placeholders.any():\n",
    "                    counts = col_upper[found_placeholders].value_counts()\n",
    "                    placeholder_counts[col] = counts[counts.index.isin(placeholder_values_upper)]\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process column {col}: {e}\")\n",
    "\n",
    "    return placeholder_counts\n",
    "\n",
    "def detect_string_anomalies(df):\n",
    "    \"\"\"Detect empty strings, whitespace-only, and special character strings\"\"\"\n",
    "    empty_counts = {}\n",
    "    whitespace_counts = {}\n",
    "    special_char_counts = {}\n",
    "\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        try:\n",
    "            df_col_str = df[col].astype(str)\n",
    "\n",
    "            # Empty strings\n",
    "            empty_count = (df_col_str == '').sum()\n",
    "            if empty_count > 0:\n",
    "                empty_counts[col] = empty_count\n",
    "\n",
    "            # Whitespace only\n",
    "            whitespace_count = df_col_str.str.isspace().sum()\n",
    "            if whitespace_count > 0:\n",
    "                whitespace_counts[col] = whitespace_count\n",
    "\n",
    "            # Special characters only (excluding specific placeholders)\n",
    "            special_chars_only = df_col_str.str.match(r'^[^a-zA-Z0-9\\s]+$').sum()\n",
    "            if special_chars_only > 0:\n",
    "                unique_special_vals = df_col_str[df_col_str.str.match(r'^[^a-zA-Z0-9\\s]+$')].unique()\n",
    "                # Don't count if only '(NULL)' placeholder\n",
    "                if not (len(unique_special_vals) == 1 and unique_special_vals[0].upper() == '(NULL)'):\n",
    "                    special_char_counts[col] = special_chars_only\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process column {col} as string: {e}\")\n",
    "\n",
    "    return {\n",
    "        'empty_strings': empty_counts,\n",
    "        'whitespace_only': whitespace_counts,\n",
    "        'special_chars_only': special_char_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced strategies and validation\n",
    "\n",
    "Heuristics for missing-value strategies, quality gates, and stratified removal to minimize distribution drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNrswYINaEef"
   },
   "outputs": [],
   "source": [
    "# Advanced missing value handling strategy\n",
    "def missing_value_strategy(df, target_column=None):\n",
    "    \"\"\"\n",
    "    Missing value handling with multiple strategies\n",
    "    based on data characteristics and ML best practices\n",
    "    \"\"\"\n",
    "    strategies = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        missing_pct = (df[col].isna().sum() / len(df)) * 100\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if missing_pct == 0:\n",
    "            strategies[col] = 'no_action'\n",
    "        elif missing_pct > 90:\n",
    "            strategies[col] = 'drop_column'  # Too sparse to be useful\n",
    "        elif missing_pct > 70:\n",
    "            # High missingness - check if pattern is informative\n",
    "            if target_column and target_column in df.columns:\n",
    "                # Test if missingness correlates with target\n",
    "                missing_mask = df[col].isna()\n",
    "                if len(df[target_column].unique()) == 2:  # Binary target\n",
    "                    missing_correlation = abs(df.loc[missing_mask, target_column].mean() -\n",
    "                                           df.loc[~missing_mask, target_column].mean())\n",
    "                    if missing_correlation > 0.05:  # Significant difference\n",
    "                        strategies[col] = 'create_missing_indicator'\n",
    "                    else:\n",
    "                        strategies[col] = 'drop_column'\n",
    "                else:\n",
    "                    strategies[col] = 'drop_column'\n",
    "            else:\n",
    "                strategies[col] = 'drop_column'\n",
    "        elif missing_pct > 30:\n",
    "            # Moderate missingness - use advanced imputation\n",
    "            if pd.api.types.is_numeric_dtype(col_type):\n",
    "                strategies[col] = 'iterative_imputation'\n",
    "            else:\n",
    "                strategies[col] = 'mode_with_indicator'\n",
    "        elif missing_pct > 5:\n",
    "            # Low missingness - standard imputation\n",
    "            if pd.api.types.is_numeric_dtype(col_type):\n",
    "                strategies[col] = 'median_imputation'\n",
    "            else:\n",
    "                strategies[col] = 'mode_imputation'\n",
    "        else:\n",
    "            # Very low missingness - simple forward fill or mode\n",
    "            strategies[col] = 'simple_imputation'\n",
    "\n",
    "    return strategies\n",
    "\n",
    "# Data validation framework\n",
    "def validate_data_quality(df, validation_rules=None):\n",
    "    \"\"\"\n",
    "    Comprehensive data validation framework\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        'passed': [],\n",
    "        'warnings': [],\n",
    "        'errors': []\n",
    "    }\n",
    "\n",
    "    # Default validation rules\n",
    "    if validation_rules is None:\n",
    "        validation_rules = {\n",
    "            'min_rows': 1000,\n",
    "            'max_missing_pct': 50,\n",
    "            'required_columns': [],\n",
    "            'numeric_ranges': {},\n",
    "            'categorical_validity': {}\n",
    "        }\n",
    "\n",
    "    # Check minimum rows\n",
    "    if len(df) < validation_rules['min_rows']:\n",
    "        validation_results['warnings'].append(\n",
    "            f\"Dataset has only {len(df)} rows, less than recommended minimum {validation_rules['min_rows']}\"\n",
    "        )\n",
    "\n",
    "    # Check overall missing data percentage\n",
    "    total_missing_pct = (df.isna().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    if total_missing_pct > validation_rules['max_missing_pct']:\n",
    "        validation_results['warnings'].append(\n",
    "            f\"Overall missing data percentage ({total_missing_pct:.1f}%) exceeds threshold ({validation_rules['max_missing_pct']}%)\"\n",
    "        )\n",
    "\n",
    "    # Check for required columns\n",
    "    for col in validation_rules['required_columns']:\n",
    "        if col not in df.columns:\n",
    "            validation_results['errors'].append(f\"Required column '{col}' is missing\")\n",
    "\n",
    "    # Validate numeric ranges\n",
    "    for col, (min_val, max_val) in validation_rules['numeric_ranges'].items():\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if df[col].min() < min_val or df[col].max() > max_val:\n",
    "                validation_results['warnings'].append(\n",
    "                    f\"Column '{col}' has values outside expected range [{min_val}, {max_val}]\"\n",
    "                )\n",
    "\n",
    "    # Check data consistency\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        validation_results['warnings'].append(f\"Found {duplicates} duplicate rows\")\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "# Stratified sampling for data removal\n",
    "def stratified_data_removal(df, target_column, removal_mask, max_removal_pct=0.1):\n",
    "    \"\"\"\n",
    "    Remove data while maintaining target distribution\n",
    "    \"\"\"\n",
    "    if target_column not in df.columns:\n",
    "        # If no target column, use simple random sampling\n",
    "        removal_indices = df[removal_mask].sample(n=min(int(len(df) * max_removal_pct), removal_mask.sum())).index\n",
    "        return df.drop(removal_indices)\n",
    "\n",
    "    # Stratified removal to maintain target distribution\n",
    "    target_distribution = df[target_column].value_counts(normalize=True)\n",
    "    removal_indices = []\n",
    "\n",
    "    for target_value in target_distribution.index:\n",
    "        target_mask = df[target_column] == target_value\n",
    "        combined_mask = removal_mask & target_mask\n",
    "\n",
    "        if combined_mask.sum() > 0:\n",
    "            # Remove proportionally from each target class\n",
    "            n_to_remove = min(\n",
    "                int(len(df[target_mask]) * max_removal_pct),\n",
    "                combined_mask.sum()\n",
    "            )\n",
    "            if n_to_remove > 0:\n",
    "                removal_indices.extend(\n",
    "                    df[combined_mask].sample(n=n_to_remove).index.tolist()\n",
    "                )\n",
    "\n",
    "    return df.drop(removal_indices)\n",
    "\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"Comprehensive missing value analysis\"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # Standard null/NaN analysis\n",
    "    null_counts = df.isna().sum()\n",
    "    null_percentages = (null_counts / len(df)) * 100\n",
    "\n",
    "    # Create the DataFrame first\n",
    "    standard_nulls_df = pd.DataFrame({\n",
    "        'Column': null_counts.index,\n",
    "        'Null_Count': null_counts.values,\n",
    "        'Null_Percentage': null_percentages.values\n",
    "    })\n",
    "\n",
    "    # Filter the DataFrame based on the 'Null_Count' column and then sort\n",
    "    result['standard_nulls'] = standard_nulls_df[standard_nulls_df['Null_Count'] > 0].sort_values('Null_Count', ascending=False)\n",
    "    missing_patterns = df.isna().value_counts()\n",
    "    # Handle cases where all values are True or False (all NaN or no NaN) which results in a single item Series\n",
    "    if len(missing_patterns) > 1:\n",
    "         result['missing_patterns'] = missing_patterns.head(10)\n",
    "    else:\n",
    "         result['missing_patterns'] = pd.Series(dtype='int64') # Return empty Series if no patterns beyond all True/False\n",
    "\n",
    "    # Correlation between missingness\n",
    "    missing_df = df.isna().astype(int)\n",
    "    missing_corr = missing_df.corr()\n",
    "    # Find high correlations (>0.5) between missing indicators\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(missing_corr.columns)):\n",
    "        for j in range(i+1, len(missing_corr.columns)):\n",
    "            # Ensure columns exist in missing_corr\n",
    "            if missing_corr.columns[i] in missing_corr.index and missing_corr.columns[j] in missing_corr.columns:\n",
    "                 if abs(missing_corr.iloc[i, j]) > 0.5 and abs(missing_corr.iloc[i, j]) < 1.0: # Exclude self-correlation (always 1.0)\n",
    "                    high_corr_pairs.append(\n",
    "                        (missing_corr.columns[i], missing_corr.columns[j], missing_corr.iloc[i, j])\n",
    "                    )\n",
    "    result['missing_correlations'] = high_corr_pairs\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-6Q4HX7g5Ae"
   },
   "source": [
    "## Paths\n",
    "\n",
    "Read from JupyterOutputs/PrePreProcessed and write to JupyterOutputs/Processed to keep artifacts versionable and easy to collect in CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aE4CrZVAg5Af"
   },
   "outputs": [],
   "source": [
    "# Path variables\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"JupyterOutputs\"))\n",
    "prepreprocessed_dir = os.path.join(base_dir, \"PrePreProcessed\")\n",
    "cleaned_data_file = os.path.join(prepreprocessed_dir, \"cleaned_crime_data.csv\")\n",
    "processed_dir = os.path.join(base_dir, \"Processed\")\n",
    "os.makedirs(processed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0UrAAOpg5Ah"
   },
   "source": [
    "# Load data\n",
    "\n",
    "Load the output from Pre-Preprocessing and print a brief schema/shape summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10441,
     "status": "ok",
     "timestamp": 1748159242562,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "G8w3gvNug5Ai",
    "outputId": "eec87286-36b0-4644-8f8d-7358ca666bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading PrePreProcessed Data ===\n",
      "Dataset loaded successfully: 2512541 rows and 18 columns\n",
      "Columns in the dataset: ['BORO_NM', 'CMPLNT_FR_DT', 'CMPLNT_FR_TM', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'Latitude', 'Longitude', 'OFNS_DESC', 'PARKS_NM', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"=== Loading PrePreProcessed Data ===\")\n",
    "try:\n",
    "    if os.path.exists(cleaned_data_file):\n",
    "        df = pd.read_csv(cleaned_data_file)\n",
    "        initial_rows = len(df)\n",
    "        print(f\"Dataset loaded successfully: {initial_rows} rows and {df.shape[1]} columns\")\n",
    "        print(f\"Columns in the dataset: {df.columns.tolist()}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find cleaned dataset at: {cleaned_data_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18hoPJ4ng5Ak"
   },
   "source": [
    "## Initial Data Overview\n",
    "\n",
    "Display basic information and summary statistics of the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1748159243052,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "perjrVckg5Al",
    "outputId": "284088a5-ee44-4beb-bb06-245a128136db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Overview ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2512541 entries, 0 to 2512540\n",
      "Data columns (total 18 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   BORO_NM            object \n",
      " 1   CMPLNT_FR_DT       object \n",
      " 2   CMPLNT_FR_TM       object \n",
      " 3   KY_CD              int64  \n",
      " 4   LAW_CAT_CD         object \n",
      " 5   LOC_OF_OCCUR_DESC  object \n",
      " 6   Latitude           float64\n",
      " 7   Longitude          float64\n",
      " 8   OFNS_DESC          object \n",
      " 9   PARKS_NM           object \n",
      " 10  PD_CD              float64\n",
      " 11  PREM_TYP_DESC      object \n",
      " 12  SUSP_AGE_GROUP     object \n",
      " 13  SUSP_RACE          object \n",
      " 14  SUSP_SEX           object \n",
      " 15  VIC_AGE_GROUP      object \n",
      " 16  VIC_RACE           object \n",
      " 17  VIC_SEX            object \n",
      "dtypes: float64(3), int64(1), object(14)\n",
      "memory usage: 345.0+ MB\n",
      "None\n",
      "\n",
      "=== Summary Statistics ===\n",
      "              KY_CD      Latitude     Longitude         PD_CD\n",
      "count  2.512541e+06  2.512513e+06  2.512513e+06  2.510451e+06\n",
      "mean   3.023611e+02  4.073665e+01 -7.392284e+01  4.079612e+02\n",
      "std    1.593475e+02  1.341624e-01  2.071862e-01  2.211310e+02\n",
      "min    1.010000e+02  0.000000e+00 -7.425474e+01  1.000000e+02\n",
      "25%    1.170000e+02  4.067566e+01 -7.397286e+01  2.540000e+02\n",
      "50%    3.410000e+02  4.073495e+01 -7.392579e+01  3.520000e+02\n",
      "75%    3.510000e+02  4.081185e+01 -7.387954e+01  6.380000e+02\n",
      "max    8.810000e+02  4.091271e+01  0.000000e+00  9.690000e+02\n"
     ]
    }
   ],
   "source": [
    "# Display basic dataset overview\n",
    "print(\"\\n=== Dataset Overview ===\")\n",
    "print(df.info())\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBbwjvvZg5Al"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "We quantify missingness, placeholders, and anomalies to drive targeted, low-risk cleaning steps. Findings below inform the rules applied in Section 2.x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25pdKLdsg5Am"
   },
   "source": [
    "## 1.1 Check for Missing and Anomalous Values\n",
    "\n",
    "Identify standard null/NaN values, empty strings, whitespace-only strings, and common placeholder strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qahhuDvVg5An"
   },
   "source": [
    "### 1.1.1 Standard Null/NaN Analysis\n",
    "\n",
    "Calculate and display the count and percentage of standard null (NaN) values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19324,
     "status": "ok",
     "timestamp": 1748159332977,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "zGCHYizkTVxX",
    "outputId": "b729250f-d500-4e9f-dd8a-1b0c2a4832ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANALYSIS SECTION ===\n",
      "Starting comprehensive analysis of the dataset...\n",
      "\n",
      "1.1 COMPREHENSIVE MISSING VALUE ANALYSIS\n",
      "Performing sophisticated missing value analysis...\n",
      "\n",
      "Columns with null/NaN values:\n",
      "   Column  Null_Count  Null_Percentage\n",
      "    PD_CD        2090         0.083183\n",
      " Latitude          28         0.001114\n",
      "Longitude          28         0.001114\n",
      "\n",
      "--- Missing Value Strategy Recommendations ---\n",
      "  PD_CD (0.08% missing): simple_imputation\n",
      "  Latitude (0.00% missing): simple_imputation\n",
      "  Longitude (0.00% missing): simple_imputation\n",
      "\n",
      "--- Top Missing Value Patterns ---\n",
      "  Pattern: BORO_NM=False, CMPLNT_FR_DT=False, CMPLNT_FR_TM=False, KY_CD=False, LAW_CAT_CD=False, LOC_OF_OCCUR_DESC=False, Latitude=False, Longitude=False, OFNS_DESC=False, PARKS_NM=False, PD_CD=False, PREM_TYP_DESC=False, SUSP_AGE_GROUP=False, SUSP_RACE=False, SUSP_SEX=False, VIC_AGE_GROUP=False, VIC_RACE=False, VIC_SEX=False -> 2510447 rows (99.92%)\n",
      "  Pattern: BORO_NM=False, CMPLNT_FR_DT=False, CMPLNT_FR_TM=False, KY_CD=False, LAW_CAT_CD=False, LOC_OF_OCCUR_DESC=False, Latitude=False, Longitude=False, OFNS_DESC=False, PARKS_NM=False, PD_CD=True, PREM_TYP_DESC=False, SUSP_AGE_GROUP=False, SUSP_RACE=False, SUSP_SEX=False, VIC_AGE_GROUP=False, VIC_RACE=False, VIC_SEX=False -> 2066 rows (0.08%)\n",
      "  Pattern: BORO_NM=False, CMPLNT_FR_DT=False, CMPLNT_FR_TM=False, KY_CD=False, LAW_CAT_CD=False, LOC_OF_OCCUR_DESC=False, Latitude=True, Longitude=True, OFNS_DESC=False, PARKS_NM=False, PD_CD=True, PREM_TYP_DESC=False, SUSP_AGE_GROUP=False, SUSP_RACE=False, SUSP_SEX=False, VIC_AGE_GROUP=False, VIC_RACE=False, VIC_SEX=False -> 24 rows (0.00%)\n",
      "  Pattern: BORO_NM=False, CMPLNT_FR_DT=False, CMPLNT_FR_TM=False, KY_CD=False, LAW_CAT_CD=False, LOC_OF_OCCUR_DESC=False, Latitude=True, Longitude=True, OFNS_DESC=False, PARKS_NM=False, PD_CD=False, PREM_TYP_DESC=False, SUSP_AGE_GROUP=False, SUSP_RACE=False, SUSP_SEX=False, VIC_AGE_GROUP=False, VIC_RACE=False, VIC_SEX=False -> 4 rows (0.00%)\n",
      "\n",
      "--- No highly correlated missing values found ---\n",
      "\n",
      "Total columns without null values: 15\n",
      "\n",
      "--- Data Quality Validation ---\n",
      "WARNINGS:\n",
      "  Column 'Latitude' has values outside expected range [40.4, 41.0]\n",
      "  Column 'Longitude' has values outside expected range [-74.3, -73.7]\n",
      "  Found 9502 duplicate rows\n",
      "\n",
      "1.1.1 Standard Null/NaN Analysis - COMPLETED\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ANALYSIS SECTION ===\")\n",
    "print(\"Starting comprehensive analysis of the dataset...\")\n",
    "\n",
    "# 1.1 Advanced Missing Value Analysis\n",
    "print(\"\\n1.1 COMPREHENSIVE MISSING VALUE ANALYSIS\")\n",
    "print(\"Performing missing value analysis...\")\n",
    "\n",
    "# Use the enhanced missing value analysis\n",
    "missing_analysis = analyze_missing_values(df)\n",
    "null_analysis = missing_analysis['standard_nulls']\n",
    "\n",
    "print(\"\\nColumns with null/NaN values:\")\n",
    "if not null_analysis.empty:\n",
    "    print(null_analysis.to_string(index=False))\n",
    "\n",
    "    # Add missing value strategy recommendations\n",
    "    print(\"\\n--- Missing Value Strategy Recommendations ---\")\n",
    "    strategies = missing_value_strategy(df)\n",
    "    for col in null_analysis['Column']:\n",
    "        strategy = strategies.get(col, 'unknown')\n",
    "        missing_pct = null_analysis[null_analysis['Column'] == col]['Null_Percentage'].iloc[0]\n",
    "        print(f\"  {col} ({missing_pct:.2f}% missing): {strategy}\")\n",
    "else:\n",
    "    print(\"No columns with standard null/NaN values found.\")\n",
    "\n",
    "# Display missing value patterns\n",
    "if 'missing_patterns' in missing_analysis and len(missing_analysis['missing_patterns']) > 1:\n",
    "    print(\"\\n--- Top Missing Value Patterns ---\")\n",
    "    for pattern, count in missing_analysis['missing_patterns'].head(5).items():\n",
    "        pattern_str = ', '.join([f\"{col}={val}\" for col, val in zip(df.columns, pattern)])\n",
    "        print(f\"  Pattern: {pattern_str} -> {count} rows ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Display correlated missingness\n",
    "if missing_analysis['missing_correlations']:\n",
    "    print(\"\\n--- Highly Correlated Missing Values (>0.5) ---\")\n",
    "    for col1, col2, corr in missing_analysis['missing_correlations']:\n",
    "        print(f\"  {col1} <-> {col2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"\\n--- No highly correlated missing values found ---\")\n",
    "\n",
    "print(f\"\\nTotal columns without null values: {df.shape[1] - len(null_analysis)}\")\n",
    "\n",
    "# Data Quality Validation\n",
    "print(\"\\n--- Data Quality Validation ---\")\n",
    "validation_rules = {\n",
    "    'min_rows': 10000,\n",
    "    'max_missing_pct': 30,\n",
    "    'required_columns': ['BORO_NM', 'LAW_CAT_CD'],\n",
    "    'numeric_ranges': {\n",
    "        'Latitude': (40.4, 41.0),  # Approximate NYC bounds\n",
    "        'Longitude': (-74.3, -73.7)\n",
    "    }\n",
    "}\n",
    "\n",
    "validation_results = validate_data_quality(df, validation_rules)\n",
    "\n",
    "if validation_results['errors']:\n",
    "    print(\"ERRORS:\")\n",
    "    for error in validation_results['errors']:\n",
    "        print(f\"  {error}\")\n",
    "\n",
    "if validation_results['warnings']:\n",
    "    print(\"WARNINGS:\")\n",
    "    for warning in validation_results['warnings']:\n",
    "        print(f\"  {warning}\")\n",
    "\n",
    "if not validation_results['errors'] and not validation_results['warnings']:\n",
    "    print(\"All data quality checks passed!\")\n",
    "\n",
    "print(\"\\n1.1.1 Standard Null/NaN Analysis - COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1lw7y93g5Ao"
   },
   "source": [
    "### 1.1.2 String Anomaly Analysis\n",
    "\n",
    "Identify and count occurrences of empty strings, whitespace-only strings, and special character strings in object-type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24360,
     "status": "ok",
     "timestamp": 1748159379596,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "pD9MEL1gg5Ao",
    "outputId": "1d03171f-4096-4359-cf3b-2adbd4541a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1.2 String Anomaly Analysis\n",
      "Checking for empty strings and other anomalous string values...\n",
      "\n",
      "No columns with empty strings found.\n",
      "\n",
      "No columns with only whitespace strings found.\n",
      "\n",
      "No columns with only special characters found.\n"
     ]
    }
   ],
   "source": [
    "# 1.1.2 Check for empty strings and other anomalous string values using helper function\n",
    "print(\"\\n1.1.2 String Anomaly Analysis\")\n",
    "print(\"Checking for empty strings and other anomalous string values...\")\n",
    "\n",
    "string_anomalies = detect_string_anomalies(df)\n",
    "\n",
    "# Display results\n",
    "if string_anomalies['empty_strings']:\n",
    "    print(\"\\nColumns with empty strings:\")\n",
    "    for col, count in string_anomalies['empty_strings'].items():\n",
    "        print(f\"  {col}: {count} empty strings ({count/len(df)*100:.4f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo columns with empty strings found.\")\n",
    "\n",
    "if string_anomalies['whitespace_only']:\n",
    "    print(\"\\nColumns with whitespace-only strings:\")\n",
    "    for col, count in string_anomalies['whitespace_only'].items():\n",
    "        print(f\"  {col}: {count} whitespace-only values ({count/len(df)*100:.4f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo columns with only whitespace strings found.\")\n",
    "\n",
    "if string_anomalies['special_chars_only']:\n",
    "    print(\"\\nColumns with special-char-only strings (excluding specific placeholders like '(NULL)'):\")\n",
    "    for col, count in string_anomalies['special_chars_only'].items():\n",
    "         print(f\"  {col}: {count} special-char-only values ({count/len(df)*100:.4f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo columns with only special characters found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVn-1v-1g5Ap"
   },
   "source": [
    "### 1.1.3 Placeholder Value Analysis\n",
    "\n",
    "Search for common placeholder strings (e.g., 'UNKNOWN', 'N/A', '-', '9999', '(NULL)') in object columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29453,
     "status": "ok",
     "timestamp": 1748159409047,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "2kCTdCMIg5Ap",
    "outputId": "ebef83ed-c8bc-4966-89ae-3601b84002a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1.3 Placeholder Value Analysis\n",
      "Checking for common placeholder values that might represent missing data...\n",
      "\n",
      "Found potential placeholder values for missing data:\n",
      "  BORO_NM:\n",
      "    '(null)': 3689 occurrences (0.1468%)\n",
      "  LOC_OF_OCCUR_DESC:\n",
      "    '(null)': 470255 occurrences (18.7163%)\n",
      "  OFNS_DESC:\n",
      "    '(null)': 44 occurrences (0.0018%)\n",
      "  PARKS_NM:\n",
      "    '(null)': 2496688 occurrences (99.3690%)\n",
      "  PREM_TYP_DESC:\n",
      "    '(null)': 36876 occurrences (1.4677%)\n",
      "  SUSP_AGE_GROUP:\n",
      "    'UNKNOWN': 941287 occurrences (37.4635%)\n",
      "    '(null)': 412809 occurrences (16.4299%)\n",
      "  SUSP_RACE:\n",
      "    'UNKNOWN': 630863 occurrences (25.1086%)\n",
      "    '(null)': 412809 occurrences (16.4299%)\n",
      "  SUSP_SEX:\n",
      "    'U': 526492 occurrences (20.9546%)\n",
      "    '(null)': 412809 occurrences (16.4299%)\n",
      "  VIC_AGE_GROUP:\n",
      "    'UNKNOWN': 715543 occurrences (28.4789%)\n",
      "    '(null)': 1 occurrences (0.0000%)\n",
      "  VIC_RACE:\n",
      "    'UNKNOWN': 767959 occurrences (30.5650%)\n",
      "    '(null)': 452 occurrences (0.0180%)\n",
      "  VIC_SEX:\n",
      "    '(null)': 1 occurrences (0.0000%)\n"
     ]
    }
   ],
   "source": [
    "# 1.1.3 Check for placeholder values using helper function\n",
    "print(\"\\n1.1.3 Placeholder Value Analysis\")\n",
    "print(\"Checking for common placeholder values that might represent missing data...\")\n",
    "\n",
    "placeholder_counts = detect_placeholder_values(df)\n",
    "\n",
    "if placeholder_counts:\n",
    "    print(\"\\nFound potential placeholder values for missing data:\")\n",
    "    for col, counts in placeholder_counts.items():\n",
    "        if not counts.empty:\n",
    "            print(f\"  {col}:\")\n",
    "            for placeholder, count in counts.items():\n",
    "                # Try to find original case for display\n",
    "                original_case_placeholder = placeholder\n",
    "                try:\n",
    "                    original_vals = df[col][df[col].astype(str).str.upper() == placeholder].unique()\n",
    "                    if len(original_vals) == 1:\n",
    "                        original_case_placeholder = original_vals[0]\n",
    "                except:\n",
    "                    pass\n",
    "                print(f\"    '{original_case_placeholder}': {count} occurrences ({count/len(df)*100:.4f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo common placeholder values found in object columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWODNQt6g5Aq"
   },
   "source": [
    "## 1.2 Analyze Age Group Values\n",
    "\n",
    "Examine the unique values and formats in suspect and victim age group columns to identify any entries that don't match expected patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10662,
     "status": "ok",
     "timestamp": 1748159421004,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "Womy3l__g5Aq",
    "outputId": "d6985661-1038-4d9f-e385-f85c63adce90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.2 ANALYZING AGE GROUP VALUES\n",
      "Checking for unusual values in age group columns: ['SUSP_AGE_GROUP', 'VIC_AGE_GROUP']\n",
      "\n",
      "--- Analyzing 'SUSP_AGE_GROUP' ---\n",
      "  Unique values count: 85\n",
      "\n",
      "  Top 10 most common non-NaN values:\n",
      "    1. 'UNKNOWN': 941287 occurrences (37.46%)\n",
      "    2. '25-44': 676766 occurrences (26.94%)\n",
      "    3. '(null)': 412809 occurrences (16.43%)\n",
      "    4. '45-64': 231471 occurrences (9.21%)\n",
      "    5. '18-24': 178229 occurrences (7.09%)\n",
      "    6. '<18': 46631 occurrences (1.86%)\n",
      "    7. '65+': 25131 occurrences (1.00%)\n",
      "    8. '1022': 25 occurrences (0.00%)\n",
      "    9. '1023': 20 occurrences (0.00%)\n",
      "    10. '2021': 19 occurrences (0.00%)\n",
      "\n",
      "  Found 78 distinct unusual values (excluding NaN and expected formats like '25-44', '<18', '65+', 'UNKNOWN', '(NULL)' ):\n",
      "    '1022': 25 occurrences (0.0010%)\n",
      "    '1023': 20 occurrences (0.0008%)\n",
      "    '2021': 19 occurrences (0.0008%)\n",
      "    '2022': 17 occurrences (0.0007%)\n",
      "    '2023': 13 occurrences (0.0005%)\n",
      "    '2024': 13 occurrences (0.0005%)\n",
      "    '2020': 10 occurrences (0.0004%)\n",
      "    '-1': 5 occurrences (0.0002%)\n",
      "    '942': 4 occurrences (0.0002%)\n",
      "    '953': 3 occurrences (0.0001%)\n",
      "    '-973': 3 occurrences (0.0001%)\n",
      "    '-969': 3 occurrences (0.0001%)\n",
      "    '-6': 3 occurrences (0.0001%)\n",
      "    '-3': 3 occurrences (0.0001%)\n",
      "    '-951': 3 occurrences (0.0001%)\n",
      "    '1024': 2 occurrences (0.0001%)\n",
      "    '-2': 2 occurrences (0.0001%)\n",
      "    '-47': 2 occurrences (0.0001%)\n",
      "    '-941': 2 occurrences (0.0001%)\n",
      "    '-971': 2 occurrences (0.0001%)\n",
      "    ... and 58 more distinct unusual values.\n",
      "\n",
      "--- Analyzing 'VIC_AGE_GROUP' ---\n",
      "  Unique values count: 93\n",
      "\n",
      "  Top 10 most common non-NaN values:\n",
      "    1. '25-44': 902146 occurrences (35.91%)\n",
      "    2. 'UNKNOWN': 715543 occurrences (28.48%)\n",
      "    3. '45-64': 474827 occurrences (18.90%)\n",
      "    4. '18-24': 211204 occurrences (8.41%)\n",
      "    5. '65+': 122598 occurrences (4.88%)\n",
      "    6. '<18': 86090 occurrences (3.43%)\n",
      "    7. '1023': 6 occurrences (0.00%)\n",
      "    8. '-1': 4 occurrences (0.00%)\n",
      "    9. '-2': 4 occurrences (0.00%)\n",
      "    10. '-961': 4 occurrences (0.00%)\n",
      "\n",
      "  Found 86 distinct unusual values (excluding NaN and expected formats like '25-44', '<18', '65+', 'UNKNOWN', '(NULL)' ):\n",
      "    '1023': 6 occurrences (0.0002%)\n",
      "    '-1': 4 occurrences (0.0002%)\n",
      "    '-961': 4 occurrences (0.0002%)\n",
      "    '-2': 4 occurrences (0.0002%)\n",
      "    '-3': 3 occurrences (0.0001%)\n",
      "    '-30': 3 occurrences (0.0001%)\n",
      "    '-964': 3 occurrences (0.0001%)\n",
      "    '950': 2 occurrences (0.0001%)\n",
      "    '-973': 2 occurrences (0.0001%)\n",
      "    '-965': 2 occurrences (0.0001%)\n",
      "    '-60': 2 occurrences (0.0001%)\n",
      "    '-963': 2 occurrences (0.0001%)\n",
      "    '-5': 2 occurrences (0.0001%)\n",
      "    '-971': 2 occurrences (0.0001%)\n",
      "    '-6': 2 occurrences (0.0001%)\n",
      "    '-48': 2 occurrences (0.0001%)\n",
      "    '-65': 2 occurrences (0.0001%)\n",
      "    '-948': 2 occurrences (0.0001%)\n",
      "    '-968': 2 occurrences (0.0001%)\n",
      "    '949': 2 occurrences (0.0001%)\n",
      "    ... and 66 more distinct unusual values.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1.2 ANALYZING AGE GROUP VALUES\")\n",
    "\n",
    "# Define age group columns to check\n",
    "age_group_columns = ['SUSP_AGE_GROUP', 'VIC_AGE_GROUP']\n",
    "print(f\"Checking for unusual values in age group columns: {age_group_columns}\")\n",
    "\n",
    "# Define the expected valid age group formats (used for cleaning)\n",
    "# These patterns help identify values that need fixing or removal\n",
    "expected_age_formats = [\n",
    "    r'^\\d{1,2}-\\d{1,2}$', # e.g., 25-44, 18-24\n",
    "    r'^<\\d{1,2}$',        # e.g., <18\n",
    "    r'^\\d{1,2}\\+$',       # e.g., 65+\n",
    "    r'^UNKNOWN$',         # The standard placeholder eventually used\n",
    "    r'^\\(NULL\\)$',        # The placeholder found in the data that needs handling\n",
    "]\n",
    "# Compile regex patterns for efficiency (case-insensitive)\n",
    "compiled_patterns = [re.compile(p, re.IGNORECASE) for p in expected_age_formats]\n",
    "\n",
    "# Function to check if a value matches any of the expected formats or is NaN\n",
    "def is_expected_age_format(value):\n",
    "    if pd.isna(value):\n",
    "        return True # Treat NaN as 'expected' for analysis (handled in cleaning)\n",
    "    value_str = str(value).strip()\n",
    "    for pattern in compiled_patterns:\n",
    "        if pattern.match(value_str):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for col in age_group_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n--- Analyzing '{col}' ---\")\n",
    "        print(f\"  Unique values count: {df[col].nunique()}\")\n",
    "\n",
    "        # Count missing values (NaN)\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"  Found {nan_count} NaN values ({nan_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "        # Get value counts (excluding NaN for this part)\n",
    "        value_counts = df[col].value_counts(dropna=True)\n",
    "        print(f\"\\n  Top 10 most common non-NaN values:\")\n",
    "        for i, (val, count) in enumerate(value_counts.head(10).items()):\n",
    "            print(f\"    {i+1}. '{val}': {count} occurrences ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "        # Identify unusual values (those not matching expected formats)\n",
    "        # This mask selects rows where the value is NOT NaN AND does NOT match expected formats\n",
    "        unusual_values_mask = ~df[col].apply(is_expected_age_format) & df[col].notna()\n",
    "        unusual_values = df.loc[unusual_values_mask, col].value_counts()\n",
    "\n",
    "        if not unusual_values.empty:\n",
    "            print(f\"\\n  Found {len(unusual_values)} distinct unusual values (excluding NaN and expected formats like '25-44', '<18', '65+', 'UNKNOWN', '(NULL)' ):\")\n",
    "            # Show top N unusual values\n",
    "            for i, (val, count) in enumerate(unusual_values.head(20).items()): # Show top 20 unusual\n",
    "                 print(f\"    '{val}': {count} occurrences ({count/len(df)*100:.4f}%)\")\n",
    "            if len(unusual_values) > 20:\n",
    "                 print(f\"    ... and {len(unusual_values) - 20} more distinct unusual values.\")\n",
    "        else:\n",
    "            print(\"\\n  No unusual age group formats found (all non-NaN values match expected patterns).\")\n",
    "\n",
    "    else:\n",
    "        print(f\"\\n  Column '{col}' not found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uN0Aj6cBg5Ar"
   },
   "source": [
    "## 1.3 Check for Duplicate Records\n",
    "\n",
    "Identify and count exact duplicate rows across all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3989,
     "status": "ok",
     "timestamp": 1748159425093,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "mmY8b5Dtg5Ar",
    "outputId": "564c95c2-7c41-446f-88e0-46e4c88f99e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.3 CHECKING FOR DUPLICATE RECORDS\n",
      "Found 9502 exact duplicate rows (0.3782% of data)\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Checking for duplicate records\n",
    "print(\"\\n1.3 CHECKING FOR DUPLICATE RECORDS\")\n",
    "\n",
    "# Check for exact duplicates across all columns\n",
    "exact_duplicates_count = df.duplicated().sum()\n",
    "if exact_duplicates_count > 0:\n",
    "    print(f\"Found {exact_duplicates_count} exact duplicate rows ({exact_duplicates_count/len(df)*100:.4f}% of data)\")\n",
    "else:\n",
    "    print(\"No exact duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GD-4XrRfg5Ar"
   },
   "source": [
    "## 1.4 Analyze Categorical Value Standardization Needs\n",
    "\n",
    "Check specific categorical columns for inconsistent representations and case variations that require standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44157,
     "status": "ok",
     "timestamp": 1748159469314,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "WsZdzLVmg5As",
    "outputId": "ca1f2b51-757d-410d-a09e-9925facbc263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.4 ANALYZING CATEGORICAL VALUE STANDARDIZATION NEEDS\n",
      "\n",
      "--- Analyzing 'BORO_NM' ---\n",
      "  Number of unique non-NaN string values: 6\n",
      "  Sample unique values: ['BRONX', 'QUEENS', 'BROOKLYN', 'MANHATTAN', 'STATEN ISLAND', '(null)']\n",
      "\n",
      "--- Analyzing 'LAW_CAT_CD' ---\n",
      "  Number of unique non-NaN string values: 3\n",
      "  Sample unique values: ['MISDEMEANOR', 'FELONY', 'VIOLATION']\n",
      "\n",
      "--- Analyzing 'LOC_OF_OCCUR_DESC' ---\n",
      "  Number of unique non-NaN string values: 6\n",
      "  Sample unique values: ['INSIDE', 'FRONT OF', '(null)', 'REAR OF', 'OPPOSITE OF', 'OUTSIDE']\n",
      "  ! Found location variations like {'FRONT OF', 'REAR OF', 'OPPOSITE OF'}. Suggest standardizing (e.g., FRONT OF -> FRONT).\n",
      "\n",
      "--- Analyzing 'OFNS_DESC' ---\n",
      "  Number of unique non-NaN string values: 69\n",
      "  Sample unique values: ['PETIT LARCENY', 'CRIMINAL MISCHIEF & RELATED OF', 'FELONY ASSAULT', 'FORGERY', 'HARRASSMENT 2', 'ASSAULT 3 & RELATED OFFENSES', 'BURGLARY', 'ROBBERY', 'FRAUDS', 'DANGEROUS WEAPONS', 'MISCELLANEOUS PENAL LAW', 'OFF. AGNST PUB ORD SENSBLTY &', 'VEHICLE AND TRAFFIC LAWS', 'ARSON', 'GRAND LARCENY']...\n",
      "\n",
      "--- Analyzing 'PARKS_NM' ---\n",
      "  Number of unique non-NaN string values: 998\n",
      "  (Too many unique values to list sample)\n",
      "\n",
      "--- Analyzing 'PREM_TYP_DESC' ---\n",
      "  Number of unique non-NaN string values: 88\n",
      "  Sample unique values: ['GROCERY/BODEGA', 'RESIDENCE - APT. HOUSE', 'DEPARTMENT STORE', 'STREET', 'CHAIN STORE', 'RESIDENCE - PUBLIC HOUSING', 'FAST FOOD', 'DRUG STORE', 'RESIDENCE-HOUSE', 'COMMERCIAL BUILDING', 'BANK', 'PUBLIC BUILDING', 'BAR/NIGHT CLUB', 'OTHER', 'FOOD SUPERMARKET']...\n",
      "\n",
      "--- Analyzing 'SUSP_RACE' ---\n",
      "  Number of unique non-NaN string values: 8\n",
      "  Sample unique values: ['BLACK', 'UNKNOWN', '(null)', 'BLACK HISPANIC', 'WHITE HISPANIC', 'WHITE', 'ASIAN / PACIFIC ISLANDER', 'AMERICAN INDIAN/ALASKAN NATIVE']\n",
      "\n",
      "--- Analyzing 'SUSP_SEX' ---\n",
      "  Number of unique non-NaN string values: 4\n",
      "  Sample unique values: ['F', 'U', 'M', '(null)']\n",
      "\n",
      "--- Analyzing 'VIC_RACE' ---\n",
      "  Number of unique non-NaN string values: 8\n",
      "  Sample unique values: ['UNKNOWN', 'ASIAN / PACIFIC ISLANDER', 'BLACK', 'BLACK HISPANIC', 'WHITE HISPANIC', 'WHITE', 'AMERICAN INDIAN/ALASKAN NATIVE', '(null)']\n",
      "\n",
      "--- Analyzing 'VIC_SEX' ---\n",
      "  Number of unique non-NaN string values: 6\n",
      "  Sample unique values: ['D', 'M', 'E', 'F', '(null)', 'L']\n",
      "  ! Found unexpected sex values like {'L', 'E', 'D'} (excluding known placeholders). Suggest mapping to M, F, or U/UNKNOWN.\n",
      "\n",
      "--- Summary of Standardization Suggestions ---\n",
      "  LOC_OF_OCCUR_DESC: Standardize location descriptions\n",
      "  VIC_SEX: Map unexpected values to M/F/U\n"
     ]
    }
   ],
   "source": [
    "# 1.4 Analyzing standardization needs for categorical values\n",
    "print(\"\\n1.4 ANALYZING CATEGORICAL VALUE STANDARDIZATION NEEDS\")\n",
    "\n",
    "# Columns to check for potential inconsistencies and variations\n",
    "categorical_cols_to_analyze = [\n",
    "    'BORO_NM', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC',\n",
    "    'PARKS_NM', 'PREM_TYP_DESC', 'SUSP_RACE', 'SUSP_SEX',\n",
    "    'VIC_RACE', 'VIC_SEX'\n",
    "]\n",
    "\n",
    "standardization_suggestions = {}\n",
    "\n",
    "for col in categorical_cols_to_analyze:\n",
    "    # Check if column exists and is of object type (likely categorical string)\n",
    "    if col in df.columns and pd.api.types.is_object_dtype(df[col]):\n",
    "        print(f\"\\n--- Analyzing '{col}' ---\")\n",
    "        # Use .astype(str) to handle potential mixed types safely\n",
    "        unique_values = df[col].astype(str).unique() # Get unique values including potential 'nan' string\n",
    "        unique_values_non_nan_str = df[col].dropna().astype(str).unique()\n",
    "        num_unique = len(unique_values_non_nan_str)\n",
    "        print(f\"  Number of unique non-NaN string values: {num_unique}\")\n",
    "\n",
    "        # Limit display for very high cardinality columns\n",
    "        if num_unique > 100:\n",
    "             print(\"  (Too many unique values to list sample)\")\n",
    "        elif num_unique > 0:\n",
    "            sample = list(unique_values_non_nan_str)[:15]\n",
    "            print(f\"  Sample unique values: {sample}{'...' if num_unique > 15 else ''}\")\n",
    "        else:\n",
    "             print(\"  No unique non-NaN string values found.\")\n",
    "\n",
    "        # Check for case inconsistencies (e.g., 'Brooklyn' vs 'BROOKLYN')\n",
    "        if num_unique > 0:\n",
    "            try:\n",
    "                lower_unique_count = df[col].dropna().astype(str).str.lower().nunique()\n",
    "                upper_unique_count = df[col].dropna().astype(str).str.upper().nunique()\n",
    "                if lower_unique_count != num_unique or upper_unique_count != num_unique:\n",
    "                    print(f\"  ! Found case inconsistencies (e.g., potentially '{unique_values_non_nan_str[0]}' vs '{unique_values_non_nan_str[0].upper()}'). Suggest converting to uniform case (e.g., UPPER).\")\n",
    "                    if col not in standardization_suggestions:\n",
    "                        standardization_suggestions[col] = []\n",
    "                    if 'Convert to UPPERCASE' not in standardization_suggestions[col]:\n",
    "                         standardization_suggestions[col].append('Convert to UPPERCASE')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not perform case check on {col}: {e}\")\n",
    "\n",
    "            # Check for leading/trailing whitespace\n",
    "            try:\n",
    "                # Check if any value is different after stripping\n",
    "                has_whitespace = (df[col].dropna().astype(str) != df[col].dropna().astype(str).str.strip()).any()\n",
    "                if has_whitespace:\n",
    "                    print(\"  ! Found values with leading/trailing whitespace. Suggest stripping whitespace.\")\n",
    "                    if col not in standardization_suggestions:\n",
    "                        standardization_suggestions[col] = []\n",
    "                    if 'Strip whitespace' not in standardization_suggestions[col]:\n",
    "                         standardization_suggestions[col].append('Strip whitespace')\n",
    "            except Exception as e:\n",
    "                 print(f\"  Warning: Could not perform whitespace check on {col}: {e}\")\n",
    "\n",
    "        # Specific checks based on column knowledge (using uppercase for comparison)\n",
    "        col_upper_series = df[col].dropna().astype(str).str.upper()\n",
    "\n",
    "        if col == 'BORO_NM':\n",
    "            abbreviations = {'MN', 'M', 'BX', 'BK', 'K', 'QN', 'Q', 'SI', 'R'}\n",
    "            found_abbr = abbreviations.intersection(set(col_upper_series.unique()))\n",
    "            if found_abbr:\n",
    "                print(f\"  ! Found potential borough abbreviations like {found_abbr}. Suggest mapping to full names (e.g., MN -> MANHATTAN).\")\n",
    "                if col not in standardization_suggestions:\n",
    "                     standardization_suggestions[col] = []\n",
    "                if 'Map abbreviations to full names' not in standardization_suggestions[col]:\n",
    "                     standardization_suggestions[col].append('Map abbreviations to full names')\n",
    "\n",
    "        elif col == 'LOC_OF_OCCUR_DESC':\n",
    "            variations = {'FRONT OF', 'OPPOSITE OF', 'REAR OF'}\n",
    "            found_vars = variations.intersection(set(col_upper_series.unique()))\n",
    "            if found_vars:\n",
    "                print(f\"  ! Found location variations like {found_vars}. Suggest standardizing (e.g., FRONT OF -> FRONT).\")\n",
    "                if col not in standardization_suggestions:\n",
    "                     standardization_suggestions[col] = []\n",
    "                if 'Standardize location descriptions' not in standardization_suggestions[col]:\n",
    "                     standardization_suggestions[col].append('Standardize location descriptions')\n",
    "\n",
    "        elif col in ['SUSP_SEX', 'VIC_SEX']:\n",
    "             # Expected values after cleaning placeholders ('(NULL)', 'U') should be M, F, U\n",
    "             expected_sex = {'M', 'F', 'U'}\n",
    "             # Check for values that are NOT M, F, U, AND also not the known placeholders we handle separately\n",
    "             known_placeholders_upper = {'UNKNOWN', '(NULL)'} # Placeholders handled in other steps\n",
    "             current_unique_upper = set(col_upper_series.unique())\n",
    "             unexpected_sex = current_unique_upper - expected_sex - known_placeholders_upper\n",
    "\n",
    "             if unexpected_sex:\n",
    "                 print(f\"  ! Found unexpected sex values like {unexpected_sex} (excluding known placeholders). Suggest mapping to M, F, or U/UNKNOWN.\")\n",
    "                 if col not in standardization_suggestions:\n",
    "                     standardization_suggestions[col] = []\n",
    "                 if 'Map unexpected values to M/F/U' not in standardization_suggestions[col]:\n",
    "                     standardization_suggestions[col].append('Map unexpected values to M/F/U')\n",
    "\n",
    "    elif col in df.columns:\n",
    "         print(f\"\\n--- Skipping '{col}' (Not object type: {df[col].dtype}) ---\")\n",
    "    # else: Column not found - implicitly skipped\n",
    "\n",
    "\n",
    "print(\"\\n--- Summary of Standardization Suggestions ---\")\n",
    "if standardization_suggestions:\n",
    "    for col, suggestions in standardization_suggestions.items():\n",
    "        print(f\"  {col}: {', '.join(suggestions)}\")\n",
    "else:\n",
    "    print(\"No obvious standardization needs detected based on these checks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGJFJfjWl2do"
   },
   "source": [
    "## 1.5 Validating Dates and Times Formats\n",
    "\n",
    "Detect columns whose names suggest dates or times, attempt to parse each value, and report counts and examples of any invalid entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9605,
     "status": "ok",
     "timestamp": 1748159478956,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "zOrsl-28l2-O",
    "outputId": "0ad336ca-e54f-4932-f2f1-5c0d3fd7695c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.5 VALIDATING DATES AND TIMES FORMATS\n",
      "Identified 1 potential date columns: ['CMPLNT_FR_DT']\n",
      "Identified 1 potential time columns: ['CMPLNT_FR_TM']\n",
      "\n",
      "--- Validating Date Format in 'CMPLNT_FR_DT' ---\n",
      "  All values seem to be either NaN or in a parseable date format.\n",
      "  Date range for parseable dates: 2020-01-01 to 2024-12-31\n",
      "\n",
      "--- Validating Time Format in 'CMPLNT_FR_TM' ---\n",
      "  All values seem to be either NaN or in a parseable time format (HH:MM:SS).\n",
      "  Time range for parseable times: 00:00:00 to 23:59:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1.5 VALIDATING DATES AND TIMES FORMATS\")\n",
    "\n",
    "# Identify potential date and time columns based on naming conventions\n",
    "date_columns = [col for col in df.columns if any(term in col.upper() for term in ['DATE', 'DT'])]\n",
    "time_columns = [col for col in df.columns if any(term in col.upper() for term in ['TIME', 'TM'])]\n",
    "\n",
    "print(f\"Identified {len(date_columns)} potential date columns: {date_columns}\")\n",
    "print(f\"Identified {len(time_columns)} potential time columns: {time_columns}\")\n",
    "\n",
    "# Validate Date Columns\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n--- Validating Date Format in '{col}' ---\")\n",
    "        # Attempt conversion to datetime, coercing errors to NaT (Not a Time)\n",
    "        # Assuming common formats; might need refinement if specific formats are known (e.g., format='%m/%d/%Y')\n",
    "        try:\n",
    "            # Make a copy to avoid modifying the original DataFrame during analysis\n",
    "            dates_series = df[col].copy()\n",
    "            # Attempt conversion - this is the most crucial step\n",
    "            # Using infer_datetime_format=True might speed up parsing if formats are consistent\n",
    "            parsed_dates = pd.to_datetime(dates_series, errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "            # Check for invalid dates (NaT values resulting from parsing errors)\n",
    "            invalid_dates_mask = parsed_dates.isna() & df[col].notna() # Focus on non-NaN originals that failed parsing\n",
    "            invalid_count = invalid_dates_mask.sum()\n",
    "\n",
    "            original_nan_count = df[col].isna().sum()\n",
    "            total_unparseable = original_nan_count + invalid_count\n",
    "\n",
    "            if total_unparseable > 0:\n",
    "                 print(f\"Found {total_unparseable} total unparseable/missing date entries ({total_unparseable/len(df)*100:.2f}%).\" )\n",
    "                 if original_nan_count > 0:\n",
    "                      print(f\"({original_nan_count} were originally NaN/missing).\")\n",
    "                 if invalid_count > 0:\n",
    "                      print(f\"    ({invalid_count} failed parsing due to format issues).\" )\n",
    "                      # Show examples of the original values that failed parsing\n",
    "                      invalid_examples = df.loc[invalid_dates_mask, col].unique() # Show unique original values that failed\n",
    "                      print(f\"    Examples of values that failed parsing: {list(invalid_examples)[:10]}{'...' if len(invalid_examples) > 10 else ''}\")\n",
    "            else:\n",
    "                print(\"  All values seem to be either NaN or in a parseable date format.\")\n",
    "\n",
    "            # Show date range for valid dates\n",
    "            valid_dates = parsed_dates.dropna()\n",
    "            if not valid_dates.empty:\n",
    "                print(f\"  Date range for parseable dates: {valid_dates.min().strftime('%Y-%m-%d')} to {valid_dates.max().strftime('%Y-%m-%d')}\")\n",
    "            elif total_unparseable < len(df):\n",
    "                 # This case should ideally not happen if the above logic is correct\n",
    "                 print(\"  Could not determine date range, although some valid dates might exist.\")\n",
    "            else:\n",
    "                 print(\"  No valid dates found to determine a range.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR validating dates in '{col}': {e}. Check column data type and content.\")\n",
    "\n",
    "# Validate Time Columns\n",
    "for col in time_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n--- Validating Time Format in '{col}' ---\")\n",
    "        try:\n",
    "            # Make a copy to avoid modifying the original DataFrame\n",
    "            time_series = df[col].copy()\n",
    "            # Handle potential non-string types before conversion\n",
    "            time_series_str = time_series.astype(str)\n",
    "\n",
    "            # Attempt conversion to time objects, coercing errors\n",
    "            # Using format='%H:%M:%S' - adjust if needed\n",
    "            parsed_times = pd.to_datetime(time_series_str, format='%H:%M:%S', errors='coerce').dt.time\n",
    "\n",
    "            # Check for invalid times (NaT values resulting from parsing errors)\n",
    "            invalid_times_mask = parsed_times.isna() & df[col].notna() # Focus on non-NaN originals that failed\n",
    "            invalid_count = invalid_times_mask.sum()\n",
    "\n",
    "            original_nan_count = df[col].isna().sum()\n",
    "            total_unparseable = original_nan_count + invalid_count\n",
    "\n",
    "            if total_unparseable > 0:\n",
    "                print(f\"  Found {total_unparseable} total unparseable/missing time entries ({total_unparseable/len(df)*100:.2f}%).\" )\n",
    "                if original_nan_count > 0:\n",
    "                     print(f\"    ({original_nan_count} were originally NaN/missing).\")\n",
    "                if invalid_count > 0:\n",
    "                     print(f\"    ({invalid_count} failed parsing due to format issues).\" )\n",
    "                     # Show examples of original values that failed parsing\n",
    "                     invalid_examples = df.loc[invalid_times_mask, col].unique()\n",
    "                     print(f\"    Examples of values that failed parsing: {list(invalid_examples)[:10]}{'...' if len(invalid_examples) > 10 else ''}\")\n",
    "            else:\n",
    "                print(\"  All values seem to be either NaN or in a parseable time format (HH:MM:SS).\")\n",
    "\n",
    "            # Show time range for valid times\n",
    "            valid_times = parsed_times.dropna()\n",
    "            if not valid_times.empty:\n",
    "                # Convert times to seconds past midnight for robust comparison\n",
    "                def time_to_seconds(t):\n",
    "                   if pd.isna(t): return -1 # Ensure NaNs sort last/first depending on need\n",
    "                   try:\n",
    "                       return t.hour * 3600 + t.minute * 60 + t.second\n",
    "                   except AttributeError:\n",
    "                       return -1 # Handle unexpected types\n",
    "\n",
    "                # Filter out potential -1 values if any conversion errors occurred\n",
    "                valid_seconds = [s for s in (time_to_seconds(t) for t in valid_times) if s >= 0]\n",
    "                if valid_seconds:\n",
    "                     min_time_obj = min(valid_times, key=time_to_seconds)\n",
    "                     max_time_obj = max(valid_times, key=time_to_seconds)\n",
    "                     print(f\"  Time range for parseable times: {min_time_obj.strftime('%H:%M:%S')} to {max_time_obj.strftime('%H:%M:%S')}\")\n",
    "                else:\n",
    "                     print(\"  Could not determine time range from valid times (possibly conversion issues).\")\n",
    "            elif total_unparseable < len(df):\n",
    "                 print(\"  Could not determine time range, although some valid times might exist.\")\n",
    "            else:\n",
    "                 print(\"  No valid times found to determine a range.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR validating times in '{col}': {e}. Check column data type and content. Expected format HH:MM:SS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR4wqUvyg5At"
   },
   "source": [
    "# Cleaning\n",
    "\n",
    "Apply deterministic, auditable rules to make the dataset robust for integration and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleaning_setup_md"
   },
   "source": [
    "## 2.0 Initial Setup for Cleaning\n",
    "\n",
    "Record the original number of rows and create a dictionary to track rows removed by each cleaning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10728,
     "status": "ok",
     "timestamp": 1748159493062,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "cleaning_setup_code",
    "outputId": "a583cc2b-2a8a-44e4-ff12-cf41961ffa84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CLEANING SECTION ===\n",
      "Starting cleaning operations on the dataset...\n",
      "Original number of rows: 2512541\n",
      "\n",
      "2.0 SOPHISTICATED MISSING VALUE ANALYSIS AND PLANNING\n",
      "\n",
      "--- Missing Value Strategy Plan ---\n",
      "  Latitude (0.0% missing): simple_imputation\n",
      "  Longitude (0.0% missing): simple_imputation\n",
      "  PD_CD (0.1% missing): simple_imputation\n",
      "\n",
      "--- Strategy Summary ---\n",
      "  no_action: 15 columns\n",
      "  simple_imputation: 3 columns\n",
      "\n",
      "--- Post-Strategy Data Quality Check ---\n",
      "🟡 WARNINGS:\n",
      "  WARNING: Found 9502 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CLEANING SECTION ===\")\n",
    "print(\"Starting cleaning operations on the dataset...\")\n",
    "\n",
    "# Store the original row count for final comparison\n",
    "# Ensure 'df' exists and has data before getting length\n",
    "if 'df' in locals() and isinstance(df, pd.DataFrame):\n",
    "    original_row_count = len(df)\n",
    "    print(f\"Original number of rows: {original_row_count}\")\n",
    "else:\n",
    "    print(\"Error: DataFrame 'df' not found or not loaded correctly. Cannot proceed with cleaning.\")\n",
    "    # Handle error appropriately, maybe raise an exception or exit\n",
    "    original_row_count = 0 # Set to 0 to avoid errors later, but indicate failure\n",
    "\n",
    "# Dictionary to track rows removed during cleaning steps\n",
    "cleaning_rows_removed = {}\n",
    "\n",
    "# Apply missing value strategy\n",
    "print(\"\\n2.0 MISSING VALUE ANALYSIS AND PLANNING\")\n",
    "if original_row_count > 0:\n",
    "    # Get missing value strategy\n",
    "    missing_strategies = missing_value_strategy(df)\n",
    "\n",
    "    print(\"\\n--- Missing Value Strategy Plan ---\")\n",
    "    strategy_counts = {}\n",
    "    for col, strategy in missing_strategies.items():\n",
    "        strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1\n",
    "        missing_pct = (df[col].isna().sum() / len(df)) * 100\n",
    "        if missing_pct > 0:\n",
    "            print(f\"  {col} ({missing_pct:.1f}% missing): {strategy}\")\n",
    "\n",
    "    print(\"\\n--- Strategy Summary ---\")\n",
    "    for strategy, count in strategy_counts.items():\n",
    "        print(f\"  {strategy}: {count} columns\")\n",
    "\n",
    "    # Apply drop_column strategy first\n",
    "    columns_to_drop = [col for col, strategy in missing_strategies.items() if strategy == 'drop_column']\n",
    "    if columns_to_drop:\n",
    "        print(f\"\\n--- Dropping High-Missingness Columns ---\")\n",
    "        print(f\"Columns to drop: {columns_to_drop}\")\n",
    "        original_cols = len(df.columns)\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        print(f\"Dropped {len(columns_to_drop)} columns. Remaining: {len(df.columns)}\")\n",
    "        cleaning_rows_removed['Dropped_Columns'] = columns_to_drop\n",
    "\n",
    "    # Validate data quality after initial cleaning\n",
    "    print(\"\\n--- Post-Strategy Data Quality Check ---\")\n",
    "    validation_results = validate_data_quality(df)\n",
    "\n",
    "    if validation_results['errors']:\n",
    "        print(\"CRITICAL ERRORS FOUND:\")\n",
    "        for error in validation_results['errors']:\n",
    "            print(f\"  ERROR: {error}\")\n",
    "            \n",
    "\n",
    "    if validation_results['warnings']:\n",
    "        print(\"WARNINGS:\")\n",
    "        for warning in validation_results['warnings']:\n",
    "            print(f\"  WARNING: {warning}\")\n",
    "\n",
    "    if not validation_results['errors'] and not validation_results['warnings']:\n",
    "        print(\"All data quality checks passed after initial cleaning!\")\n",
    "else:\n",
    "    print(\"Skipping missing value analysis due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_geo_md"
   },
   "source": [
    "## 2.1 Clean: Handle Missing Geographic Coordinates with Stratified Sampling\n",
    "\n",
    "Remove rows with missing coordinates while maintaining the distribution of other important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1748159498509,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_geo_code",
    "outputId": "fff57375-b106-48a5-c7a1-f908503294bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.1 CLEANING GEOGRAPHIC COORDINATES DATA WITH STRATIFIED SAMPLING\n",
      "Initial missing values - Latitude: 28, Longitude: 28\n",
      "Found 28 rows with missing coordinates (0.00%)\n",
      "Using simple removal (missing data percentage is low)...\n",
      "Removed 28 rows with missing coordinates.\n",
      "Rows remaining: 2512513\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.1 CLEANING GEOGRAPHIC COORDINATES DATA WITH STRATIFIED SAMPLING\")\n",
    "\n",
    "if original_row_count > 0:\n",
    "    # Identify latitude and longitude columns\n",
    "    lat_col = next((col for col in df.columns if 'LATITUDE' in col.upper()), None)\n",
    "    lon_col = next((col for col in df.columns if 'LONGITUDE' in col.upper()), None)\n",
    "\n",
    "    if lat_col and lon_col:\n",
    "        rows_before_geo = len(df)\n",
    "        initial_na_lat = df[lat_col].isna().sum()\n",
    "        initial_na_lon = df[lon_col].isna().sum()\n",
    "        print(f\"Initial missing values - {lat_col}: {initial_na_lat}, {lon_col}: {initial_na_lon}\")\n",
    "\n",
    "        # Create mask for rows with missing coordinates\n",
    "        missing_coords_mask = df[lat_col].isna() | df[lon_col].isna()\n",
    "        missing_coords_count = missing_coords_mask.sum()\n",
    "\n",
    "        if missing_coords_count > 0:\n",
    "            print(f\"Found {missing_coords_count} rows with missing coordinates ({missing_coords_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "            # Use stratified removal if we have a suitable stratification column\n",
    "            stratify_col = None\n",
    "            for potential_col in ['BORO_NM', 'LAW_CAT_CD', 'OFNS_DESC']:\n",
    "                if potential_col in df.columns and df[potential_col].notna().sum() > 0:\n",
    "                    stratify_col = potential_col\n",
    "                    break\n",
    "\n",
    "            if stratify_col and missing_coords_count / len(df) > 0.05:  # Use stratified if >5% missing\n",
    "                print(f\"Using stratified removal based on '{stratify_col}' to maintain distribution...\")\n",
    "\n",
    "                # Show distribution before removal\n",
    "                before_dist = df[stratify_col].value_counts(normalize=True)\n",
    "                print(\"Distribution before removal:\")\n",
    "                for val, prop in before_dist.head().items():\n",
    "                    print(f\"  {val}: {prop:.3f}\")\n",
    "\n",
    "                # Apply stratified removal\n",
    "                df = stratified_data_removal(df, stratify_col, missing_coords_mask, max_removal_pct=0.15)\n",
    "\n",
    "                # Show distribution after removal\n",
    "                after_dist = df[stratify_col].value_counts(normalize=True)\n",
    "                print(\"Distribution after removal:\")\n",
    "                for val, prop in after_dist.head().items():\n",
    "                    print(f\"  {val}: {prop:.3f}\")\n",
    "\n",
    "            else:\n",
    "                # Simple removal for small amounts of missing data\n",
    "                print(\"Using simple removal (missing data percentage is low)...\")\n",
    "                df = df.dropna(subset=[lat_col, lon_col])\n",
    "\n",
    "        rows_after_geo = len(df)\n",
    "        rows_removed = rows_before_geo - rows_after_geo\n",
    "        cleaning_rows_removed['Missing Geographic Coordinates'] = rows_removed\n",
    "        print(f\"Removed {rows_removed} rows with missing coordinates.\")\n",
    "        print(f\"Rows remaining: {rows_after_geo}\")\n",
    "    else:\n",
    "        print(\"Warning: Latitude or Longitude columns not found. Skipping geographic coordinate cleaning.\")\n",
    "        cleaning_rows_removed['Missing Geographic Coordinates'] = 0\n",
    "else:\n",
    "    print(\"Skipping step 2.1 due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_pdcd_md"
   },
   "source": [
    "## 2.2 Clean: Handle Missing PD_CD Values\n",
    "\n",
    "Remove rows where the `PD_CD` column has a NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1748159508591,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_pdcd_code",
    "outputId": "37d387ae-508c-4bfd-db9d-2e1d8ce5d4db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.2 CLEANING PD_CD VALUES\n",
      "Initial missing values in PD_CD: 2066\n",
      "Dropped 2066 rows with missing PD_CD values.\n",
      "Rows remaining: 2510447\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.2 CLEANING PD_CD VALUES\")\n",
    "\n",
    "if original_row_count > 0: # Proceed only if df was loaded\n",
    "    pd_cd_col = 'PD_CD' # Assuming this is the correct column name\n",
    "\n",
    "    if pd_cd_col in df.columns:\n",
    "        rows_before_pdcd = len(df)\n",
    "        initial_na_pdcd = df[pd_cd_col].isna().sum()\n",
    "        print(f\"Initial missing values in {pd_cd_col}: {initial_na_pdcd}\")\n",
    "\n",
    "        # Drop rows where PD_CD is missing\n",
    "        df = df.dropna(subset=[pd_cd_col])\n",
    "        rows_after_pdcd = len(df)\n",
    "        rows_removed = rows_before_pdcd - rows_after_pdcd\n",
    "\n",
    "        # Store the count of removed rows\n",
    "        cleaning_rows_removed['Missing PD_CD'] = rows_removed\n",
    "        print(f\"Dropped {rows_removed} rows with missing {pd_cd_col} values.\")\n",
    "        print(f\"Rows remaining: {rows_after_pdcd}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{pd_cd_col}' not found. Skipping PD_CD cleaning.\")\n",
    "        cleaning_rows_removed['Missing PD_CD'] = 0\n",
    "else:\n",
    "    print(\"Skipping step 2.2 due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_placeholders_md"
   },
   "source": [
    "## 2.3 Clean: Handle Placeholder Values in Specific Columns\n",
    "\n",
    "Remove rows where `BORO_NM` contains the value `'(NULL)'` (case-insensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2010,
     "status": "ok",
     "timestamp": 1748159514408,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_placeholders_code",
    "outputId": "d2b1bc4b-1831-4daa-c3d1-f6eff11c69ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.3 CLEANING SPECIFIC PLACEHOLDER VALUES\n",
      "Found 3689 rows with '(NULL)' (case-insensitive) in BORO_NM.\n",
      "Removed 3689 rows with '(NULL)' in BORO_NM.\n",
      "Total rows removed due to '(NULL)' in specified columns: 3689\n",
      "Rows remaining: 2506758\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.3 CLEANING SPECIFIC PLACEHOLDER VALUES\")\n",
    "\n",
    "if original_row_count > 0: # Proceed only if df was loaded\n",
    "    placeholder_to_remove = '(NULL)'\n",
    "    columns_to_clean = {'BORO_NM'} # Set for efficient lookup\n",
    "    total_placeholder_rows_removed = 0\n",
    "\n",
    "    for col in columns_to_clean:\n",
    "        if col in df.columns:\n",
    "            rows_before_placeholder = len(df)\n",
    "            # Check for placeholder case-insensitively\n",
    "            try:\n",
    "                # Use .astype(str) to safely handle potential non-string data before comparison\n",
    "                is_placeholder_mask = df[col].astype(str).str.upper() == placeholder_to_remove.upper()\n",
    "                placeholder_count = is_placeholder_mask.sum()\n",
    "                print(f\"Found {placeholder_count} rows with '{placeholder_to_remove}' (case-insensitive) in {col}.\")\n",
    "\n",
    "                if placeholder_count > 0:\n",
    "                    # Remove rows with the placeholder\n",
    "                    df = df[~is_placeholder_mask]\n",
    "                    rows_after_placeholder = len(df)\n",
    "                    rows_removed = rows_before_placeholder - rows_after_placeholder\n",
    "                    # Use a more specific key for tracking\n",
    "                    removal_key = f'Placeholder {placeholder_to_remove} in {col}'\n",
    "                    cleaning_rows_removed[removal_key] = rows_removed\n",
    "                    total_placeholder_rows_removed += rows_removed\n",
    "                    print(f\"Removed {rows_removed} rows with '{placeholder_to_remove}' in {col}.\")\n",
    "                else:\n",
    "                     # Ensure the key exists even if 0 rows were removed\n",
    "                     removal_key = f'Placeholder {placeholder_to_remove} in {col}'\n",
    "                     cleaning_rows_removed[removal_key] = 0\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not clean placeholder '{placeholder_to_remove}' in column {col}: {e}\")\n",
    "                removal_key = f'Placeholder {placeholder_to_remove} in {col}'\n",
    "                cleaning_rows_removed[removal_key] = 0 # Record 0 removals on error\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found for placeholder cleaning.\")\n",
    "            removal_key = f'Placeholder {placeholder_to_remove} in {col}'\n",
    "            cleaning_rows_removed[removal_key] = 0 # Record 0 removals if column missing\n",
    "\n",
    "    print(f\"Total rows removed due to '{placeholder_to_remove}' in specified columns: {total_placeholder_rows_removed}\")\n",
    "    print(f\"Rows remaining: {len(df)}\")\n",
    "else:\n",
    "    print(\"Skipping step 2.3 due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_fill_standardize_md"
   },
   "source": [
    "## 2.4 Clean: Fill or Standardize Missing/Placeholder Values\n",
    "\n",
    "Replace missing values or placeholders with standardized values in specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10702,
     "status": "ok",
     "timestamp": 1748159529780,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_fill_standardize_code",
    "outputId": "5a6e6299-9caa-40ee-99d9-6f5ba9169056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.4 FILLING AND STANDARDIZING MISSING/PLACEHOLDER VALUES\n",
      "--- Processing 'LOC_OF_OCCUR_DESC' (Fill/Standardize with 'UNKNOWN') ---\n",
      "  Standardized 466658 placeholder values (e.g., ['(NULL)', 'UNKNOWN']) to 'UNKNOWN'.\n",
      "--- Processing 'PREM_TYP_DESC' (Fill/Standardize with 'UNKNOWN') ---\n",
      "  Standardized 35745 placeholder values (e.g., ['(NULL)', 'UNKNOWN']) to 'UNKNOWN'.\n",
      "--- Processing 'SUSP_AGE_GROUP' (Fill/Standardize with 'UNKNOWN') ---\n",
      "  Standardized 408893 placeholder values (e.g., ['(NULL)', 'UNKNOWN']) to 'UNKNOWN'.\n",
      "--- Processing 'SUSP_RACE' (Fill/Standardize with 'UNKNOWN') ---\n",
      "  Standardized 408893 placeholder values (e.g., ['(NULL)', 'UNKNOWN']) to 'UNKNOWN'.\n",
      "--- Processing 'SUSP_SEX' (Fill/Standardize with 'U') ---\n",
      "  Standardized 408893 placeholder values (e.g., ['(NULL)', 'UNKNOWN']) to 'U'.\n",
      "--- Processing 'VIC_RACE' (Fill/Standardize with 'UNKNOWN') ---\n",
      "  Standardized 331 placeholder values (e.g., ['(NULL)', 'UNKNOWN']) to 'UNKNOWN'.\n",
      "--- Processing 'VIC_AGE_GROUP' ---\n",
      "  Standardized 1 '(NULL)' values to 'UNKNOWN'.\n",
      "Finished filling and standardizing missing/placeholder values.\n",
      "Rows remaining: 2506758\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.4 FILLING AND STANDARDIZING MISSING/PLACEHOLDER VALUES\")\n",
    "\n",
    "if original_row_count > 0: # Proceed only if df was loaded\n",
    "    # Define columns and their fill/standardization strategy\n",
    "    fill_strategy = {\n",
    "        'LOC_OF_OCCUR_DESC': 'UNKNOWN', # Fill NaN only initially, (NULL) handled later if needed\n",
    "        'PREM_TYP_DESC': 'UNKNOWN',   # Fill NaN and specific placeholders\n",
    "        'SUSP_AGE_GROUP': 'UNKNOWN',  # Fill NaN and specific placeholders\n",
    "        'SUSP_RACE': 'UNKNOWN',     # Fill NaN and specific placeholders\n",
    "        'SUSP_SEX': 'U',            # Fill NaN and specific placeholders\n",
    "        'VIC_RACE': 'UNKNOWN'       # Fill specific placeholders only initially (NaN handled later)\n",
    "        # 'VIC_AGE_GROUP' has (NULL) handled separately below, NaN handled later\n",
    "        # 'PARKS_NM' is intentionally skipped\n",
    "    }\n",
    "\n",
    "    # Placeholders to target for replacement (case-insensitive)\n",
    "    placeholders_to_replace = ['(NULL)', 'UNKNOWN']\n",
    "    placeholders_to_replace_upper = [p.upper() for p in placeholders_to_replace]\n",
    "\n",
    "    for col, fill_value in fill_strategy.items():\n",
    "        if col in df.columns:\n",
    "            print(f\"--- Processing '{col}' (Fill/Standardize with '{fill_value}') ---\")\n",
    "            nan_before = df[col].isna().sum()\n",
    "            placeholders_replaced_count = 0\n",
    "\n",
    "            # --- Apply Fill/Standardization ---\n",
    "            # 1. Fill NaN values first\n",
    "            df[col] = df[col].fillna(fill_value)\n",
    "            nan_filled = nan_before # Assume all NaNs are filled\n",
    "            if nan_filled > 0:\n",
    "                print(f\"  Filled {nan_filled} NaN values with '{fill_value}'.\")\n",
    "\n",
    "            # 2. Replace specific placeholders (case-insensitive)\n",
    "            # Ensure the column is string type for accurate comparison\n",
    "            try:\n",
    "                col_str_upper = df[col].astype(str).str.upper()\n",
    "                # Create a mask for all specified placeholders\n",
    "                mask_is_placeholder = col_str_upper.isin(placeholders_to_replace_upper)\n",
    "                # Exclude cases where the placeholder IS the intended fill value\n",
    "                mask_is_not_fill_value = col_str_upper != fill_value.upper()\n",
    "\n",
    "                # Combine masks: Is a placeholder AND is not the target fill value\n",
    "                mask_to_replace = mask_is_placeholder & mask_is_not_fill_value\n",
    "\n",
    "                placeholders_replaced_count = mask_to_replace.sum()\n",
    "                if placeholders_replaced_count > 0:\n",
    "                     df.loc[mask_to_replace, col] = fill_value\n",
    "                     print(f\"  Standardized {placeholders_replaced_count} placeholder values (e.g., {placeholders_to_replace}) to '{fill_value}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not replace placeholders in {col}: {e}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found for filling/standardizing.\")\n",
    "\n",
    "    # Special handling for (NULL) in VIC_AGE_GROUP if not covered above\n",
    "    vic_age_col = 'VIC_AGE_GROUP'\n",
    "    if vic_age_col in df.columns and vic_age_col not in fill_strategy:\n",
    "         try:\n",
    "            # Ensure string type for comparison\n",
    "            vic_age_null_mask = df[vic_age_col].astype(str).str.upper() == '(NULL)'\n",
    "            vic_age_null_count = vic_age_null_mask.sum()\n",
    "            if vic_age_null_count > 0:\n",
    "                 df.loc[vic_age_null_mask, vic_age_col] = 'UNKNOWN' # Standardize to UNKNOWN\n",
    "                 print(f\"--- Processing '{vic_age_col}' ---\")\n",
    "                 print(f\"  Standardized {vic_age_null_count} '(NULL)' values to 'UNKNOWN'.\")\n",
    "         except Exception as e:\n",
    "              print(f\"Warning: Could not replace (NULL) in {vic_age_col}: {e}\")\n",
    "\n",
    "    print(\"Finished filling and standardizing missing/placeholder values.\")\n",
    "    print(f\"Rows remaining: {len(df)}\") # Row count should not change here\n",
    "else:\n",
    "    print(\"Skipping step 2.4 due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_victim_md"
   },
   "source": [
    "## 2.5 Clean: Handle Missing Victim Information\n",
    "\n",
    "Remove rows with missing victim information or with `'(NULL)'` in `VIC_SEX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3342,
     "status": "ok",
     "timestamp": 1748159536154,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_victim_code",
    "outputId": "7883df1e-6bea-46f2-c965-858a74351570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.5 CLEANING MISSING VICTIM DATA\n",
      "Removed 1 rows with '(NULL)' in VIC_SEX.\n",
      "Initial count of rows with NaN in any of ['VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX']: 0\n",
      "Removed 0 rows with missing (NaN) victim age group, race, or sex.\n",
      "Total rows removed during victim data cleaning (step 2.5): 1\n",
      "Rows remaining: 2506757\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.5 CLEANING MISSING VICTIM DATA\")\n",
    "\n",
    "if original_row_count > 0: # Proceed only if df was loaded\n",
    "    rows_before_victim_clean = len(df)\n",
    "    total_victim_rows_removed = 0\n",
    "\n",
    "    # Step 1: Remove rows with '(NULL)' in VIC_SEX (as a safety check)\n",
    "    vic_sex_col = 'VIC_SEX'\n",
    "    placeholder_vic_sex_key = 'Placeholder (NULL) in VIC_SEX'\n",
    "    cleaning_rows_removed[placeholder_vic_sex_key] = 0 # Initialize\n",
    "    if vic_sex_col in df.columns:\n",
    "        try:\n",
    "            vic_sex_null_mask = df[vic_sex_col].astype(str).str.upper() == '(NULL)'\n",
    "            vic_sex_null_count = vic_sex_null_mask.sum()\n",
    "            if vic_sex_null_count > 0:\n",
    "                df = df[~vic_sex_null_mask]\n",
    "                rows_removed_sex_null = rows_before_victim_clean - len(df)\n",
    "                cleaning_rows_removed[placeholder_vic_sex_key] = rows_removed_sex_null\n",
    "                total_victim_rows_removed += rows_removed_sex_null\n",
    "                print(f\"Removed {rows_removed_sex_null} rows with '(NULL)' in {vic_sex_col}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not clean (NULL) in {vic_sex_col}: {e}\")\n",
    "\n",
    "    # Step 2: Remove rows with missing victim age, race, or sex (NaN values)\n",
    "    vic_columns = ['VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX']\n",
    "    vic_columns_in_df = [col for col in vic_columns if col in df.columns]\n",
    "    missing_victim_nan_key = 'Missing Victim Info (NaN)'\n",
    "    cleaning_rows_removed[missing_victim_nan_key] = 0 # Initialize\n",
    "\n",
    "    if vic_columns_in_df:\n",
    "        rows_before_nan_removal = len(df)\n",
    "        # Calculate how many rows have at least one NaN in these columns\n",
    "        initial_na_mask = df[vic_columns_in_df].isna().any(axis=1)\n",
    "        initial_na_count = initial_na_mask.sum()\n",
    "        print(f\"Initial count of rows with NaN in any of {vic_columns_in_df}: {initial_na_count}\")\n",
    "\n",
    "        # Drop rows where any of these columns are NaN\n",
    "        df = df.dropna(subset=vic_columns_in_df)\n",
    "        rows_after_nan_removal = len(df)\n",
    "        rows_removed_nan = rows_before_nan_removal - rows_after_nan_removal\n",
    "\n",
    "        cleaning_rows_removed[missing_victim_nan_key] = rows_removed_nan\n",
    "        total_victim_rows_removed += rows_removed_nan # Add to total removed in this step\n",
    "        print(f\"Removed {rows_removed_nan} rows with missing (NaN) victim age group, race, or sex.\")\n",
    "    else:\n",
    "        print(f\"Warning: One or more standard victim columns {vic_columns} not found. Skipping NaN removal for victim info.\")\n",
    "\n",
    "    print(f\"Total rows removed during victim data cleaning (step 2.5): {cleaning_rows_removed[placeholder_vic_sex_key] + cleaning_rows_removed[missing_victim_nan_key]}\")\n",
    "    print(f\"Rows remaining: {len(df)}\")\n",
    "else:\n",
    "    print(\"Skipping step 2.5 due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_general_null_md"
   },
   "source": [
    "## 2.6 Clean: Handle Remaining General `(NULL)` Values\n",
    "\n",
    "Replace any remaining occurrences of the `'(NULL)'` placeholder in object-type columns with `'UNKNOWN'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9072,
     "status": "ok",
     "timestamp": 1748159547862,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_general_null_code",
    "outputId": "43e06f5f-9fb2-4bce-abc1-089ebfb05b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.6 HANDLING REMAINING GENERAL '(NULL)' VALUES\n",
      "No general '(NULL)' values found requiring replacement in applicable object columns.\n",
      "Rows remaining: 2506757\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.6 HANDLING REMAINING GENERAL '(NULL)' VALUES\")\n",
    "\n",
    "if original_row_count > 0: # Proceed only if df was loaded\n",
    "    placeholder_to_replace = '(NULL)'\n",
    "    replacement_value = 'UNKNOWN'\n",
    "    # Define columns already handled or intentionally skipped\n",
    "    skipped_cols = {'SUSP_SEX', 'PARKS_NM', 'BORO_NM', 'OFNS_DESC', 'VIC_SEX'} # Add cols where (NULL) was removed\n",
    "    total_general_null_replaced = 0\n",
    "\n",
    "    object_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    for col in object_columns:\n",
    "        if col not in skipped_cols:\n",
    "            try:\n",
    "                # Check for the placeholder case-insensitively\n",
    "                null_mask = df[col].astype(str).str.upper() == placeholder_to_replace.upper()\n",
    "                null_count = null_mask.sum()\n",
    "\n",
    "                if null_count > 0:\n",
    "                    df.loc[null_mask, col] = replacement_value\n",
    "                    total_general_null_replaced += null_count\n",
    "                    print(f\"Replaced {null_count} '{placeholder_to_replace}' values in '{col}' with '{replacement_value}'.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process column {col} for general '{placeholder_to_replace}' replacement: {e}\")\n",
    "\n",
    "    if total_general_null_replaced > 0:\n",
    "         print(f\"Total general '{placeholder_to_replace}' values replaced in this step: {total_general_null_replaced}\")\n",
    "    else:\n",
    "         print(f\"No general '{placeholder_to_replace}' values found requiring replacement in applicable object columns.\")\n",
    "\n",
    "    print(f\"Rows remaining: {len(df)}\") # Row count should not change here\n",
    "else:\n",
    "    print(\"Skipping step 2.6 due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_age_format_md"
   },
   "source": [
    "## 2.7 Clean: Handle Invalid Age Group Formats\n",
    "\n",
    "Remove rows containing invalid or corrupt age group values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7826,
     "status": "ok",
     "timestamp": 1748159555691,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_age_format_code",
    "outputId": "29cb48c6-b4c8-4d2a-d663-99081797d1cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.7 CLEANING INVALID AGE GROUP FORMATS\n",
      "Found 215 invalid age formats in 'SUSP_AGE_GROUP'.\n",
      "  Examples being removed: ['-969', '2020', '-977', '-962', '2019', '-71', '-12', '-942', '1020', '-965']...\n",
      "Removed 215 rows with invalid age formats in 'SUSP_AGE_GROUP'.\n",
      "Found 131 invalid age formats in 'VIC_AGE_GROUP'.\n",
      "  Examples being removed: ['943', '-967', '-4', '-958', '-968', '949', '-948', '-973', '-2', '932']...\n",
      "Removed 131 rows with invalid age formats in 'VIC_AGE_GROUP'.\n",
      "Total rows removed due to invalid age formats: 346\n",
      "Rows remaining: 2506411\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.7 CLEANING INVALID AGE GROUP FORMATS\")\n",
    "\n",
    "if original_row_count > 0: # Proceed only if df was loaded\n",
    "    # Define the final set of valid patterns\n",
    "    # '(NULL)' should have been converted to 'UNKNOWN' already.\n",
    "    valid_age_patterns_final = [\n",
    "        re.compile(r'^\\d{1,2}-\\d{1,2}$'), # e.g., 25-44\n",
    "        re.compile(r'^<\\d{1,2}$'),        # e.g., <18\n",
    "        re.compile(r'^\\d{1,2}\\+$'),       # e.g., 65+\n",
    "        re.compile(r'^UNKNOWN$', re.IGNORECASE) # Allow UNKNOWN (case-insensitive)\n",
    "    ]\n",
    "\n",
    "    # Function to check if a value is a valid age format (NaNs are considered valid here as they were handled)\n",
    "    def is_valid_age_format_final(value):\n",
    "        if pd.isna(value):\n",
    "            return True # NaNs were handled earlier\n",
    "        value_str = str(value).strip()\n",
    "        for pattern in valid_age_patterns_final:\n",
    "            if pattern.match(value_str):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    age_cols_to_validate = ['SUSP_AGE_GROUP', 'VIC_AGE_GROUP']\n",
    "    total_age_format_rows_removed = 0\n",
    "\n",
    "    for col in age_cols_to_validate:\n",
    "        # Define a unique key for tracking removals for this column\n",
    "        removal_key = f'Invalid Format {col}'\n",
    "        cleaning_rows_removed[removal_key] = 0 # Initialize\n",
    "\n",
    "        if col in df.columns:\n",
    "            rows_before_age_clean = len(df)\n",
    "            # Identify invalid formats (excluding NaN)\n",
    "            invalid_age_mask = ~df[col].apply(is_valid_age_format_final) & df[col].notna()\n",
    "            invalid_count = invalid_age_mask.sum()\n",
    "\n",
    "            if invalid_count > 0:\n",
    "                print(f\"Found {invalid_count} invalid age formats in '{col}'.\")\n",
    "                # Display samples of invalid values being removed\n",
    "                invalid_examples = df.loc[invalid_age_mask, col].unique()\n",
    "                print(f\"  Examples being removed: {list(invalid_examples)[:10]}{'...' if len(invalid_examples) > 10 else ''}\")\n",
    "\n",
    "                # Remove rows with invalid formats\n",
    "                df = df[~invalid_age_mask]\n",
    "                rows_removed = rows_before_age_clean - len(df)\n",
    "                cleaning_rows_removed[removal_key] = rows_removed\n",
    "                total_age_format_rows_removed += rows_removed\n",
    "                print(f\"Removed {rows_removed} rows with invalid age formats in '{col}'.\")\n",
    "            else:\n",
    "                print(f\"No invalid age formats found requiring removal in '{col}'.\")\n",
    "        else:\n",
    "             print(f\"Warning: Column '{col}' not found for age format cleaning.\")\n",
    "\n",
    "    print(f\"Total rows removed due to invalid age formats: {total_age_format_rows_removed}\")\n",
    "    print(f\"Rows remaining: {len(df)}\")\n",
    "else:\n",
    "    print(\"Skipping step 2.7 due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_duplicates_md"
   },
   "source": [
    "## 2.8 Clean: Remove Duplicate Records\n",
    "\n",
    "Remove exact duplicate rows, keeping only the first occurrence of each unique row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9603,
     "status": "ok",
     "timestamp": 1748159565328,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_duplicates_code",
    "outputId": "c2a97875-efe5-4b69-dd18-310202754458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.8 REMOVING EXACT DUPLICATE RECORDS\n",
      "Found 9652 exact duplicate rows (excluding first occurrences).\n",
      "Removed 9652 exact duplicate rows.\n",
      "Rows remaining: 2496759\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.8 REMOVING EXACT DUPLICATE RECORDS\")\n",
    "\n",
    "if original_row_count > 0: # Proceed only if df was loaded\n",
    "    rows_before_duplicates = len(df)\n",
    "    # Calculate the number of duplicate rows (excluding the first instance)\n",
    "    initial_duplicate_count = df.duplicated().sum()\n",
    "    print(f\"Found {initial_duplicate_count} exact duplicate rows (excluding first occurrences).\" )\n",
    "    removal_key = 'Exact Duplicates'\n",
    "    cleaning_rows_removed[removal_key] = 0 # Initialize\n",
    "\n",
    "    if initial_duplicate_count > 0:\n",
    "        # Remove exact duplicates, keeping the first instance\n",
    "        df = df.drop_duplicates(keep='first')\n",
    "        rows_after_duplicates = len(df)\n",
    "        rows_removed = rows_before_duplicates - rows_after_duplicates\n",
    "\n",
    "        # Store the count of removed rows\n",
    "        cleaning_rows_removed[removal_key] = rows_removed\n",
    "        print(f\"Removed {rows_removed} exact duplicate rows.\")\n",
    "    else:\n",
    "        print(\"No exact duplicate rows found to remove.\")\n",
    "\n",
    "    print(f\"Rows remaining: {len(df)}\")\n",
    "else:\n",
    "    print(\"Skipping step 2.8 due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_standardize_cats_md"
   },
   "source": [
    "## 2.9 Clean: Standardize Categorical Values (Case and Mapping)\n",
    "\n",
    "Standardize values in categorical columns for consistency, including case conversion and value mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10162,
     "status": "ok",
     "timestamp": 1748159575491,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_standardize_cats_code",
    "outputId": "3f19d13c-6786-4361-82d6-6729f2b637f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.9 STANDARDIZING CATEGORICAL VALUES (CASE AND MAPPING)\n",
      "Applying specific value mappings...\n",
      "  Mapping 723310 values in 'LOC_OF_OCCUR_DESC'...\n",
      "\n",
      "Applying uppercase standardization...\n",
      "Uppercase standardization applied.\n",
      "Categorical value standardization complete.\n",
      "Rows remaining: 2496759\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.9 STANDARDIZING CATEGORICAL VALUES (CASE AND MAPPING)\")\n",
    "\n",
    "if original_row_count > 0: # Proceed only if df was loaded\n",
    "    # 1. Apply specific value mappings (case-insensitive matching, then standardize)\n",
    "    standardization_map = {\n",
    "        'BORO_NM': {'MN':'MANHATTAN', 'M':'MANHATTAN', 'BX':'BRONX', 'BK':'BROOKLYN', 'K':'BROOKLYN', 'QN':'QUEENS', 'Q':'QUEENS', 'SI':'STATEN ISLAND', 'R':'STATEN ISLAND'},\n",
    "        'LOC_OF_OCCUR_DESC': {'FRONT OF':'FRONT', 'OPPOSITE OF':'OPPOSITE', 'REAR OF':'REAR'}\n",
    "        # Add other mappings if needed based on analysis\n",
    "    }\n",
    "\n",
    "    print(\"Applying specific value mappings...\")\n",
    "    for col, mapping in standardization_map.items():\n",
    "        if col in df.columns and pd.api.types.is_object_dtype(df[col]):\n",
    "            # Create a mapping with uppercase keys for case-insensitive lookup\n",
    "            mapping_upper_keys = {k.upper(): v for k, v in mapping.items()}\n",
    "            # Get the series to work with\n",
    "            col_series = df[col].astype(str)\n",
    "            col_series_upper = col_series.str.upper()\n",
    "            # Create a mask for rows that need mapping\n",
    "            mask_needs_mapping = col_series_upper.isin(mapping_upper_keys.keys())\n",
    "            count_to_map = mask_needs_mapping.sum()\n",
    "\n",
    "            if count_to_map > 0:\n",
    "                print(f\"  Mapping {count_to_map} values in '{col}'...\")\n",
    "                # Apply mapping using the uppercase keys\n",
    "                # The .map() function can apply the mapping efficiently\n",
    "                # We replace only the values that need mapping to preserve original case if desired, though we convert to upper later\n",
    "                df[col] = col_series.mask(mask_needs_mapping, col_series_upper.map(mapping_upper_keys))\n",
    "                # Verify - df.loc[mask_needs_mapping, col] = df.loc[mask_needs_mapping, col].str.upper().map(mapping_upper_keys)\n",
    "            # else: print(f\"  No values requiring mapping found in '{col}'.\")\n",
    "\n",
    "        elif col in df.columns:\n",
    "             print(f\"  Skipping mapping for non-object column '{col}'.\")\n",
    "        # else: Column not found - skipped\n",
    "\n",
    "    # 2. Convert specific columns to uppercase for final uniformity\n",
    "    # Ensure all relevant categorical columns are included\n",
    "    case_standardization_cols = [\n",
    "        'BORO_NM', 'LOC_OF_OCCUR_DESC', 'PREM_TYP_DESC', 'PARKS_NM',\n",
    "        'LAW_CAT_CD', 'OFNS_DESC', 'SUSP_RACE', 'SUSP_SEX', 'VIC_RACE', 'VIC_SEX',\n",
    "        'SUSP_AGE_GROUP', 'VIC_AGE_GROUP' # Also standardize age group strings\n",
    "        ]\n",
    "\n",
    "    print(\"\\nApplying uppercase standardization...\")\n",
    "    for col in case_standardization_cols:\n",
    "        if col in df.columns:\n",
    "            # Check if it's an object dtype before attempting string operations\n",
    "            if pd.api.types.is_object_dtype(df[col]):\n",
    "                 try:\n",
    "                     df[col] = df[col].astype(str).str.upper()\n",
    "                     # print(f\"  Converted column '{col}' to uppercase.\") # Optional: reduce verbosity\n",
    "                 except Exception as e:\n",
    "                     print(f\"  Warning: Could not convert column {col} to uppercase: {e}\")\n",
    "            # else: Skipped non-object columns\n",
    "        # else: Column not found - skipped\n",
    "    print(\"Uppercase standardization applied.\")\n",
    "\n",
    "    print(\"Categorical value standardization complete.\")\n",
    "    print(f\"Rows remaining: {len(df)}\") # Row count should not change here\n",
    "else:\n",
    "    print(\"Skipping step 2.9 due to DataFrame loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_summary_md"
   },
   "source": [
    "## 2.10 Final Cleaning Summary\n",
    "\n",
    "Summarize the changes made to the dataset after all cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2264,
     "status": "ok",
     "timestamp": 1748159577754,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_summary_code",
    "outputId": "1e070d4f-d60f-4f7d-ff8f-1e1005088460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.10 FINAL CLEANING SUMMARY\n",
      "Original dataset size: 2512541 rows\n",
      "Final dataset size after cleaning: 2496759 rows, 18 columns\n",
      "Total rows removed: 15782 (0.63% of original data)\n",
      "\n",
      "--- Breakdown of Rows Removed by Cleaning Step (approximate) ---\n",
      "  - Exact Duplicates: 9652 rows (0.38% of original)\n",
      "  - Placeholder (NULL) in BORO_NM: 3689 rows (0.15% of original)\n",
      "  - Missing PD_CD: 2066 rows (0.08% of original)\n",
      "  - Invalid Format SUSP_AGE_GROUP: 215 rows (0.01% of original)\n",
      "  - Invalid Format VIC_AGE_GROUP: 131 rows (0.01% of original)\n",
      "  - Missing Geographic Coordinates: 28 rows (0.00% of original)\n",
      "  - Placeholder (NULL) in VIC_SEX: 1 rows (0.00% of original)\n",
      "\n",
      "Sum of rows removed across tracked steps: 15782\n",
      "\n",
      "Total missing values (NaN) remaining in cleaned dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.10 FINAL CLEANING SUMMARY\")\n",
    "\n",
    "if original_row_count > 0: # Proceed only if df was loaded and processed\n",
    "    final_row_count = len(df)\n",
    "    final_col_count = df.shape[1]\n",
    "    total_rows_removed = original_row_count - final_row_count\n",
    "    percent_removed = (total_rows_removed / original_row_count * 100) if original_row_count > 0 else 0\n",
    "\n",
    "    print(f\"Original dataset size: {original_row_count} rows\") # Initial columns might differ if some were dropped\n",
    "    print(f\"Final dataset size after cleaning: {final_row_count} rows, {final_col_count} columns\")\n",
    "    print(f\"Total rows removed: {total_rows_removed} ({percent_removed:.2f}% of original data)\")\n",
    "\n",
    "    if cleaning_rows_removed:\n",
    "        print(\"\\n--- Breakdown of Rows Removed by Cleaning Step (approximate) ---\")\n",
    "        # Sort by number of rows removed for clarity\n",
    "        # Filter out steps that removed 0 rows\n",
    "        sorted_removals = sorted(\n",
    "            [(k, v) for k, v in cleaning_rows_removed.items() if v > 0],\n",
    "            key=lambda item: item[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        if sorted_removals:\n",
    "            for category, count in sorted_removals:\n",
    "                category_percent = (count / original_row_count * 100) if original_row_count > 0 else 0\n",
    "                print(f\"  - {category}: {count} rows ({category_percent:.2f}% of original)\")\n",
    "        else:\n",
    "             print(\"  No rows were recorded as removed by specific cleaning steps.\")\n",
    "\n",
    "        # Verify total removed matches sum of steps (approximate check, order matters)\n",
    "        sum_removed_steps = sum(cleaning_rows_removed.values())\n",
    "        print(f\"\\nSum of rows removed across tracked steps: {sum_removed_steps}\")\n",
    "        if total_rows_removed != sum_removed_steps:\n",
    "            print(f\"  Note: The total removed ({total_rows_removed}) might differ from the sum of tracked steps ({sum_removed_steps}) due to the order of operations or untracked removals. The final count ({final_row_count}) is accurate.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo row removal tracking data available.\")\n",
    "\n",
    "    final_missing_values = df.isna().sum().sum()\n",
    "    print(f\"\\nTotal missing values (NaN) remaining in cleaned dataset: {final_missing_values}\")\n",
    "    if final_missing_values > 0:\n",
    "        print(\"Remaining missing values breakdown:\")\n",
    "        print(df.isna().sum()[df.isna().sum() > 0])\n",
    "else:\n",
    "    print(\"Skipping final summary due to DataFrame loading or processing issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_scan_md"
   },
   "source": [
    "## 2.11 Scan of the Cleaned Dataset\n",
    "\n",
    "Perform a final scan of the dataset to check the status of each column after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6915,
     "status": "ok",
     "timestamp": 1748159584734,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_scan_code",
    "outputId": "bf27995a-344d-458a-dc84-9a45f025cf90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.11 SCAN OF CLEANED DATASET\n",
      "Analyzing the values in each column after cleaning...\n",
      "\n",
      "Scanning all columns in the cleaned dataset...\n",
      "\n",
      "=== Column: BORO_NM ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 5\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. BROOKLYN: 699518 occurrences (28.02%)\n",
      "  2. MANHATTAN: 606565 occurrences (24.29%)\n",
      "  3. QUEENS: 546234 occurrences (21.88%)\n",
      "  4. BRONX: 537491 occurrences (21.53%)\n",
      "  5. STATEN ISLAND: 106951 occurrences (4.28%)\n",
      "\n",
      "=== Column: CMPLNT_FR_DT ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 1827\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. 2020-06-01: 1916 occurrences (0.08%)\n",
      "  2. 2020-01-01: 1889 occurrences (0.08%)\n",
      "  3. 2024-03-01: 1872 occurrences (0.07%)\n",
      "  4. 2023-09-01: 1842 occurrences (0.07%)\n",
      "  5. 2024-07-05: 1840 occurrences (0.07%)\n",
      "  6. 2024-08-01: 1831 occurrences (0.07%)\n",
      "  7. 2023-06-01: 1829 occurrences (0.07%)\n",
      "  8. 2024-09-20: 1827 occurrences (0.07%)\n",
      "  9. 2024-11-01: 1819 occurrences (0.07%)\n",
      "  10. 2023-11-01: 1814 occurrences (0.07%)\n",
      "  11. 2024-09-13: 1811 occurrences (0.07%)\n",
      "  12. 2022-01-01: 1806 occurrences (0.07%)\n",
      "  13. 2022-07-01: 1801 occurrences (0.07%)\n",
      "  14. 2024-05-24: 1792 occurrences (0.07%)\n",
      "  15. 2022-09-01: 1789 occurrences (0.07%)\n",
      "  ... and 1812 more unique values\n",
      "\n",
      "=== Column: CMPLNT_FR_TM ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 1440\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. 12:00:00: 57518 occurrences (2.30%)\n",
      "  2. 15:00:00: 47772 occurrences (1.91%)\n",
      "  3. 18:00:00: 45666 occurrences (1.83%)\n",
      "  4. 17:00:00: 44725 occurrences (1.79%)\n",
      "  5. 20:00:00: 42561 occurrences (1.70%)\n",
      "  6. 16:00:00: 42513 occurrences (1.70%)\n",
      "  7. 19:00:00: 41412 occurrences (1.66%)\n",
      "  8. 14:00:00: 37893 occurrences (1.52%)\n",
      "  9. 10:00:00: 35550 occurrences (1.42%)\n",
      "  10. 21:00:00: 35534 occurrences (1.42%)\n",
      "  11. 13:00:00: 35326 occurrences (1.41%)\n",
      "  12. 22:00:00: 35232 occurrences (1.41%)\n",
      "  13. 09:00:00: 34622 occurrences (1.39%)\n",
      "  14. 08:00:00: 32959 occurrences (1.32%)\n",
      "  15. 11:00:00: 30414 occurrences (1.22%)\n",
      "  ... and 1425 more unique values\n",
      "\n",
      "=== Column: KY_CD ===\n",
      "Data type: int64\n",
      "Unique values (non-NaN): 69\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. 341: 498316 occurrences (19.96%)\n",
      "  2. 578: 392254 occurrences (15.71%)\n",
      "  3. 344: 265096 occurrences (10.62%)\n",
      "  4. 109: 224958 occurrences (9.01%)\n",
      "  5. 351: 147545 occurrences (5.91%)\n",
      "  6. 106: 125434 occurrences (5.02%)\n",
      "  7. 361: 86776 occurrences (3.48%)\n",
      "  8. 105: 77887 occurrences (3.12%)\n",
      "  9. 126: 76484 occurrences (3.06%)\n",
      "  10. 348: 75382 occurrences (3.02%)\n",
      "  11. 107: 70900 occurrences (2.84%)\n",
      "  12. 121: 68863 occurrences (2.76%)\n",
      "  13. 110: 63067 occurrences (2.53%)\n",
      "  14. 359: 33134 occurrences (1.33%)\n",
      "  15. 233: 30153 occurrences (1.21%)\n",
      "  ... and 54 more unique values\n",
      "\n",
      "Numeric statistics:\n",
      "  Count: 2496759\n",
      "  Mean: 302.5155\n",
      "  Std Dev: 159.5039\n",
      "  Min: 102.0\n",
      "  25%: 117.0\n",
      "  Median (50%): 341.0\n",
      "  75%: 351.0\n",
      "  Max: 881.0\n",
      "\n",
      "=== Column: LAW_CAT_CD ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 3\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. MISDEMEANOR: 1271510 occurrences (50.93%)\n",
      "  2. FELONY: 825904 occurrences (33.08%)\n",
      "  3. VIOLATION: 399345 occurrences (15.99%)\n",
      "\n",
      "=== Column: LOC_OF_OCCUR_DESC ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 5\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. INSIDE: 1309075 occurrences (52.43%)\n",
      "  2. FRONT: 632458 occurrences (25.33%)\n",
      "  3. UNKNOWN: 464374 occurrences (18.60%)\n",
      "  4. OPPOSITE: 50331 occurrences (2.02%)\n",
      "  5. REAR: 40521 occurrences (1.62%)\n",
      "\n",
      "=== Column: Latitude ===\n",
      "Data type: float64\n",
      "Unique values (non-NaN): 124369\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. 40.750423: 7207 occurrences (0.29%)\n",
      "  2. 40.733729: 3625 occurrences (0.15%)\n",
      "  3. 40.757691: 3441 occurrences (0.14%)\n",
      "  4. 40.822622: 3409 occurrences (0.14%)\n",
      "  5. 40.691005: 2614 occurrences (0.10%)\n",
      "  6. 40.873944: 2598 occurrences (0.10%)\n",
      "  7. 40.684454: 2573 occurrences (0.10%)\n",
      "  8. 40.779159: 2483 occurrences (0.10%)\n",
      "  9. 40.714608: 2426 occurrences (0.10%)\n",
      "  10. 40.650466: 2399 occurrences (0.10%)\n",
      "  11. 40.716074: 2386 occurrences (0.10%)\n",
      "  12. 40.734955: 2355 occurrences (0.09%)\n",
      "  13. 40.808372: 2216 occurrences (0.09%)\n",
      "  14. 40.789593: 2151 occurrences (0.09%)\n",
      "  15. 40.768122: 2077 occurrences (0.08%)\n",
      "  ... and 124354 more unique values\n",
      "\n",
      "Numeric statistics:\n",
      "  Count: 2496759\n",
      "  Mean: 40.7366\n",
      "  Std Dev: 0.1344\n",
      "  Min: 0.0\n",
      "  25%: 40.675608\n",
      "  Median (50%): 40.734955\n",
      "  75%: 40.811938\n",
      "  Max: 40.912714\n",
      "\n",
      "=== Column: Longitude ===\n",
      "Data type: float64\n",
      "Unique values (non-NaN): 125517\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. -73.98928: 7239 occurrences (0.29%)\n",
      "  2. -73.871197: 3625 occurrences (0.15%)\n",
      "  3. -73.834115: 3441 occurrences (0.14%)\n",
      "  4. -73.930942: 3438 occurrences (0.14%)\n",
      "  5. -73.983456: 2614 occurrences (0.10%)\n",
      "  6. -73.908788: 2598 occurrences (0.10%)\n",
      "  7. -73.97775: 2573 occurrences (0.10%)\n",
      "  8. -74.011443: 2423 occurrences (0.10%)\n",
      "  9. -73.987128: 2388 occurrences (0.10%)\n",
      "  10. -73.869986: 2387 occurrences (0.10%)\n",
      "  11. -73.874983: 2313 occurrences (0.09%)\n",
      "  12. -73.954784: 2295 occurrences (0.09%)\n",
      "  13. -73.946904: 2216 occurrences (0.09%)\n",
      "  14. -73.929984: 2151 occurrences (0.09%)\n",
      "  15. -73.961781: 2079 occurrences (0.08%)\n",
      "  ... and 125502 more unique values\n",
      "\n",
      "Numeric statistics:\n",
      "  Count: 2496759\n",
      "  Mean: -73.9228\n",
      "  Std Dev: 0.2078\n",
      "  Min: -74.254741\n",
      "  25%: -73.97273609\n",
      "  Median (50%): -73.925712\n",
      "  75%: -73.879477\n",
      "  Max: 0.0\n",
      "\n",
      "=== Column: OFNS_DESC ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 68\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. PETIT LARCENY: 498316 occurrences (19.96%)\n",
      "  2. HARRASSMENT 2: 392254 occurrences (15.71%)\n",
      "  3. ASSAULT 3 & RELATED OFFENSES: 265096 occurrences (10.62%)\n",
      "  4. GRAND LARCENY: 224958 occurrences (9.01%)\n",
      "  5. CRIMINAL MISCHIEF & RELATED OF: 216408 occurrences (8.67%)\n",
      "  6. FELONY ASSAULT: 125434 occurrences (5.02%)\n",
      "  7. OFF. AGNST PUB ORD SENSBLTY &: 86773 occurrences (3.48%)\n",
      "  8. MISCELLANEOUS PENAL LAW: 78981 occurrences (3.16%)\n",
      "  9. ROBBERY: 77887 occurrences (3.12%)\n",
      "  10. VEHICLE AND TRAFFIC LAWS: 75416 occurrences (3.02%)\n",
      "  11. BURGLARY: 70900 occurrences (2.84%)\n",
      "  12. GRAND LARCENY OF MOTOR VEHICLE: 63067 occurrences (2.53%)\n",
      "  13. DANGEROUS DRUGS: 52656 occurrences (2.11%)\n",
      "  14. SEX CRIMES: 34998 occurrences (1.40%)\n",
      "  15. DANGEROUS WEAPONS: 34346 occurrences (1.38%)\n",
      "  ... and 53 more unique values\n",
      "\n",
      "=== Column: PARKS_NM ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 998\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. (NULL): 2481020 occurrences (99.37%)\n",
      "  2. WASHINGTON SQUARE PARK: 1240 occurrences (0.05%)\n",
      "  3. FLUSHING MEADOWS CORONA PARK: 1109 occurrences (0.04%)\n",
      "  4. CENTRAL PARK: 1106 occurrences (0.04%)\n",
      "  5. CONEY ISLAND BEACH & BOARDWALK: 589 occurrences (0.02%)\n",
      "  6. SARA D. ROOSEVELT PARK: 344 occurrences (0.01%)\n",
      "  7. UNION SQUARE PARK: 332 occurrences (0.01%)\n",
      "  8. PROSPECT PARK: 329 occurrences (0.01%)\n",
      "  9. HUDSON RIVER PARK: 329 occurrences (0.01%)\n",
      "  10. BRYANT PARK: 326 occurrences (0.01%)\n",
      "  11. RIVERSIDE PARK: 254 occurrences (0.01%)\n",
      "  12. MARCUS GARVEY PARK: 225 occurrences (0.01%)\n",
      "  13. TOMPKINS SQUARE PARK: 199 occurrences (0.01%)\n",
      "  14. RANDALL'S ISLAND PARK: 187 occurrences (0.01%)\n",
      "  15. SUNSET PARK: 182 occurrences (0.01%)\n",
      "  ... and 983 more unique values\n",
      "\n",
      "=== Column: PD_CD ===\n",
      "Data type: float64\n",
      "Unique values (non-NaN): 405\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. 638.0: 294638 occurrences (11.80%)\n",
      "  2. 333.0: 225309 occurrences (9.02%)\n",
      "  3. 101.0: 200075 occurrences (8.01%)\n",
      "  4. 637.0: 97616 occurrences (3.91%)\n",
      "  5. 109.0: 94879 occurrences (3.80%)\n",
      "  6. 639.0: 79786 occurrences (3.20%)\n",
      "  7. 259.0: 61424 occurrences (2.46%)\n",
      "  8. 254.0: 53648 occurrences (2.15%)\n",
      "  9. 113.0: 49717 occurrences (1.99%)\n",
      "  10. 321.0: 49531 occurrences (1.98%)\n",
      "  11. 352.0: 48210 occurrences (1.93%)\n",
      "  12. 198.0: 47941 occurrences (1.92%)\n",
      "  13. 441.0: 47409 occurrences (1.90%)\n",
      "  14. 916.0: 40565 occurrences (1.62%)\n",
      "  15. 338.0: 35628 occurrences (1.43%)\n",
      "  ... and 390 more unique values\n",
      "\n",
      "Numeric statistics:\n",
      "  Count: 2496759\n",
      "  Mean: 408.4761\n",
      "  Std Dev: 221.1384\n",
      "  Min: 100.0\n",
      "  25%: 254.0\n",
      "  Median (50%): 352.0\n",
      "  75%: 638.0\n",
      "  Max: 969.0\n",
      "\n",
      "=== Column: PREM_TYP_DESC ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 79\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. STREET: 731832 occurrences (29.31%)\n",
      "  2. RESIDENCE - APT. HOUSE: 552635 occurrences (22.13%)\n",
      "  3. RESIDENCE-HOUSE: 234082 occurrences (9.38%)\n",
      "  4. RESIDENCE - PUBLIC HOUSING: 165658 occurrences (6.63%)\n",
      "  5. CHAIN STORE: 123550 occurrences (4.95%)\n",
      "  6. TRANSIT - NYC SUBWAY: 66399 occurrences (2.66%)\n",
      "  7. DEPARTMENT STORE: 60630 occurrences (2.43%)\n",
      "  8. COMMERCIAL BUILDING: 54120 occurrences (2.17%)\n",
      "  9. DRUG STORE: 52980 occurrences (2.12%)\n",
      "  10. OTHER: 48324 occurrences (1.94%)\n",
      "  11. GROCERY/BODEGA: 37938 occurrences (1.52%)\n",
      "  12. UNKNOWN: 35590 occurrences (1.43%)\n",
      "  13. HOMELESS SHELTER: 27168 occurrences (1.09%)\n",
      "  14. RESTAURANT/DINER: 25402 occurrences (1.02%)\n",
      "  15. CLOTHING/BOUTIQUE: 21983 occurrences (0.88%)\n",
      "  ... and 64 more unique values\n",
      "\n",
      "=== Column: SUSP_AGE_GROUP ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 6\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. UNKNOWN: 1345854 occurrences (53.90%)\n",
      "  2. 25-44: 672305 occurrences (26.93%)\n",
      "  3. 45-64: 230399 occurrences (9.23%)\n",
      "  4. 18-24: 176892 occurrences (7.08%)\n",
      "  5. <18: 46253 occurrences (1.85%)\n",
      "  6. 65+: 25056 occurrences (1.00%)\n",
      "\n",
      "=== Column: SUSP_RACE ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 7\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. UNKNOWN: 1036043 occurrences (41.50%)\n",
      "  2. BLACK: 741235 occurrences (29.69%)\n",
      "  3. WHITE HISPANIC: 343510 occurrences (13.76%)\n",
      "  4. WHITE: 182059 occurrences (7.29%)\n",
      "  5. BLACK HISPANIC: 113270 occurrences (4.54%)\n",
      "  6. ASIAN / PACIFIC ISLANDER: 75900 occurrences (3.04%)\n",
      "  7. AMERICAN INDIAN/ALASKAN NATIVE: 4742 occurrences (0.19%)\n",
      "\n",
      "=== Column: SUSP_SEX ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 3\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. M: 1230362 occurrences (49.28%)\n",
      "  2. U: 931945 occurrences (37.33%)\n",
      "  3. F: 334452 occurrences (13.40%)\n",
      "\n",
      "=== Column: VIC_AGE_GROUP ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 6\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. 25-44: 895506 occurrences (35.87%)\n",
      "  2. UNKNOWN: 711535 occurrences (28.50%)\n",
      "  3. 45-64: 473191 occurrences (18.95%)\n",
      "  4. 18-24: 209258 occurrences (8.38%)\n",
      "  5. 65+: 122290 occurrences (4.90%)\n",
      "  6. <18: 84979 occurrences (3.40%)\n",
      "\n",
      "=== Column: VIC_RACE ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 7\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. UNKNOWN: 764147 occurrences (30.61%)\n",
      "  2. BLACK: 616026 occurrences (24.67%)\n",
      "  3. WHITE HISPANIC: 443561 occurrences (17.77%)\n",
      "  4. WHITE: 357517 occurrences (14.32%)\n",
      "  5. ASIAN / PACIFIC ISLANDER: 197541 occurrences (7.91%)\n",
      "  6. BLACK HISPANIC: 108103 occurrences (4.33%)\n",
      "  7. AMERICAN INDIAN/ALASKAN NATIVE: 9864 occurrences (0.40%)\n",
      "\n",
      "=== Column: VIC_SEX ===\n",
      "Data type: object\n",
      "Unique values (non-NaN): 5\n",
      "Missing values (NaN): 0\n",
      "\n",
      "Top values (showing max 15):\n",
      "  1. F: 953382 occurrences (38.18%)\n",
      "  2. M: 864994 occurrences (34.64%)\n",
      "  3. D: 409833 occurrences (16.41%)\n",
      "  4. E: 258785 occurrences (10.36%)\n",
      "  5. L: 9765 occurrences (0.39%)\n",
      "\n",
      "Clean dataset scan completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.11 SCAN OF CLEANED DATASET\")\n",
    "\n",
    "if original_row_count > 0 and 'df' in locals() and len(df) > 0: # Proceed only if df exists and has rows\n",
    "    print(\"Analyzing the values in each column after cleaning...\")\n",
    "\n",
    "    # Function to limit long string values for display purposes\n",
    "    def format_value_for_display(val, max_len=50):\n",
    "        if pd.isna(val):\n",
    "            return \"<NaN>\"\n",
    "        val_str = str(val)\n",
    "        if len(val_str) > max_len:\n",
    "            return val_str[:max_len - 3] + \"...\"\n",
    "        return val_str\n",
    "\n",
    "    # Function to analyze and display column value information\n",
    "    def analyze_column_values_post_cleaning(df, column_name):\n",
    "        print(f\"\\n=== Column: {column_name} ===\")\n",
    "        col_data = df[column_name]\n",
    "        num_rows = len(col_data)\n",
    "\n",
    "        # Get data type and count of unique values\n",
    "        dtype = col_data.dtype\n",
    "        unique_count = col_data.nunique()\n",
    "        nan_count = col_data.isna().sum()\n",
    "        nan_percent = (nan_count / num_rows) * 100 if num_rows > 0 else 0\n",
    "\n",
    "        print(f\"Data type: {dtype}\")\n",
    "        print(f\"Unique values (non-NaN): {unique_count}\")\n",
    "        if nan_count > 0:\n",
    "            print(f\"Missing values (NaN): {nan_count} ({nan_percent:.2f}%)\")\n",
    "        else:\n",
    "            print(\"Missing values (NaN): 0\")\n",
    "\n",
    "        # Get value counts (including NaN if present)\n",
    "        try:\n",
    "             value_counts = col_data.value_counts(dropna=False)\n",
    "             # Display top values\n",
    "             print(f\"\\nTop values (showing max 15):\")\n",
    "             num_to_show = min(15, len(value_counts))\n",
    "\n",
    "             for i, (value, count) in enumerate(value_counts.iloc[:num_to_show].items()):\n",
    "                 percent = (count / num_rows) * 100 if num_rows > 0 else 0\n",
    "                 formatted_value = format_value_for_display(value)\n",
    "                 print(f\"  {i+1}. {formatted_value}: {count} occurrences ({percent:.2f}%)\")\n",
    "\n",
    "             if len(value_counts) > num_to_show:\n",
    "                 remaining = len(value_counts) - num_to_show\n",
    "                 print(f\"  ... and {remaining} more unique values\")\n",
    "\n",
    "        except Exception as e:\n",
    "             # Handle potential errors with unhashable types if data is very messy\n",
    "             print(f\"\\nCould not display value counts for {column_name}: {e}\")\n",
    "\n",
    "        # For numeric columns, show basic statistics\n",
    "        if pd.api.types.is_numeric_dtype(col_data) and not col_data.isna().all():\n",
    "            print(\"\\nNumeric statistics:\")\n",
    "            try:\n",
    "                stats = col_data.describe()\n",
    "                print(f\"  Count: {int(stats['count'])}\")\n",
    "                print(f\"  Mean: {stats['mean']:.4f}\")\n",
    "                print(f\"  Std Dev: {stats['std']:.4f}\")\n",
    "                print(f\"  Min: {stats['min']}\")\n",
    "                print(f\"  25%: {stats['25%']}\")\n",
    "                print(f\"  Median (50%): {stats['50%']}\")\n",
    "                print(f\"  75%: {stats['75%']}\")\n",
    "                print(f\"  Max: {stats['max']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not calculate numeric statistics for this column: {e}\")\n",
    "        elif pd.api.types.is_numeric_dtype(col_data):\n",
    "             print(\"\\nNumeric statistics: Column is numeric but contains only NaN values.\")\n",
    "\n",
    "    # Analyze each column in the cleaned dataframe\n",
    "    print(\"\\nScanning all columns in the cleaned dataset...\")\n",
    "    for column in df.columns:\n",
    "        analyze_column_values_post_cleaning(df, column)\n",
    "    print(\"\\nClean dataset scan completed.\")\n",
    "elif 'df' in locals() and len(df) == 0:\n",
    "    print(\"Skipping scan as the cleaned dataset has no rows.\")\n",
    "else:\n",
    "    print(\"Skipping scan due to DataFrame loading or processing issues.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean_save_md"
   },
   "source": [
    "## Save cleaned data\n",
    "\n",
    "Persist the cleaned dataset as cleaned_crime_data_processed.csv for the Data Integration notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39019,
     "status": "ok",
     "timestamp": 1748159623756,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "clean_save_code",
    "outputId": "2dd6ae93-0f6a-4dc1-b77a-814bee3dfea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.12 SAVING CLEANED DATASET\n",
      "Cleaned dataset successfully saved to: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Processed\\cleaned_crime_data_processed.csv\n",
      "Final dimensions: 2496759 rows, 18 columns\n",
      "\n",
      "=== CLEANING SECTION COMPLETED ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2.12 SAVING CLEANED DATASET\")\n",
    "\n",
    "if original_row_count > 0 and 'df' in locals() and len(df) > 0: # Proceed only if df exists and has rows\n",
    "    # Define the output file path\n",
    "    cleaned_output_file_name = \"cleaned_crime_data_processed.csv\"\n",
    "    # Ensure processed_dir is defined from the Setup section\n",
    "    if 'processed_dir' in locals() and os.path.isdir(processed_dir):\n",
    "        cleaned_file_path = os.path.join(processed_dir, cleaned_output_file_name)\n",
    "        try:\n",
    "            # Save the cleaned dataframe to CSV\n",
    "            df.to_csv(cleaned_file_path, index=False)\n",
    "            print(f\"Cleaned dataset successfully saved to: {cleaned_file_path}\")\n",
    "            print(f\"Final dimensions: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cleaned dataset: {e}\")\n",
    "    else:\n",
    "        print(f\"Error: Output directory '{processed_dir}' not found or not defined. Cannot save file.\")\n",
    "elif 'df' in locals() and len(df) == 0:\n",
    "    print(\"Skipping save as the cleaned dataset has no rows.\")\n",
    "else:\n",
    "    print(\"Skipping save due to DataFrame loading or processing issues.\")\n",
    "\n",
    "print(\"\\n=== CLEANING SECTION COMPLETED ===\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
