{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2Hj0Iuf1MyZ"
   },
   "source": [
    "# Setup\n",
    "\n",
    "This section covers the initial setup, including library imports, path definitions, and mounting Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41DtTmv71Myi"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2510,
     "status": "ok",
     "timestamp": 1748178712046,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "Tuu7pkI9UfzG",
    "outputId": "4946fc35-bf52-44c3-98e6-975637ac92d7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sSfOzCv1Myp"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3007,
     "status": "ok",
     "timestamp": 1748178715066,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "1LacEnEqU9Le",
    "outputId": "a887bf07-3312-4cdc-8f12-7c1a2dd03052"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "# !pip install openpyxl\n",
    "import openpyxl\n",
    "# !pip install holidays\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "\n",
    "# Enhanced ML imports for better preprocessing pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKclXTRD1Mys"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1748178715118,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "TiQNMfDG1Myt",
    "outputId": "10b92807-ca09-4996-8a5b-c9efbe9a4357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\n",
      "Feature engineering output directory: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\FeatureEngineered\n",
      "Looking for cleaned integrated data at: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\DataIntegrated\\cleaned_integrated_crime_data.csv\n",
      "Looking for PD codes file at: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\..\\Documents\\PDCode_PenalLaw.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Path variables\n",
    "# base_dir = \"/drive/MyDrive/Data Mining and Machine Learning/Progetto\"\n",
    "base_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), \"JupyterOutputs\")\n",
    "cleaned_integrated_dir = os.path.join(base_dir, \"DataIntegrated\")\n",
    "cleaned_integrated_data_file = os.path.join(cleaned_integrated_dir, \"cleaned_integrated_crime_data.csv\")\n",
    "feature_engineering_dir = os.path.join(base_dir, \"FeatureEngineered\")\n",
    "feature_engineered_file_path = os.path.join(feature_engineering_dir, \"feature_engineered_crime_data.csv\")\n",
    "pd_codes_file = os.path.join(os.path.dirname(os.path.dirname(base_dir)), \"Documents\", \"PDCode_PenalLaw.xlsx\")\n",
    "\n",
    "# Check if pd_codes_file exists\n",
    "if not os.path.exists(pd_codes_file):\n",
    "    raise FileNotFoundError(f\"PD codes file not found at: {pd_codes_file}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(feature_engineering_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "print(f\"Feature engineering output directory: {feature_engineering_dir}\")\n",
    "print(f\"Looking for cleaned integrated data at: {cleaned_integrated_data_file}\")\n",
    "print(f\"Looking for PD codes file at: {pd_codes_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o_3FSAu1Myv"
   },
   "source": [
    "# Load Integrated Data\n",
    "\n",
    "Load the dataset produced by the Data Integration phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12966,
     "status": "ok",
     "timestamp": 1748178728085,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "hQa_cHI51Myw",
    "outputId": "0445f303-999a-4007-814f-8f66325564f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Integrated Data ===\n",
      "Dataset loaded successfully: 2496759 rows and 34 columns\n",
      "Columns in the dataset: ['BORO_NM', 'CMPLNT_FR_DT', 'CMPLNT_FR_TM', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PARKS_NM', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT', 'SCHOOLS_COUNT', 'METRO_DISTANCE', 'MIN_POI_DISTANCE', 'AVG_POI_DISTANCE', 'MAX_POI_DISTANCE', 'TOTAL_POI_COUNT', 'POI_DIVERSITY', 'POI_DENSITY_SCORE']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"=== Loading Integrated Data ===\")\n",
    "try:\n",
    "    if os.path.exists(cleaned_integrated_data_file):\n",
    "        df = pd.read_csv(cleaned_integrated_data_file)\n",
    "        initial_rows = len(df)\n",
    "        print(f\"Dataset loaded successfully: {initial_rows} rows and {df.shape[1]} columns\")\n",
    "        print(f\"Columns in the dataset: {df.columns.tolist()}\")\n",
    "\n",
    "        # Basic validation\n",
    "        if initial_rows == 0:\n",
    "            raise ValueError(\"Dataset is empty\")\n",
    "        if df.shape[1] < 5:\n",
    "            raise ValueError(\"Dataset has insufficient columns\")\n",
    "\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find cleaned integrated dataset at: {cleaned_integrated_data_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise RuntimeError(f\"Failed to load required dataset for feature engineering: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-YTL1oE1Myy"
   },
   "source": [
    "## Initial Data Overview\n",
    "\n",
    "Display basic information, summary statistics, and a sample of the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1765,
     "status": "ok",
     "timestamp": 1748178729879,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "h2oYsWIX1Myy",
    "outputId": "562f587f-ea73-4be6-f2e2-0332525a1cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Overview ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2496759 entries, 0 to 2496758\n",
      "Data columns (total 34 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   BORO_NM             object \n",
      " 1   CMPLNT_FR_DT        object \n",
      " 2   CMPLNT_FR_TM        object \n",
      " 3   KY_CD               int64  \n",
      " 4   LAW_CAT_CD          object \n",
      " 5   LOC_OF_OCCUR_DESC   object \n",
      " 6   OFNS_DESC           object \n",
      " 7   PARKS_NM            object \n",
      " 8   PD_CD               int64  \n",
      " 9   PREM_TYP_DESC       object \n",
      " 10  SUSP_AGE_GROUP      object \n",
      " 11  SUSP_RACE           object \n",
      " 12  SUSP_SEX            object \n",
      " 13  VIC_AGE_GROUP       object \n",
      " 14  VIC_RACE            object \n",
      " 15  VIC_SEX             object \n",
      " 16  Latitude            float64\n",
      " 17  Longitude           float64\n",
      " 18  BAR_DISTANCE        float64\n",
      " 19  NIGHTCLUB_DISTANCE  float64\n",
      " 20  ATM_DISTANCE        float64\n",
      " 21  ATMS_COUNT          float64\n",
      " 22  BARS_COUNT          float64\n",
      " 23  BUS_STOPS_COUNT     float64\n",
      " 24  METROS_COUNT        float64\n",
      " 25  NIGHTCLUBS_COUNT    float64\n",
      " 26  SCHOOLS_COUNT       float64\n",
      " 27  METRO_DISTANCE      float64\n",
      " 28  MIN_POI_DISTANCE    float64\n",
      " 29  AVG_POI_DISTANCE    float64\n",
      " 30  MAX_POI_DISTANCE    float64\n",
      " 31  TOTAL_POI_COUNT     float64\n",
      " 32  POI_DIVERSITY       int64  \n",
      " 33  POI_DENSITY_SCORE   float64\n",
      "dtypes: float64(17), int64(3), object(14)\n",
      "memory usage: 647.7+ MB\n",
      "None\n",
      "\n",
      "=== Summary Statistics ===\n",
      "              KY_CD         PD_CD      Latitude     Longitude  BAR_DISTANCE  \\\n",
      "count  2.496759e+06  2.496759e+06  2.496759e+06  2.496759e+06  2.496759e+06   \n",
      "mean   3.025155e+02  4.084761e+02  4.073661e+01 -7.392281e+01  9.304923e+02   \n",
      "std    1.595039e+02  2.211384e+02  1.344466e-01  2.077602e-01  3.581217e+04   \n",
      "min    1.020000e+02  1.000000e+02  0.000000e+00 -7.425474e+01  4.102764e+00   \n",
      "25%    1.170000e+02  2.540000e+02  4.067561e+01 -7.397274e+01  2.132541e+02   \n",
      "50%    3.410000e+02  3.520000e+02  4.073495e+01 -7.392571e+01  5.848092e+02   \n",
      "75%    3.510000e+02  6.380000e+02  4.081194e+01 -7.387948e+01  1.180303e+03   \n",
      "max    8.810000e+02  9.690000e+02  4.091271e+01  0.000000e+00  1.372163e+07   \n",
      "\n",
      "       NIGHTCLUB_DISTANCE  ATM_DISTANCE    ATMS_COUNT    BARS_COUNT  \\\n",
      "count        2.496759e+06  2.496759e+06  2.496742e+06  2.496742e+06   \n",
      "mean         2.115418e+03  1.444943e+03  1.463507e-02  6.523982e-02   \n",
      "std          3.584537e+04  3.582254e+04  1.318330e-01  3.305598e-01   \n",
      "min          5.889870e+00  9.675179e+00  0.000000e+00  0.000000e+00   \n",
      "25%          7.718525e+02  4.615127e+02  0.000000e+00  0.000000e+00   \n",
      "50%          1.524675e+03  1.065681e+03  0.000000e+00  0.000000e+00   \n",
      "75%          2.537621e+03  1.878874e+03  0.000000e+00  0.000000e+00   \n",
      "max          1.372083e+07  1.372270e+07  5.000000e+00  6.000000e+00   \n",
      "\n",
      "       BUS_STOPS_COUNT  METROS_COUNT  NIGHTCLUBS_COUNT  SCHOOLS_COUNT  \\\n",
      "count     2.496742e+06  2.496742e+06      2.496742e+06   2.496742e+06   \n",
      "mean      4.109676e-01  5.793270e-02      4.794648e-03   2.806698e-02   \n",
      "std       7.815628e-01  2.529411e-01      7.512655e-02   2.069132e-01   \n",
      "min       0.000000e+00  0.000000e+00      0.000000e+00   0.000000e+00   \n",
      "25%       0.000000e+00  0.000000e+00      0.000000e+00   0.000000e+00   \n",
      "50%       0.000000e+00  0.000000e+00      0.000000e+00   0.000000e+00   \n",
      "75%       1.000000e+00  0.000000e+00      0.000000e+00   0.000000e+00   \n",
      "max       6.000000e+00  2.000000e+00      2.000000e+00   7.000000e+00   \n",
      "\n",
      "       METRO_DISTANCE  MIN_POI_DISTANCE  AVG_POI_DISTANCE  MAX_POI_DISTANCE  \\\n",
      "count    2.496759e+06      2.496759e+06      2.496759e+06      2.496759e+06   \n",
      "mean     6.870371e+02      4.815086e+02      1.294473e+03      2.450955e+03   \n",
      "std      3.580790e+04      3.580184e+04      3.581097e+04      3.584812e+04   \n",
      "min      1.336284e-01      1.336284e-01      4.315403e+01      7.243958e+01   \n",
      "25%      1.926433e+02      1.212059e+02      5.940558e+02      1.178856e+03   \n",
      "50%      3.482616e+02      2.584415e+02      9.681934e+02      1.878874e+03   \n",
      "75%      6.627598e+02      4.907645e+02      1.593445e+03      2.938993e+03   \n",
      "max      1.372000e+07      1.372000e+07      1.372129e+07      1.372270e+07   \n",
      "\n",
      "       TOTAL_POI_COUNT  POI_DIVERSITY  POI_DENSITY_SCORE  \n",
      "count     2.496759e+06   2.496759e+06       2.496759e+06  \n",
      "mean      5.816328e-01   4.172161e-01       6.462587e-02  \n",
      "std       1.009304e+00   6.428944e-01       1.121449e-01  \n",
      "min       0.000000e+00   0.000000e+00       0.000000e+00  \n",
      "25%       0.000000e+00   0.000000e+00       0.000000e+00  \n",
      "50%       0.000000e+00   0.000000e+00       0.000000e+00  \n",
      "75%       1.000000e+00   1.000000e+00       1.111111e-01  \n",
      "max       9.000000e+00   4.000000e+00       1.000000e+00  \n",
      "\n",
      "=== Sample Row ===\n",
      "        BORO_NM CMPLNT_FR_DT  CMPLNT_FR_TM  KY_CD LAW_CAT_CD  \\\n",
      "2085206  QUEENS   2022/03/15  06:30:00.000    578  VIOLATION   \n",
      "\n",
      "        LOC_OF_OCCUR_DESC      OFNS_DESC PARKS_NM  PD_CD  \\\n",
      "2085206            INSIDE  HARRASSMENT 2   (NULL)    637   \n",
      "\n",
      "                  PREM_TYP_DESC  ... METROS_COUNT NIGHTCLUBS_COUNT  \\\n",
      "2085206  RESIDENCE - APT. HOUSE  ...          0.0              0.0   \n",
      "\n",
      "        SCHOOLS_COUNT METRO_DISTANCE MIN_POI_DISTANCE AVG_POI_DISTANCE  \\\n",
      "2085206           0.0    1158.635538       988.785484      1585.535707   \n",
      "\n",
      "         MAX_POI_DISTANCE  TOTAL_POI_COUNT  POI_DIVERSITY  POI_DENSITY_SCORE  \n",
      "2085206       2907.344456              0.0              0                0.0  \n",
      "\n",
      "[1 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display basic dataset overview\n",
    "print(\"\\n=== Dataset Overview ===\")\n",
    "print(df.info())\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(df.describe())\n",
    "print(\"\\n=== Sample Row ===\")\n",
    "print(df.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiEUP0bl1Myz"
   },
   "source": [
    "# Load External PD Codes\n",
    "\n",
    "Load the external Excel file containing mappings between PD codes and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1748178730498,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "9GFR487mVSuk",
    "outputId": "7189dc67-8bcf-4cdf-8b3a-49ebff3a6317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading External PD Codes ===\n",
      "PD Codes dataset loaded successfully: 4671 rows and 5 columns\n",
      "Columns in the codes dataset: ['PDCODE_VALUE', 'LAW_NYS', 'CATEGORY', 'LIT_LONG', 'LIT_SHORT']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Loading External PD Codes ===\")\n",
    "try:\n",
    "    if os.path.exists(pd_codes_file):\n",
    "        df_codes = pd.read_excel(pd_codes_file)\n",
    "        initial_rows = len(df_codes)\n",
    "        print(f\"PD Codes dataset loaded successfully: {initial_rows} rows and {df_codes.shape[1]} columns\")\n",
    "        print(f\"Columns in the codes dataset: {df_codes.columns.tolist()}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find PD codes dataset at: {pd_codes_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PD codes dataset: {e}\")\n",
    "    # Instead of exit(1), print the error and continue\n",
    "    print(f\"Error details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiG2vYHD1My1"
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This section focuses on creating new features and refining existing ones based on domain knowledge and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBOSQZPJ1My1"
   },
   "source": [
    "## 1. Impute Missing `OFNS_DESC`\n",
    "\n",
    "Fill missing offense descriptions (`OFNS_DESC`) using the mapping from `PD_CD` to `LIT_SHORT` found in the external codes file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1436,
     "status": "ok",
     "timestamp": 1748178731941,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "tIIijAchmG0T",
    "outputId": "1b8a2c88-d318-402d-db36-6f91f4083fc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: Imputing OFNS_DESC ===\n",
      "OFNS_DESC missing before imputation: 44\n",
      "OFNS_DESC missing after imputation:  0\n",
      "Number of values filled: 44\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: Imputing OFNS_DESC ===\")\n",
    "# Ensure both codes are of type string\n",
    "df['PD_CD'] = df['PD_CD'].astype(str)\n",
    "df_codes['PDCODE_VALUE'] = df_codes['PDCODE_VALUE'].astype(str)\n",
    "\n",
    "# Create a mapping dictionary from the first occurrence of each code\n",
    "code_to_lit_short = df_codes.drop_duplicates(subset='PDCODE_VALUE').set_index('PDCODE_VALUE')['LIT_SHORT'].to_dict()\n",
    "\n",
    "# Fill OFNS_DESC where it's '(null)' using the mapping\n",
    "null_mask = df['OFNS_DESC'] == '(null)'\n",
    "initial_nulls = df['OFNS_DESC'].isin(['(null)', None, np.nan]).sum()\n",
    "df.loc[null_mask, 'OFNS_DESC'] = (\n",
    "    df.loc[null_mask, 'PD_CD'].map(code_to_lit_short)\n",
    "    .str.strip()\n",
    "    .fillna(df.loc[null_mask, 'OFNS_DESC']) # Keep original if map fails\n",
    ")\n",
    "\n",
    "# count missing after imputation\n",
    "final_nulls = df['OFNS_DESC'].isin(['(null)', None, np.nan]).sum()\n",
    "\n",
    "# print summary\n",
    "print(f\"OFNS_DESC missing before imputation: {initial_nulls}\")\n",
    "print(f\"OFNS_DESC missing after imputation:  {final_nulls}\")\n",
    "print(f\"Number of values filled: {initial_nulls - final_nulls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nj6cDf2-1My2"
   },
   "source": [
    "## 2. Create Temporal Features\n",
    "\n",
    "Extract time-based features from the complaint date and time:\n",
    "- `HOUR`: Hour of the day (0-23)\n",
    "- `WEEKDAY`: Name of the day (e.g., MONDAY)\n",
    "- `IS_WEEKEND`: Binary flag (1 for Saturday/Sunday, 0 otherwise)\n",
    "- `MONTH`: Month of the year (1-12)\n",
    "- `SEASON`: Categorical season (WINTER, SPRING, SUMMER, AUTUMN)\n",
    "- `TIME_BUCKET`: Categorical time of day (NIGHT, MORNING, AFTERNOON, EVENING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5281,
     "status": "ok",
     "timestamp": 1748178737233,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "ZlJflmjWRDb0",
    "outputId": "74faa24a-8b28-4180-d099-752777246fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: Temporal Features ===\n",
      "Created temporal features: HOUR, DAY, WEEKDAY, IS_WEEKEND, MONTH, YEAR, SEASON, TIME_BUCKET\n",
      "Columns after adding temporal features: ['BORO_NM', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PARKS_NM', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT', 'SCHOOLS_COUNT', 'METRO_DISTANCE', 'MIN_POI_DISTANCE', 'AVG_POI_DISTANCE', 'MAX_POI_DISTANCE', 'TOTAL_POI_COUNT', 'POI_DIVERSITY', 'POI_DENSITY_SCORE', 'HOUR', 'DAY', 'WEEKDAY', 'IS_WEEKEND', 'MONTH', 'YEAR', 'SEASON', 'TIME_BUCKET']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: Temporal Features ===\")\n",
    "# 1) Unified timestamp (temporary)\n",
    "df['TIMESTAMP'] = pd.to_datetime(\n",
    "    df['CMPLNT_FR_DT'] + ' ' + df['CMPLNT_FR_TM'],\n",
    "    format='%Y/%m/%d %H:%M:%S.%f', # Adjusted format based on previous steps\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Handle potential coercion errors (if any)\n",
    "invalid_timestamps = df['TIMESTAMP'].isna().sum()\n",
    "if invalid_timestamps > 0:\n",
    "    print(f\"Warning: {invalid_timestamps} rows had invalid date/time formats and resulted in NaT timestamps.\")\n",
    "\n",
    "# 2) Hour of the day\n",
    "df['HOUR'] = df['TIMESTAMP'].dt.hour\n",
    "\n",
    "# 3) Day\n",
    "df['DAY'] = df['TIMESTAMP'].dt.day\n",
    "\n",
    "# 4) Weekday\n",
    "df['WEEKDAY'] = df['TIMESTAMP'].dt.day_name().str.upper()\n",
    "\n",
    "# 5) Flag weekend (1 = Saturday/Sunday)\n",
    "df['IS_WEEKEND'] = df['WEEKDAY'].isin(['SATURDAY', 'SUNDAY']).astype(int)\n",
    "\n",
    "# 6) Month\n",
    "df['MONTH'] = df['TIMESTAMP'].dt.month\n",
    "\n",
    "# 7) Year\n",
    "df['YEAR'] = df['TIMESTAMP'].dt.year\n",
    "\n",
    "# 8) Season\n",
    "def map_season(month):\n",
    "    if pd.isna(month): return 'UNKNOWN' # Handle potential NaNs from NaT timestamps\n",
    "    if month in [12, 1, 2]: return 'WINTER'\n",
    "    elif month in [3, 4, 5]: return 'SPRING'\n",
    "    elif month in [6, 7, 8]: return 'SUMMER'\n",
    "    else: return 'AUTUMN'\n",
    "df['SEASON'] = df['MONTH'].apply(map_season)\n",
    "\n",
    "# 9) Time Bucket\n",
    "def time_bucket(hour):\n",
    "    if pd.isna(hour): return 'UNKNOWN' # Handle potential NaNs\n",
    "    if hour < 6: return 'NIGHT'\n",
    "    elif hour < 12: return 'MORNING'\n",
    "    elif hour < 18: return 'AFTERNOON'\n",
    "    else: return 'EVENING'\n",
    "df['TIME_BUCKET'] = df['HOUR'].apply(time_bucket)\n",
    "\n",
    "# Drop intermediate and original time columns\n",
    "df = df.drop(columns=['CMPLNT_FR_TM', 'CMPLNT_FR_DT', 'TIMESTAMP'])\n",
    "print(\"Created temporal features: HOUR, DAY, WEEKDAY, IS_WEEKEND, MONTH, YEAR, SEASON, TIME_BUCKET\")\n",
    "print(f\"Columns after adding temporal features: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz8JIr4zLXfo"
   },
   "source": [
    "## 2.1. Create Holiday and Payday Features\n",
    "\n",
    "Create binary flags for holidays and assumed paydays to capture potential temporal patterns related to these events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8731,
     "status": "ok",
     "timestamp": 1748178745966,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "7zLOtg-xLXfo",
    "outputId": "3f8f7a53-44e5-4df2-fe2b-1512d05ab0d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: Holiday and Payday Features ===\n",
      "Created IS_HOLIDAY feature. 82010 instances on a holiday.\n",
      "Created IS_PAYDAY feature. 180576 instances on an assumed payday.\n",
      "Columns after adding holiday/payday features: ['BORO_NM', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PARKS_NM', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT', 'SCHOOLS_COUNT', 'METRO_DISTANCE', 'MIN_POI_DISTANCE', 'AVG_POI_DISTANCE', 'MAX_POI_DISTANCE', 'TOTAL_POI_COUNT', 'POI_DIVERSITY', 'POI_DENSITY_SCORE', 'HOUR', 'DAY', 'WEEKDAY', 'IS_WEEKEND', 'MONTH', 'YEAR', 'SEASON', 'TIME_BUCKET', 'IS_HOLIDAY', 'IS_PAYDAY']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: Holiday and Payday Features ===\")\n",
    "# Recreate a datetime series from YEAR, MONTH, DAY for holiday and payday checking\n",
    "# Ensure YEAR, MONTH, DAY are present and are not NaN before attempting conversion\n",
    "if 'YEAR' in df.columns and 'MONTH' in df.columns and 'DAY' in df.columns:\n",
    "    # Create a temporary date column, handling potential errors by setting to NaT\n",
    "    # This is crucial if YEAR, MONTH, DAY could form invalid dates (e.g., Feb 30)\n",
    "    # However, pd.to_datetime will handle standard invalid dates by raising errors if not coerced.\n",
    "    # For safety, let's ensure components are integer and handle NaNs that might arise from prior steps.\n",
    "    date_components = df[['YEAR', 'MONTH', 'DAY']].dropna()\n",
    "    df_dates = pd.to_datetime(date_components, errors='coerce')\n",
    "\n",
    "    # Initialize US holidays\n",
    "    us_holidays = holidays.US(years=df_dates.dt.year.unique())\n",
    "\n",
    "    # Create IS_HOLIDAY column\n",
    "    # Apply to the original df index to ensure alignment\n",
    "    df['IS_HOLIDAY'] = 0\n",
    "    df.loc[df_dates.index, 'IS_HOLIDAY'] = df_dates.dt.date.apply(lambda date: 1 if date in us_holidays else 0).astype(int)\n",
    "    print(f\"Created IS_HOLIDAY feature. {df['IS_HOLIDAY'].sum()} instances on a holiday.\")\n",
    "\n",
    "    # Create IS_PAYDAY column (assuming 1st and 15th of the month)\n",
    "    df['IS_PAYDAY'] = 0\n",
    "    df.loc[df_dates.index, 'IS_PAYDAY'] = df_dates.apply(lambda x: 1 if x.day == 1 or x.day == 15 else 0).astype(int)\n",
    "    print(f\"Created IS_PAYDAY feature. {df['IS_PAYDAY'].sum()} instances on an assumed payday.\")\n",
    "else:\n",
    "    print(\"Warning: YEAR, MONTH, or DAY column not found. Skipping IS_HOLIDAY and IS_PAYDAY creation.\")\n",
    "\n",
    "print(f\"Columns after adding holiday/payday features: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSKpGluI1My4"
   },
   "source": [
    "## 3. Refine Location Features\n",
    "\n",
    "Improve location-related features using imputation and rule-based logic:\n",
    "- Impute missing `PREM_TYP_DESC` based on proximity to POIs (Bar, Nightclub, ATM, Metro) and offense type (`OFNS_DESC`).\n",
    "- Group `PARKS_NM` entries into `PREM_TYP_DESC` as 'PARK/PLAYGROUND'.\n",
    "- Impute missing `LOC_OF_OCCUR_DESC` based on `PREM_TYP_DESC` and `OFNS_DESC`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lno3XGwD1My4"
   },
   "source": [
    "### 3.1 Impute `PREM_TYP_DESC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3925,
     "status": "ok",
     "timestamp": 1748178749979,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "sicU4WxF1My4",
    "outputId": "afae5e07-85cc-4609-9d87-f43721abbfe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: Refining Location Features - Impute PREM_TYP_DESC ===\n",
      "Initial missing/placeholder PREM_TYP_DESC: 35590\n",
      "Imputed 2225 PREM_TYP_DESC based on POI proximity (<= 30m).\n",
      "Imputed 12816 PREM_TYP_DESC as 'STREET' based on OFNS_DESC.\n",
      "Filled remaining 20549 missing PREM_TYP_DESC with 'OTHER'.\n",
      "PREM_TYP_DESC missing/placeholders after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: Refining Location Features - Impute PREM_TYP_DESC ===\")\n",
    "# 1) Identify rows where PREM_TYP_DESC is missing or placeholder\n",
    "missing_prem_placeholders = ['(NULL)', None, np.nan, 'UNKNOWN'] # Include UNKNOWN from previous cleaning\n",
    "mask_null_prem = df['PREM_TYP_DESC'].isin(missing_prem_placeholders)\n",
    "initial_missing_prem = mask_null_prem.sum()\n",
    "print(f\"Initial missing/placeholder PREM_TYP_DESC: {initial_missing_prem}\")\n",
    "\n",
    "# 2) Compute nearest POI and its distance (helper columns)\n",
    "poi_cols = ['BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'METRO_DISTANCE']\n",
    "df['NEAREST_POI_DIST'] = df[poi_cols].min(axis=1)\n",
    "df['NEAREST_POI_TYPE'] = df[poi_cols].idxmin(axis=1)\n",
    "\n",
    "# Mapping from distance column name to PREM_TYP_DESC value\n",
    "distance_to_prem = {\n",
    "    'BAR_DISTANCE': 'BAR/NIGHT CLUB',\n",
    "    'NIGHTCLUB_DISTANCE': 'BAR/NIGHT CLUB',\n",
    "    'ATM_DISTANCE': 'ATM',\n",
    "    'METRO_DISTANCE': 'TRANSIT - NYC SUBWAY'\n",
    "}\n",
    "\n",
    "# 3) Distance-based imputation: only if nearest POI is within 30 meters\n",
    "mask_impute_by_dist = mask_null_prem & (df['NEAREST_POI_DIST'] <= 30)\n",
    "df.loc[mask_impute_by_dist, 'PREM_TYP_DESC'] = df.loc[mask_impute_by_dist, 'NEAREST_POI_TYPE'].map(distance_to_prem)\n",
    "imputed_by_dist_count = mask_impute_by_dist.sum()\n",
    "print(f\"Imputed {imputed_by_dist_count} PREM_TYP_DESC based on POI proximity (<= 30m).\")\n",
    "\n",
    "# Drop helper columns\n",
    "df.drop(columns=['NEAREST_POI_DIST', 'NEAREST_POI_TYPE'], inplace=True)\n",
    "\n",
    "# 4) Fallback using OFNS_DESC for street-related crimes\n",
    "mask_still_null_prem = df['PREM_TYP_DESC'].isin(missing_prem_placeholders)\n",
    "street_crime_keywords = ['BURGLARY', 'ROBBERY', 'ASSAULT', 'GRAND LARCENY', 'PETIT LARCENY', 'UNAUTHORIZED USE OF A VEHICLE']\n",
    "mask_impute_by_offense = mask_still_null_prem & df['OFNS_DESC'].str.contains('|'.join(street_crime_keywords), na=False, case=False)\n",
    "df.loc[mask_impute_by_offense, 'PREM_TYP_DESC'] = 'STREET'\n",
    "imputed_by_offense_count = mask_impute_by_offense.sum()\n",
    "print(f\"Imputed {imputed_by_offense_count} PREM_TYP_DESC as 'STREET' based on OFNS_DESC.\")\n",
    "\n",
    "# 5) Final default for any remaining missing values\n",
    "mask_final_null_prem = df['PREM_TYP_DESC'].isin(missing_prem_placeholders)\n",
    "final_default_count = mask_final_null_prem.sum()\n",
    "df.loc[mask_final_null_prem, 'PREM_TYP_DESC'] = 'OTHER'\n",
    "print(f\"Filled remaining {final_default_count} missing PREM_TYP_DESC with 'OTHER'.\")\n",
    "\n",
    "# Final check\n",
    "final_missing_prem = df['PREM_TYP_DESC'].isin(missing_prem_placeholders).sum()\n",
    "print(f\"PREM_TYP_DESC missing/placeholders after imputation: {final_missing_prem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ah5Cdf0o1My5"
   },
   "source": [
    "### 3.2 Consolidate Park Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 944,
     "status": "ok",
     "timestamp": 1748178751012,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "IbvIjHx81My5",
    "outputId": "1fcd2dcf-c2c1-4444-da03-b52bab6260f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: Refining Location Features - Consolidate Park Info ===\n",
      "Updated 15739 PREM_TYP_DESC entries to 'PARK/PLAYGROUND' based on PARKS_NM.\n",
      "Dropped PARKS_NM column.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: Refining Location Features - Consolidate Park Info ===\")\n",
    "# Group PARKS_NM into PREM_TYP_DESC\n",
    "if 'PARKS_NM' in df.columns:\n",
    "    park_mask = df['PARKS_NM'].notna() & (df['PARKS_NM'] != '(NULL)') & (df['PARKS_NM'] != 'UNKNOWN')\n",
    "    park_update_count = park_mask.sum()\n",
    "    df.loc[park_mask, 'PREM_TYP_DESC'] = 'PARK/PLAYGROUND'\n",
    "    print(f\"Updated {park_update_count} PREM_TYP_DESC entries to 'PARK/PLAYGROUND' based on PARKS_NM.\")\n",
    "    # Drop the original PARKS_NM column as it's now consolidated\n",
    "    df.drop(columns=['PARKS_NM'], inplace=True)\n",
    "    print(\"Dropped PARKS_NM column.\")\n",
    "else:\n",
    "    print(\"PARKS_NM column not found, skipping consolidation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gwV91q31My6"
   },
   "source": [
    "### 3.3 Impute `LOC_OF_OCCUR_DESC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2488,
     "status": "ok",
     "timestamp": 1748178753532,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "zn8gKULH1My6",
    "outputId": "4ca0fb87-58d1-44d4-98e7-6a2e78a495c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: Refining Location Features - Impute LOC_OF_OCCUR_DESC ===\n",
      "Initial missing/placeholder LOC_OF_OCCUR_DESC: 464374\n",
      "Imputed 378151 LOC_OF_OCCUR_DESC based on PREM_TYP_DESC mapping.\n",
      "Imputed 15240 LOC_OF_OCCUR_DESC as 'INSIDE' based on OFNS_DESC.\n",
      "Filled remaining 70983 missing LOC_OF_OCCUR_DESC with 'UNKNOWN'.\n",
      "LOC_OF_OCCUR_DESC missing/placeholders after imputation: 70983\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: Refining Location Features - Impute LOC_OF_OCCUR_DESC ===\")\n",
    "# 1) Identify missing/placeholder values\n",
    "missing_loc_placeholders = ['(NULL)', None, np.nan, 'UNKNOWN']\n",
    "mask_null_loc = df['LOC_OF_OCCUR_DESC'].isin(missing_loc_placeholders)\n",
    "initial_missing_loc = mask_null_loc.sum()\n",
    "print(f\"Initial missing/placeholder LOC_OF_OCCUR_DESC: {initial_missing_loc}\")\n",
    "\n",
    "# 2) Rule-based mapping from PREM_TYP_DESC to LOC_OF_OCCUR_DESC\n",
    "prem_to_loc = {\n",
    "    # indoor locations\n",
    "    'GROCERY/BODEGA':                     'INSIDE',\n",
    "    'RESIDENCE - APT. HOUSE':             'INSIDE',\n",
    "    'RESIDENCE-HOUSE':                    'INSIDE',\n",
    "    'RESIDENCE - PUBLIC HOUSING':         'INSIDE',\n",
    "    'DEPARTMENT STORE':                   'INSIDE',\n",
    "    'CHAIN STORE':                        'INSIDE',\n",
    "    'DRUG STORE':                         'INSIDE',\n",
    "    'FOOD SUPERMARKET':                   'INSIDE',\n",
    "    'COMMERCIAL BUILDING':                'INSIDE',\n",
    "    'BANK':                               'INSIDE',\n",
    "    'PUBLIC BUILDING':                    'INSIDE',\n",
    "    'HOTEL/MOTEL':                        'INSIDE',\n",
    "    'HOMELESS SHELTER':                   'INSIDE',\n",
    "    'PUBLIC SCHOOL':                      'INSIDE',\n",
    "    'PRIVATE/PAROCHIAL SCHOOL':           'INSIDE',\n",
    "    'COLLEGE/UNIVERSITY':                 'INSIDE',\n",
    "    'HOSPITAL':                           'INSIDE',\n",
    "    'DOCTOR/DENTIST OFFICE':              'INSIDE',\n",
    "    'GYM/FITNESS FACILITY':               'INSIDE',\n",
    "    'BAR/NIGHT CLUB':                     'INSIDE',\n",
    "    'RESTAURANT/DINER':                   'INSIDE',\n",
    "    'FAST FOOD':                          'INSIDE',\n",
    "    'DRY CLEANER/LAUNDRY':                'INSIDE',\n",
    "    'BEAUTY & NAIL SALON':                'INSIDE',\n",
    "    'CLOTHING/BOUTIQUE':                  'INSIDE',\n",
    "    'JEWELRY':                            'INSIDE',\n",
    "    'PHOTO/COPY':                         'INSIDE',\n",
    "    'VIDEO STORE':                        'INSIDE',\n",
    "    'STORE UNCLASSIFIED':                 'INSIDE',\n",
    "    'SMALL MERCHANT':                     'INSIDE',\n",
    "    'CANDY STORE':                        'INSIDE',\n",
    "    'VARIETY STORE':                      'INSIDE',\n",
    "    'SHOE':                               'INSIDE',\n",
    "    'CHECK CASHING BUSINESS':             'INSIDE',\n",
    "    'STORAGE FACILITY':                   'INSIDE',\n",
    "    'REAL ESTATE':                        'INSIDE',\n",
    "    'SOCIAL CLUB/POLICY':                 'INSIDE',\n",
    "    'OTHER HOUSE OF WORSHIP':             'INSIDE',\n",
    "    'CHURCH':                             'INSIDE',\n",
    "    'SYNAGOGUE':                          'INSIDE',\n",
    "    'MOSQUE':                             'INSIDE',\n",
    "    'DAYCARE FACILITY':                   'INSIDE',\n",
    "    'ABANDONED BUILDING':                 'INSIDE',\n",
    "    'LOAN COMPANY':                       'INSIDE',\n",
    "    'TAXI (YELLOW LICENSED)':             'INSIDE',\n",
    "    'TAXI (LIVERY LICENSED)':             'INSIDE',\n",
    "    'TAXI/LIVERY (UNLICENSED)':           'INSIDE',\n",
    "    'BUS (NYC TRANSIT)':                  'INSIDE',\n",
    "    'BUS (OTHER)':                        'INSIDE',\n",
    "    'TRANSIT FACILITY (OTHER)':           'INSIDE',\n",
    "    'AIRPORT TERMINAL':                   'INSIDE',\n",
    "    'FERRY/FERRY TERMINAL':               'INSIDE',\n",
    "\n",
    "    # outdoor or semi‐outdoor locations\n",
    "    'STREET':                             'REAR',\n",
    "    'HIGHWAY/PARKWAY':                    'REAR',\n",
    "    'TUNNEL':                             'REAR',\n",
    "    'PARK/PLAYGROUND':                    'REAR',\n",
    "    'OPEN AREAS (OPEN LOTS)':             'REAR',\n",
    "    'PARKING LOT/GARAGE (PUBLIC)':        'REAR',\n",
    "    'PARKING LOT/GARAGE (PRIVATE)':       'REAR',\n",
    "    'GAS STATION':                        'REAR',\n",
    "    'ATM':                                'REAR',\n",
    "    'BUS STOP':                           'REAR',\n",
    "    'BUS TERMINAL':                       'REAR',\n",
    "    'TRAMWAY':                            'REAR',\n",
    "    'BRIDGE':                             'REAR',\n",
    "    'MARINA/PIER':                        'REAR',\n",
    "    'CEMETERY':                           'REAR',\n",
    "    'CONSTRUCTION SITE':                  'REAR',\n",
    "    'MOBILE FOOD':                        'REAR',\n",
    "    'FERRY/FERRY TERMINAL':               'REAR',  # also treated as terminal interior above\n",
    "\n",
    "    # catch-all for others\n",
    "    'OTHER':                              'UNKNOWN',\n",
    "    'STORE UNCLASSIFIED':                 'UNKNOWN',\n",
    "    'TRANSIT - NYC SUBWAY':               'UNKNOWN' # Can be inside station, on platform, or on train\n",
    "}\n",
    "\n",
    "# Apply the mapping to fill missing LOC_OF_OCCUR_DESC\n",
    "df.loc[mask_null_loc, 'LOC_OF_OCCUR_DESC'] = df.loc[mask_null_loc, 'PREM_TYP_DESC'].map(prem_to_loc)\n",
    "imputed_by_prem_count = mask_null_loc.sum() - df['LOC_OF_OCCUR_DESC'].isin(missing_loc_placeholders).sum()\n",
    "print(f\"Imputed {imputed_by_prem_count} LOC_OF_OCCUR_DESC based on PREM_TYP_DESC mapping.\")\n",
    "\n",
    "# 3) Refinement using OFNS_DESC for potential inside crimes\n",
    "mask_still_null_loc = df['LOC_OF_OCCUR_DESC'].isin(missing_loc_placeholders)\n",
    "inside_crime_keywords = ['BURGLARY', 'TRESPASS', 'ROBBERY', 'ASSAULT'] # Keywords suggesting inside location\n",
    "mask_impute_by_offense_loc = mask_still_null_loc & df['OFNS_DESC'].str.contains('|'.join(inside_crime_keywords), na=False, case=False)\n",
    "df.loc[mask_impute_by_offense_loc, 'LOC_OF_OCCUR_DESC'] = 'INSIDE'\n",
    "imputed_by_offense_loc_count = mask_impute_by_offense_loc.sum()\n",
    "print(f\"Imputed {imputed_by_offense_loc_count} LOC_OF_OCCUR_DESC as 'INSIDE' based on OFNS_DESC.\")\n",
    "\n",
    "# 4) Final default for any remaining missing values\n",
    "mask_final_null_loc = df['LOC_OF_OCCUR_DESC'].isin(missing_loc_placeholders)\n",
    "final_default_loc_count = mask_final_null_loc.sum()\n",
    "df.loc[mask_final_null_loc, 'LOC_OF_OCCUR_DESC'] = 'UNKNOWN'\n",
    "print(f\"Filled remaining {final_default_loc_count} missing LOC_OF_OCCUR_DESC with 'UNKNOWN'.\")\n",
    "\n",
    "# Final check\n",
    "final_missing_loc = df['LOC_OF_OCCUR_DESC'].isin(missing_loc_placeholders).sum()\n",
    "print(f\"LOC_OF_OCCUR_DESC missing/placeholders after imputation: {final_missing_loc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPL1WSeHfQ70"
   },
   "source": [
    "## 5. Create Demographic Interaction Features\n",
    "\n",
    "Generate features indicating whether the suspect and victim share the same demographic characteristics:\n",
    "- `SAME_AGE_GROUP`: 1 if suspect and victim age groups match, 0 otherwise.\n",
    "- `SAME_SEX`: 1 if suspect and victim sexes match, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1748178754397,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "KGyrhN7V1My8",
    "outputId": "3677c43c-2f94-4ca2-a3e0-bf712db9f6f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: Demographic Interaction Features ===\n",
      "Created SAME_AGE_GROUP feature. 388596 instances where age groups match.\n",
      "Created SAME_SEX feature. 515461 instances where sexes match.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: Demographic Interaction Features ===\")\n",
    "# 1) SAME_AGE_GROUP: 1 if suspect and victim share the same age group, else 0\n",
    "df['SAME_AGE_GROUP'] = 0\n",
    "# Define valid (non-null, non-placeholder) age-group rows\n",
    "valid_age_placeholders = ['UNKNOWN', '(NULL)', None, np.nan]\n",
    "age_valid = (~df['SUSP_AGE_GROUP'].isin(valid_age_placeholders)) & (~df['VIC_AGE_GROUP'].isin(valid_age_placeholders))\n",
    "# Set flag where age groups match\n",
    "df.loc[age_valid & (df['SUSP_AGE_GROUP'] == df['VIC_AGE_GROUP']), 'SAME_AGE_GROUP'] = 1\n",
    "print(f\"Created SAME_AGE_GROUP feature. {df['SAME_AGE_GROUP'].sum()} instances where age groups match.\")\n",
    "\n",
    "# 2) SAME_SEX: 1 if suspect and victim have the same sex, else 0\n",
    "df['SAME_SEX'] = 0\n",
    "# Define valid (non-null, non-placeholder) sex rows\n",
    "valid_sex_placeholders = ['U', 'UNKNOWN', '(NULL)', None, np.nan] # 'U' often means Unknown\n",
    "sex_valid = (~df['SUSP_SEX'].isin(valid_sex_placeholders)) & (~df['VIC_SEX'].isin(valid_sex_placeholders))\n",
    "# Set flag where sexes match\n",
    "df.loc[sex_valid & (df['SUSP_SEX'] == df['VIC_SEX']), 'SAME_SEX'] = 1\n",
    "print(f\"Created SAME_SEX feature. {df['SAME_SEX'].sum()} instances where sexes match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1748178754492,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "TrrxSdbY1My9",
    "outputId": "cb162e4c-4676-4523-e53d-c4b6a4396831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: TO_CHECK_CITIZENS Feature ===\n",
      "Created TO_CHECK_CITIZENS feature. 2270901 instances flagged.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: TO_CHECK_CITIZENS Feature ===\")\n",
    "# List of offense descriptions that trigger the flag\n",
    "to_check_citizens_list = [\n",
    "    'CRIMINAL TRESPASS', 'CRIMINAL MISCHIEF & RELATED OF', 'HARRASSMENT 2',\n",
    "    'GRAND LARCENY OF MOTOR VEHICLE', 'PETIT LARCENY', 'GRAND LARCENY',\n",
    "    'INTOXICATED & IMPAIRED DRIVING', 'FRAUDS', 'THEFT-FRAUD',\n",
    "    'OFF. AGNST PUB ORD SENSBLTY &', 'BURGLARY', 'ROBBERY', 'FELONY ASSAULT',\n",
    "    'ASSAULT 3 & RELATED OFFENSES', 'DANGEROUS DRUGS', 'RAPE', 'SEX CRIMES',\n",
    "    'DANGEROUS WEAPONS', 'ARSON', 'POSSESSION OF STOLEN PROPERTY',\n",
    "    'UNAUTHORIZED USE OF A VEHICLE', 'FORGERY', 'ENDAN WELFARE INCOMP',\n",
    "    'OTHER OFFENSES RELATED TO THEF', 'AGRICULTURE & MRKTS LAW-UNCLASSIFIED',\n",
    "    'KIDNAPPING & RELATED OFFENSES', 'OTHER OFFENSES RELATED TO THEFT', # Duplicate, keep one\n",
    "    'OFFENSES RELATED TO CHILDREN', 'BURGLAR\\S TOOLS', 'ESCAPE 3',\n",
    "    'CANNABIS RELATED OFFENSES', 'PETIT LARCENY OF MOTOR VEHICLE',\n",
    "    'FELONY SEX CRIMES', 'PROSTITUTION & RELATED OFFENSES', 'JOSTLING',\n",
    "    'KIDNAPPING', 'DISORDERLY CONDUCT', 'INTOXICATED/IMPAIRED DRIVING',\n",
    "    'DISRUPTION OF A RELIGIOUS SERV', 'FRAUDULENT ACCOSTING', 'THEFT OF SERVICES',\n",
    "    'UNRSNBLE NOISE', 'OTHER TRAFFIC INFRACTION', 'LOITERING/GAMBLING (CARDS, DIC',\n",
    "    'UNLAWFUL POSS. WEAP. ON SCHOOL', 'FORTUNE TELLING', 'LOITERING',\n",
    "    'FAIL RPT WOUNDS'\n",
    "]\n",
    "# Remove duplicates from the list\n",
    "to_check_citizens_list = list(dict.fromkeys(to_check_citizens_list))\n",
    "\n",
    "# Create the boolean column based on the condition\n",
    "df['TO_CHECK_CITIZENS'] = df['OFNS_DESC'].isin(to_check_citizens_list).astype(int)\n",
    "print(f\"Created TO_CHECK_CITIZENS feature. {df['TO_CHECK_CITIZENS'].sum()} instances flagged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1748178770730,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "oG9DLJLtkwgk",
    "outputId": "e1b96931-030e-4c94-a1e0-9060a135d9df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dropping Rows with NaN values ===\n",
      "Initial number of rows: 2496759\n",
      "Number of rows after dropping NaNs: 2496742\n",
      "Number of rows dropped: 17\n",
      "Current DataFrame shape: (2496742, 44)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Dropping Rows with NaN values ===\")\n",
    "initial_rows = len(df)\n",
    "print(f\"Initial number of rows: {initial_rows}\")\n",
    "\n",
    "# Drop rows that contain any NaN value\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "final_rows = len(df_cleaned)\n",
    "rows_dropped = initial_rows - final_rows\n",
    "\n",
    "print(f\"Number of rows after dropping NaNs: {final_rows}\")\n",
    "print(f\"Number of rows dropped: {rows_dropped}\")\n",
    "\n",
    "# Update the main DataFrame reference to the cleaned one\n",
    "df = df_cleaned\n",
    "\n",
    "print(f\"Current DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65943,
     "status": "ok",
     "timestamp": 1748179011283,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "Bg9rAWr8lXox",
    "outputId": "d7e787a6-a618-496e-f0f1-8dac1bbfe3b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saving Feature Engineered Dataset (Before Scaling/PCA) ===\n",
      "Feature engineered dataset (before scaling/PCA) saved successfully to: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\FeatureEngineered\\feature_engineered_crime_data.csv\n",
      "Dataset shape: (2496742, 44)\n",
      "Columns in the dataset: ['BORO_NM', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT', 'SCHOOLS_COUNT', 'METRO_DISTANCE', 'MIN_POI_DISTANCE', 'AVG_POI_DISTANCE', 'MAX_POI_DISTANCE', 'TOTAL_POI_COUNT', 'POI_DIVERSITY', 'POI_DENSITY_SCORE', 'HOUR', 'DAY', 'WEEKDAY', 'IS_WEEKEND', 'MONTH', 'YEAR', 'SEASON', 'TIME_BUCKET', 'IS_HOLIDAY', 'IS_PAYDAY', 'SAME_AGE_GROUP', 'SAME_SEX', 'TO_CHECK_CITIZENS']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Saving Feature Engineered Dataset (Before Scaling/PCA) ===\")\n",
    "# Ensure feature_engineering_dir and feature_engineered_file_path are defined\n",
    "\n",
    "if 'df' in globals() and 'feature_engineered_file_path' in globals() and 'os' in globals():\n",
    "    try:\n",
    "        df.to_csv(feature_engineered_file_path, index=False)\n",
    "        print(f\"Feature engineered dataset (before scaling/PCA) saved successfully to: {feature_engineered_file_path}\")\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Columns in the dataset: {df.columns.tolist()}\")\n",
    "    except NameError as ne:\n",
    "        print(f\"A NameError occurred while trying to save: {ne}. Ensure 'df' and path variables are correctly defined.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving feature engineered dataset (before scaling/PCA): {e}\")\n",
    "else:\n",
    "    print(\"Error: 'df' or 'feature_engineered_file_path' or 'os' not found in globals. Cannot save the intermediate DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGk4xFeu1MzA"
   },
   "source": [
    "## Attribute Type Classification\n",
    "\n",
    "Below is a classification of each column in the final feature-engineered dataset according to its data type and semantic meaning, *before* encoding and transformation. This helps guide the next steps in the Data Reduction and Transformation phase.\n",
    "\n",
    "| Column                | Data Type                     | Details/Notes                                  | Transformation Needed | Encoding Type (if needed) |\n",
    "|-----------------------|-------------------------------|------------------------------------------------|-----------------------|---------------------------|\n",
    "| BORO_NM               | Categorical (nominal)         | Borough name                                   | Encoding              | One-Hot               |         |\n",
    "| KY_CD            | Categorical (nominal)         | Crime classification code | Encoding              | One-Hot                   |       |\n",
    "| LAW_CAT_CD            | Categorical (ordinal)         | Crime level (FELONY > MISDEMEANOR > VIOLATION) | Encoding              | Ordinal                   |       |\n",
    "| LOC_OF_OCCUR_DESC     | Categorical (nominal)         | Location description (INSIDE, FRONT, etc.)     | Encoding              | One-Hot                   |\n",
    "| OFNS_DESC             | Categorical (nominal)         | Offense description                            | Encoding              | One-Hot (High Cardinality)|\n",
    "| PD_CD                 | Categorical (nominal)         | Detailed police code                           | Encoding              | One-Hot (High Cardinality)|\n",
    "| PREM_TYP_DESC         | Categorical (nominal)         | Premises type description                      | Encoding              | One-Hot (High Cardinality)|\n",
    "| SUSP_AGE_GROUP        | Categorical (ordinal)         | Age group (e.g., '<18', '18-24', 'UNKNOWN')    | Encoding              | Ordinal                   |\n",
    "| SUSP_RACE             | Categorical (nominal)         | Suspect race                                   | Encoding              | One-Hot                   |\n",
    "| SUSP_SEX              | Categorical (nominal)         | Suspect sex (M/F/U)                            | Encoding              | One-Hot                   |\n",
    "| VIC_AGE_GROUP         | Categorical (ordinal)         | Age group (e.g., '<18', '18-24', 'UNKNOWN')    | Encoding              | Ordinal                   |\n",
    "| VIC_RACE              | Categorical (nominal)         | Victim race                                    | Encoding              | One-Hot                   |\n",
    "| VIC_SEX               | Categorical (nominal)         | Victim sex (M/F/D/E/U)                         | Encoding              | One-Hot                   |\n",
    "| LATITUDE              | Numeric (continuous)          | Geographic coordinate                          | Scaling               | -                         |\n",
    "| LONGITUDE             | Numeric (continuous)          | Geographic coordinate                          | Scaling               | -                         |\n",
    "| BAR_DISTANCE          | Numeric (continuous)          | Distance to nearest bar (meters)               | Scaling               | -                         |\n",
    "| NIGHTCLUB_DISTANCE    | Numeric (continuous)          | Distance to nearest nightclub (meters)         | Scaling               | -                         |\n",
    "| ATM_DISTANCE          | Numeric (continuous)          | Distance to nearest ATM (meters)               | Scaling               | -                         |\n",
    "| METRO_DISTANCE        | Numeric (continuous)          | Distance to nearest metro station (meters)     | Scaling               | -                         |\n",
    "| ATMS_COUNT            | Numeric (discrete)            | Count of ATMs nearby                           | Scaling               | -                         |\n",
    "| BARS_COUNT            | Numeric (discrete)            | Count of bars nearby                           | Scaling               | -                         |\n",
    "| BUS_STOPS_COUNT       | Numeric (discrete)            | Count of bus stops nearby                      | Scaling               | -                         |\n",
    "| METROS_COUNT | Numeric (discrete)            | Count of metro stations nearby                 | Scaling               | -                         |\n",
    "| NIGHTCLUBS_COUNT      | Numeric (discrete)            | Count of nightclubs nearby                     | Scaling               | -                         |\n",
    "| SCHOOLS_COUNT         | Numeric (discrete)            | Count of schools nearby                        | Scaling               | -                         |\n",
    "| HOUR                  | Numeric (cyclical)            | Hour of the day (0-23)                         | Encoding              | Cyclical (Sine/Cosine)    |\n",
    "| DAY                   | Numeric (cyclical)            | Day of the month (1-31)                        | Encoding              | Cyclical (Sine/Cosine)    |\n",
    "| WEEKDAY               | Categorical (cyclical)        | Day of the week (MON-SUN)                      | Encoding              | Cyclical (Sine/Cosine)    |\n",
    "| IS_WEEKEND            | Binary                        | 0 = weekday, 1 = weekend                       | None                  | -                         |\n",
    "| MONTH                 | Numeric (cyclical)            | Month of the year (1-12)                       | Encoding              | Cyclical (Sine/Cosine)    |\n",
    "| SEASON                | Categorical (nominal)         | Season (WINTER, SPRING, SUMMER, AUTUMN)        | Encoding              | One-Hot                   |\n",
    "| TIME_BUCKET           | Categorical (nominal)         | Time of day (NIGHT, MORNING, AFTERNOON, EVENING) | Encoding              | One-Hot                   |\n",
    "| YEAR                  | Numeric (discrete)            | Year of the event                              | Scaling               | -                         |\n",
    "| IS_HOLIDAY            | Binary                        | 0 = not a holiday, 1 = holiday                 | None                  | -                         |\n",
    "| IS_PAYDAY             | Binary                        | 0 = not a payday, 1 = payday (1st or 15th)     | None                  | -                         |\n",
    "| SAME_AGE_GROUP        | Binary                        | 1 if suspect/victim age groups match, else 0   | None                  | -                         |\n",
    "| SAME_SEX              | Binary                        | 1 if suspect/victim sexes match, else 0        | None                  | -                         |\n",
    "| TO_CHECK_CITIZENS     | Binary                        | 1 if offense is relevant for citizens, else 0 | None                  | -                         |\n",
    "| TOTAL_POI_COUNT       | Numeric (discrete)            | Sum of all POI counts  | Scaling               | -                         |\n",
    "| POI_DIVERSITY         | Numeric (discrete)            | Number of different POI types present          | Scaling               | -                         |\n",
    "| POI_DENSITY_SCORE      | Numeric (continuous)           | Density score                           | Scaling                  | -                             |\n",
    "| MIN_POI_DISTANCE      | Numeric (continuous)          | Distance to closest POI of any type            | Scaling               | -                         |\n",
    "| AVG_POI_DISTANCE      | Numeric (continuous)          | Average distance to POIs                       | Scaling               | -                         |\n",
    "| MAX_POI_DISTANCE       | Numeric (continuous)           | Maximum distance to POIs                       | Scaling                  | -                             |\n",
    "\n",
    "### Reasons for Cyclical/Ordinal Encoding\n",
    "\n",
    "Cyclical and Ordinal encoding are valuable techniques for representing categorical features while preserving inherent relationships and limiting dimensionality increase compared to One-Hot encoding.\n",
    "- **Cyclical Encoding:** This is particularly useful for features where the end of the sequence connects back to the beginning, such as time-related attributes (`HOUR`, `DAY`, `WEEKDAY`, `MONTH`). For example, hour 23 is conceptually close to hour 0, December is close to January. Cyclical encoding (often using sine and cosine transformations) captures this wrap-around nature, preventing the model from incorrectly interpreting these values as distant.\n",
    "- **Ordinal Encoding:** This method is suitable for features with a clear, inherent order or ranking (`LAW_CAT_CD`, `SUSP_AGE_GROUP`, `VIC_AGE_GROUP`). Assigning sequential integers (e.g., 0, 1, 2...) allows the model to understand the relative magnitude or progression between categories (e.g., 'FELONY' > 'MISDEMEANOR', '<18' < '18-24').\n",
    "\n",
    "### Reasons for One-Hot Encoding\n",
    "\n",
    "One-Hot encoding is applied to nominal categorical features where there is no inherent order or ranking among the categories (`BORO_NM`, `KY_CD`, `LOC_OF_OCCUR_DESC`, `OFNS_DESC`, `PD_CD`, `PREM_TYP_DESC`, `SUSP_RACE`, `SUSP_SEX`, `VIC_RACE`, `VIC_SEX`, `SEASON`, `TIME_BUCKET`). It creates new binary (0 or 1) columns for each unique category in the original feature. This prevents the model from assuming any ordinal relationship between categories that doesn't exist. While it can significantly increase the number of features (especially for high-cardinality columns like `OFNS_DESC`), it ensures that each category is treated independently.\n",
    "\n",
    "### Reasons for Scaling\n",
    "\n",
    "Scaling is essential for numeric features (`Latitude`, `Longitude`, distance columns, count columns, `YEAR`) that have different ranges or units. Many machine learning algorithms (especially those based on distance calculations like k-NN or SVM, or those using gradient descent like linear regression and neural networks) are sensitive to the scale of input features. Features with larger values might disproportionately influence the model's learning process. Scaling methods like Standardization (Z-score normalization) or Min-Max scaling transform the data to a common scale (e.g., mean 0 and standard deviation 1, or range [0, 1]), ensuring that all features contribute more equally to the model's outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlz4ElIQeU6w"
   },
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1412,
     "status": "ok",
     "timestamp": 1748179058669,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "g4PlXBCbeU6w",
    "outputId": "d16463a3-adcf-486f-84ad-aca67977f63b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: Ordinal Encoding ===\n",
      "LAW_CAT_CD categories → ['VIOLATION', 'MISDEMEANOR', 'FELONY']\n",
      "SUSP_AGE_GROUP categories → ['<18', '18-24', '25-44', '45-64', '65+', 'UNKNOWN']\n",
      "VIC_AGE_GROUP categories → ['<18', '18-24', '25-44', '45-64', '65+', 'UNKNOWN']\n",
      "Ordinal encoding applied to: ['LAW_CAT_CD', 'SUSP_AGE_GROUP', 'VIC_AGE_GROUP']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: Ordinal Encoding ===\")\n",
    "\n",
    "# Ordinal categories for correct severity order\n",
    "law_cat_order = ['VIOLATION', 'MISDEMEANOR', 'FELONY']\n",
    "age_group_order = ['<18', '18-24', '25-44', '45-64', '65+', 'UNKNOWN']\n",
    "\n",
    "enc = OrdinalEncoder(categories=[law_cat_order, age_group_order, age_group_order])\n",
    "cols_to_encode = ['LAW_CAT_CD', 'SUSP_AGE_GROUP', 'VIC_AGE_GROUP']\n",
    "df[cols_to_encode] = enc.fit_transform(df[cols_to_encode]).astype(int)\n",
    "\n",
    "for feature, cats in zip(cols_to_encode, enc.categories_):\n",
    "    print(f\"{feature} categories → {list(cats)}\")\n",
    "\n",
    "print(f\"Ordinal encoding applied to: {cols_to_encode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AavtHCEmeU6w"
   },
   "source": [
    "## Cyclical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1906,
     "status": "ok",
     "timestamp": 1748179060640,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "0iYkluoheU6w",
    "outputId": "4d94005f-d7f5-4331-b451-684e0c55786e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: Cyclical Encoding ===\n",
      "Created 'WEEKDAY_NUM' from 'WEEKDAY'.\n",
      "Dropped temporary intermediate columns: ['WEEKDAY_NUM']\n",
      "Dropped original cyclical source columns: ['HOUR', 'DAY', 'WEEKDAY', 'MONTH']\n",
      "Cyclical encoding (sin/cos) applied to: HOUR, DAY, WEEKDAY, MONTH.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: Cyclical Encoding ===\")\n",
    "\n",
    "weekday_order    = ['MONDAY','TUESDAY','WEDNESDAY','THURSDAY','FRIDAY','SATURDAY','SUNDAY']\n",
    "\n",
    "# Create WEEKDAY_NUM if WEEKDAY column exists\n",
    "if 'WEEKDAY' in df.columns:\n",
    "    df['WEEKDAY_NUM'] = df['WEEKDAY'].map({d:i for i,d in enumerate(weekday_order)})\n",
    "    print(\"Created 'WEEKDAY_NUM' from 'WEEKDAY'.\")\n",
    "else:\n",
    "    print(\"Warning: 'WEEKDAY' column not found, cannot create 'WEEKDAY_NUM'.\")\n",
    "\n",
    "# Apply cyclical encoding if source columns exist\n",
    "encoded_features = []\n",
    "original_cyclical_cols_to_drop = []\n",
    "\n",
    "if 'HOUR' in df.columns:\n",
    "    df['HOUR_SIN']         = np.sin(2*np.pi*df['HOUR']/24)\n",
    "    df['HOUR_COS']         = np.cos(2*np.pi*df['HOUR']/24)\n",
    "    encoded_features.append('HOUR')\n",
    "    original_cyclical_cols_to_drop.append('HOUR')\n",
    "else:\n",
    "    print(\"Warning: 'HOUR' column not found, skipping its cyclical encoding.\")\n",
    "\n",
    "if 'DAY' in df.columns:\n",
    "    df['DAY_SIN']          = np.sin(2*np.pi*df['DAY']/31) # Using 31 as max days in a month\n",
    "    df['DAY_COS']          = np.cos(2*np.pi*df['DAY']/31)\n",
    "    encoded_features.append('DAY')\n",
    "    original_cyclical_cols_to_drop.append('DAY')\n",
    "else:\n",
    "    print(\"Warning: 'DAY' column not found, skipping its cyclical encoding.\")\n",
    "\n",
    "if 'WEEKDAY_NUM' in df.columns: # Relies on WEEKDAY_NUM\n",
    "    df['WEEKDAY_SIN']      = np.sin(2*np.pi*df['WEEKDAY_NUM']/7)\n",
    "    df['WEEKDAY_COS']      = np.cos(2*np.pi*df['WEEKDAY_NUM']/7)\n",
    "    encoded_features.append('WEEKDAY')\n",
    "    if 'WEEKDAY' in df.columns: # Ensure original WEEKDAY (string) is also dropped\n",
    "        original_cyclical_cols_to_drop.append('WEEKDAY')\n",
    "else:\n",
    "    print(\"Warning: 'WEEKDAY_NUM' column not found, skipping WEEKDAY cyclical encoding.\")\n",
    "\n",
    "if 'MONTH' in df.columns:\n",
    "    df['MONTH_SIN']        = np.sin(2*np.pi*df['MONTH']/12)\n",
    "    df['MONTH_COS']        = np.cos(2*np.pi*df['MONTH']/12)\n",
    "    encoded_features.append('MONTH')\n",
    "    original_cyclical_cols_to_drop.append('MONTH')\n",
    "else:\n",
    "    print(\"Warning: 'MONTH' column not found, skipping its cyclical encoding.\")\n",
    "\n",
    "# Drop temporary and original cyclical columns that were created and exist\n",
    "columns_to_drop_intermediate = ['WEEKDAY_NUM']\n",
    "existing_intermediate_to_drop = [col for col in columns_to_drop_intermediate if col in df.columns]\n",
    "if existing_intermediate_to_drop:\n",
    "    df.drop(columns=existing_intermediate_to_drop, inplace=True)\n",
    "    print(f\"Dropped temporary intermediate columns: {existing_intermediate_to_drop}\")\n",
    "\n",
    "existing_original_cyclical_to_drop = [col for col in original_cyclical_cols_to_drop if col in df.columns]\n",
    "if existing_original_cyclical_to_drop:\n",
    "    df.drop(columns=existing_original_cyclical_to_drop, inplace=True)\n",
    "    print(f\"Dropped original cyclical source columns: {existing_original_cyclical_to_drop}\")\n",
    "\n",
    "if encoded_features:\n",
    "    print(f\"Cyclical encoding (sin/cos) applied to: {', '.join(encoded_features)}.\")\n",
    "else:\n",
    "    print(\"No cyclical encoding was applied as relevant columns were not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG-CQmW8eU6w"
   },
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "One-Hot encoding is used for nominal categorical features where no ordinal relationship exists. It creates a new binary column for each category, indicating its presence (1) or absence (0). This method is suitable for columns like `BORO_NM`, `KY_CD`, `LOC_OF_OCCUR_DESC`, `OFNS_DESC`, `PD_CD`, `PREM_TYP_DESC`, `SUSP_RACE`, `SUSP_SEX`, `VIC_RACE`, `VIC_SEX`, `SEASON`, and `TIME_BUCKET`. While it can increase dimensionality, especially with high-cardinality features, it ensures the model does not infer an incorrect order between categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4354,
     "status": "ok",
     "timestamp": 1748179065338,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "fH1josweeU61",
    "outputId": "7586cba3-b198-41a4-800f-16ee13f5388f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Engineering: One Hot Encoding ===\n",
      "Columns converted with One-Hot Encoding: ['BORO_NM', 'KY_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_RACE', 'SUSP_SEX', 'VIC_RACE', 'VIC_SEX', 'SEASON', 'TIME_BUCKET']\n",
      "Final shape after encoding: (2496742, 699)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Engineering: One Hot Encoding ===\")\n",
    "\n",
    "# Features to one-hot encode (nominal categorical, not ordinal/cyclical)\n",
    "# These are columns that do not have an inherent order and are not cyclical.\n",
    "cols_to_encode = [\n",
    "    'BORO_NM', 'KY_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PD_CD',\n",
    "    'PREM_TYP_DESC', 'SUSP_RACE', 'SUSP_SEX', 'VIC_RACE', 'VIC_SEX',\n",
    "    'SEASON', 'TIME_BUCKET'\n",
    "]\n",
    "\n",
    "# Filter out columns that might not exist in the DataFrame to avoid errors\n",
    "existing_cols_to_encode = [col for col in cols_to_encode if col in df.columns]\n",
    "if existing_cols_to_encode:\n",
    "    # Apply one-hot encoding using pandas get_dummies\n",
    "    # drop_first=False is used to avoid issues with interpretability or specific model requirements.\n",
    "    df = pd.get_dummies(df, columns=existing_cols_to_encode, prefix=existing_cols_to_encode, dtype=np.uint8)\n",
    "    print(\"Columns converted with One-Hot Encoding:\", existing_cols_to_encode)\n",
    "    print(f\"Final shape after encoding: {df.shape}\")\n",
    "else:\n",
    "    print(\"No specified columns for One-Hot Encoding found in the DataFrame.\")\n",
    "\n",
    "# Display a sample of the DataFrame to check new columns (optional)\n",
    "# print(\"\\nSample of DataFrame after One-Hot Encoding:\")\n",
    "# print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2i6CQIChxlu"
   },
   "source": [
    "## Data Scaling and Dimensionality Reduction (PCA)\n",
    "\n",
    "In this section, we apply different scaling techniques—**StandardScaler**, **MinMaxScaler**, and **RobustScaler**—to all numeric features of the dataset. The goal is to compare these normalization methods and identify which one is most suitable for our data, especially in preparation for dimensionality reduction.\n",
    "\n",
    "After scaling, we perform **Principal Component Analysis (PCA)** on each version of the encoded dataset (i.e., after all categorical variables have been encoded). PCA is used to reduce the number of features while retaining 95% of the total variance, making the dataset more manageable for subsequent modeling.\n",
    "\n",
    "By visualizing the cumulative explained variance and the distributions of selected features, we can objectively choose the best scaling method to use for the final, definitive PCA and downstream machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106874,
     "status": "ok",
     "timestamp": 1748179172225,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "vYh7hboohxl0",
    "outputId": "d41a78d1-93d9-4b76-bdce-e5e44a6d2448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Scaling and Dimensionality Reduction (PCA) ===\n",
      "Target column 'LAW_CAT_CD' separated. X_features shape: (2496742, 698), y_target shape: (2496742,)\n",
      "Numeric features for PCA (698): ['SUSP_AGE_GROUP', 'VIC_AGE_GROUP', 'Latitude', 'Longitude', 'BAR_DISTANCE']...\n",
      "StandardScaler (sample): 384 components for 95% variance\n",
      "MinMaxScaler (sample):   90 components for 95% variance\n",
      "RobustScaler (sample):   66 components for 95% variance\n",
      "\n",
      "Using RobustScaler for full dataset scaling on numeric features of X_features.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 13.0 GiB for an array with shape (698, 2496742) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_scaler_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for full dataset scaling on numeric features of X_features.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Apply scaling to the full set of numeric features from X_features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m X_scaled_full_numeric_features = \u001b[43mchosen_scaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_numeric_features_to_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m pca_final = PCA(n_components=\u001b[32m0.95\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Apply PCA to the scaled numeric features\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\base.py:918\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m         warnings.warn(\n\u001b[32m    904\u001b[39m             (\n\u001b[32m    905\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    914\u001b[39m         )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1686\u001b[39m, in \u001b[36mRobustScaler.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1673\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Center and scale the data.\u001b[39;00m\n\u001b[32m   1674\u001b[39m \n\u001b[32m   1675\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1683\u001b[39m \u001b[33;03m    Transformed array.\u001b[39;00m\n\u001b[32m   1684\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1685\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1686\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1698\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_scaling:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1117\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmay_share_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray_orig\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1118\u001b[39m             array = _asarray_with_order(\n\u001b[32m   1119\u001b[39m                 array, dtype=dtype, order=order, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, xp=xp\n\u001b[32m   1120\u001b[39m             )\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1122\u001b[39m         \u001b[38;5;66;03m# always make a copy for non-numpy arrays\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\pandas\\core\\generic.py:2152\u001b[39m, in \u001b[36mNDFrame.__array__\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__array__\u001b[39m(\n\u001b[32m   2150\u001b[39m     \u001b[38;5;28mself\u001b[39m, dtype: npt.DTypeLike | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, copy: bool_t | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2151\u001b[39m ) -> np.ndarray:\n\u001b[32m-> \u001b[39m\u001b[32m2152\u001b[39m     values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\n\u001b[32m   2153\u001b[39m     arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2155\u001b[39m         astype_is_view(values.dtype, arr.dtype)\n\u001b[32m   2156\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[32m   2157\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mgr.is_single_block\n\u001b[32m   2158\u001b[39m     ):\n\u001b[32m   2159\u001b[39m         \u001b[38;5;66;03m# Check if both conversions can be done without a copy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\pandas\\core\\frame.py:1127\u001b[39m, in \u001b[36mDataFrame._values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1125\u001b[39m blocks = mgr.blocks\n\u001b[32m   1126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) != \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m)\n\u001b[32m   1129\u001b[39m arr = blocks[\u001b[32m0\u001b[39m].values\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arr.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m   1131\u001b[39m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\pandas\\core\\frame.py:12664\u001b[39m, in \u001b[36mDataFrame.values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m  12590\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m  12591\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> np.ndarray:\n\u001b[32m  12592\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m  12593\u001b[39m \u001b[33;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[32m  12594\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m  12662\u001b[39m \u001b[33;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[32m  12663\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m12664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1694\u001b[39m, in \u001b[36mBlockManager.as_array\u001b[39m\u001b[34m(self, dtype, copy, na_value)\u001b[39m\n\u001b[32m   1692\u001b[39m         arr.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m     arr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[32m   1696\u001b[39m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferdi\\anaconda3\\envs\\DMML\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1727\u001b[39m, in \u001b[36mBlockManager._interleave\u001b[39m\u001b[34m(self, dtype, na_value)\u001b[39m\n\u001b[32m   1724\u001b[39m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[32m   1725\u001b[39m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[32m   1726\u001b[39m dtype = ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1727\u001b[39m result = np.empty(\u001b[38;5;28mself\u001b[39m.shape, dtype=dtype)\n\u001b[32m   1729\u001b[39m itemmask = np.zeros(\u001b[38;5;28mself\u001b[39m.shape[\u001b[32m0\u001b[39m])\n\u001b[32m   1731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype == np.dtype(\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1732\u001b[39m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 13.0 GiB for an array with shape (698, 2496742) and data type float64"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "#import matplotlib.pyplot as plt # Ensure plt is imported if plots are enabled\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'LAW_CAT_CD'\n",
    "# The global 'df' DataFrame is assumed to be the one after all previous feature engineering and encoding steps.\n",
    "\n",
    "print(\"\\n=== Data Scaling and Dimensionality Reduction (PCA) ===\")\n",
    "\n",
    "# 1. Separate Target Variable and Features\n",
    "if target_column in df.columns:\n",
    "    y_target = df[target_column].copy()\n",
    "    X_features = df.drop(columns=[target_column])\n",
    "    print(f\"Target column '{target_column}' separated. X_features shape: {X_features.shape}, y_target shape: {y_target.shape}\")\n",
    "else:\n",
    "    print(f\"Warning: Target column '{target_column}' not found in DataFrame. PCA will proceed on all numeric columns of the current 'df' (which will be treated as X_features).\")\n",
    "    y_target = None\n",
    "    X_features = df.copy() # Treat the whole df as features for PCA processing\n",
    "\n",
    "# 2. Identify Feature Types in X_features\n",
    "numeric_cols_in_X = X_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Preserve non-numeric columns from X_features, ensuring their index is maintained for later concatenation\n",
    "non_numeric_cols_from_X = X_features.select_dtypes(exclude=[np.number])\n",
    "\n",
    "# 3. Handle No Numeric Features Case\n",
    "if not numeric_cols_in_X:\n",
    "    print(\"Error: No numeric features found in X_features for PCA. Skipping scaling and PCA.\")\n",
    "    # Reconstruct df with original X_features (which includes non-numeric) and y_target if it exists\n",
    "    # The global 'df' will be updated to reflect this state.\n",
    "    if y_target is not None:\n",
    "        # Ensure original df index is preserved if possible, or reset if combining disparate parts\n",
    "        df = pd.concat([X_features.reset_index(drop=True), y_target.to_frame(name=target_column).reset_index(drop=True)], axis=1)\n",
    "    else:\n",
    "        df = X_features # df is already X_features if y_target was None\n",
    "    print(f\"Global 'df' (no PCA performed) shape: {df.shape}\")\n",
    "else:\n",
    "    print(f\"Numeric features for PCA ({len(numeric_cols_in_X)}): {numeric_cols_in_X[:5]}...\")\n",
    "    if not non_numeric_cols_from_X.empty:\n",
    "        print(f\"Non-numeric features to carry over ({len(non_numeric_cols_from_X.columns)}): {non_numeric_cols_from_X.columns.tolist()[:5]}...\")\n",
    "\n",
    "    # 4. Scaling and PCA (on numeric features of X_features)\n",
    "    X_numeric_features_to_scale = X_features[numeric_cols_in_X]\n",
    "\n",
    "    # Sample for scaler selection\n",
    "    sample_size = min(50000, len(X_numeric_features_to_scale))\n",
    "    X_numeric_sample = X_numeric_features_to_scale.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    scaler_std = StandardScaler()\n",
    "    X_sample_std_scaled = scaler_std.fit_transform(X_numeric_sample)\n",
    "    pca_std_sample = PCA(n_components=0.95, random_state=42).fit(X_sample_std_scaled)\n",
    "    n_std = pca_std_sample.n_components_\n",
    "\n",
    "    scaler_mm = MinMaxScaler()\n",
    "    X_sample_mm_scaled = scaler_mm.fit_transform(X_numeric_sample)\n",
    "    pca_mm_sample = PCA(n_components=0.95, random_state=42).fit(X_sample_mm_scaled)\n",
    "    n_mm = pca_mm_sample.n_components_\n",
    "\n",
    "    scaler_rb = RobustScaler()\n",
    "    X_sample_rb_scaled = scaler_rb.fit_transform(X_numeric_sample)\n",
    "    pca_rb_sample = PCA(n_components=0.95, random_state=42).fit(X_sample_rb_scaled)\n",
    "    n_rb = pca_rb_sample.n_components_\n",
    "\n",
    "    # Plotting code is commented out as per original\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.figure(figsize=(10,6))\n",
    "    # plt.plot(np.cumsum(pca_std_sample.explained_variance_ratio_), label='StandardScaler (Z-score)')\n",
    "    # plt.plot(np.cumsum(pca_mm_sample.explained_variance_ratio_), label='MinMaxScaler')\n",
    "    # plt.plot(np.cumsum(pca_rb_sample.explained_variance_ratio_), label='RobustScaler')\n",
    "    # plt.xlabel('Number of components')\n",
    "    # plt.ylabel('Cumulative explained variance')\n",
    "    # plt.title('PCA: Cumulative Explained Variance by Normalization Method (Sample)')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    print(f\"StandardScaler (sample): {n_std} components for 95% variance\")\n",
    "    print(f\"MinMaxScaler (sample):   {n_mm} components for 95% variance\")\n",
    "    print(f\"RobustScaler (sample):   {n_rb} components for 95% variance\")\n",
    "\n",
    "    del X_numeric_sample, X_sample_std_scaled, X_sample_mm_scaled, X_sample_rb_scaled\n",
    "    del pca_std_sample, pca_mm_sample, pca_rb_sample\n",
    "    gc.collect()\n",
    "\n",
    "    chosen_scaler = None\n",
    "    best_scaler_name = \"\"\n",
    "    if n_std <= n_mm and n_std <= n_rb:\n",
    "        best_scaler_name = \"StandardScaler\"\n",
    "        chosen_scaler = StandardScaler()\n",
    "    elif n_mm <= n_std and n_mm <= n_rb:\n",
    "        best_scaler_name = \"MinMaxScaler\"\n",
    "        chosen_scaler = MinMaxScaler()\n",
    "    else:\n",
    "        best_scaler_name = \"RobustScaler\"\n",
    "        chosen_scaler = RobustScaler()\n",
    "\n",
    "    print(f\"\\nUsing {best_scaler_name} for full dataset scaling on numeric features of X_features.\")\n",
    "    # Apply scaling to the full set of numeric features from X_features\n",
    "    X_scaled_full_numeric_features = chosen_scaler.fit_transform(X_numeric_features_to_scale)\n",
    "\n",
    "    pca_final = PCA(n_components=0.95, random_state=42)\n",
    "    # Apply PCA to the scaled numeric features\n",
    "    X_pca_transformed_numeric_features = pca_final.fit_transform(X_scaled_full_numeric_features)\n",
    "    num_pca_components = X_pca_transformed_numeric_features.shape[1]\n",
    "    print(f\"Full PCA with {best_scaler_name}: {num_pca_components} components for 95% variance (on numeric part of X_features)\")\n",
    "\n",
    "    # Create DataFrame for PCA components, maintaining original index from X_features\n",
    "    df_pca_components = pd.DataFrame(X_pca_transformed_numeric_features,\n",
    "                                     columns=[f'PC{i+1}' for i in range(num_pca_components)],\n",
    "                                     index=X_features.index) # Use index from X_features\n",
    "\n",
    "    # 5. Reconstruct the global 'df'\n",
    "    # Start with PCA components (already indexed like X_features)\n",
    "    final_df_parts = [df_pca_components]\n",
    "\n",
    "    # Add non-numeric columns (which were already indexed correctly from X_features)\n",
    "    if not non_numeric_cols_from_X.empty:\n",
    "        final_df_parts.append(non_numeric_cols_from_X) # non_numeric_cols_from_X also has index from X_features\n",
    "\n",
    "    # Concatenate PCA components and non-numeric features\n",
    "    df_transformed_features = pd.concat(final_df_parts, axis=1)\n",
    "\n",
    "    # Add back the target variable if it was separated, ensuring index alignment\n",
    "    if y_target is not None:\n",
    "        # y_target has the same index as original df, and thus X_features\n",
    "        df = pd.concat([df_transformed_features, y_target.to_frame(name=target_column)], axis=1)\n",
    "    else:\n",
    "        df = df_transformed_features # If no target, df is just the transformed features\n",
    "\n",
    "    print(f\"Global 'df' updated. Final shape: {df.shape}\")\n",
    "    print(f\"Final columns (first 20): {df.columns.tolist()[:20]}...\")\n",
    "\n",
    "# The global 'df' is now the DataFrame with feature engineering, encoding, scaling (on numeric features),\n",
    "# and PCA (on scaled numeric features), and includes the target variable if present.\n",
    "# It is ready to be saved by a subsequent cell.\n",
    "# The original df_pca_full logic is now integrated into updating the global 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283152,
     "status": "ok",
     "timestamp": 1748179455383,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "N1TcSlu2LXfv",
    "outputId": "84898415-8b42-4868-975a-bf69a10c548c"
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Saving Final Processed Dataset ===\")\n",
    "# The global 'df' now contains the data after feature engineering, encoding, scaling, and PCA (if applicable),\n",
    "# with the target variable re-attached if it was separated.\n",
    "\n",
    "# Define the path for the final dataset using feature_engineering_dir (defined in an earlier cell)\n",
    "# Ensure 'os' module is imported if not already (it was imported in cell 3 in the provided notebook content)\n",
    "# import os\n",
    "final_processed_file_name = \"feature_engineered_scaled_pca_crime_data.csv\"\n",
    "# Ensure feature_engineering_dir is defined. It was defined in cell 60e02e84 as:\n",
    "# feature_engineering_dir = os.path.join(base_dir, \"FeatureEngineered\")\n",
    "# base_dir = \"/drive/MyDrive/Data Mining and Machine Learning/Progetto\"\n",
    "# So, final_processed_file_path will be relative to that Google Drive path if run in Colab,\n",
    "# or a local path if base_dir is defined differently in the execution environment.\n",
    "\n",
    "if 'feature_engineering_dir' not in globals() and 'os' not in globals():\n",
    "    import os\n",
    "    # Attempt to define feature_engineering_dir based on common notebook structure if not found\n",
    "    # This is a fallback and might need adjustment based on actual execution environment\n",
    "    print(\"Warning: 'feature_engineering_dir' or 'os' not found in globals. Attempting to define os and a default path.\")\n",
    "    if 'base_dir' in globals():\n",
    "         feature_engineering_dir = os.path.join(globals()['base_dir'], \"FeatureEngineered\")\n",
    "         os.makedirs(feature_engineering_dir, exist_ok=True)\n",
    "    else:\n",
    "        # Fallback to a local directory if base_dir is also not found\n",
    "        feature_engineering_dir = \"FeatureEngineered\"\n",
    "        os.makedirs(feature_engineering_dir, exist_ok=True)\n",
    "        print(f\"Warning: 'base_dir' not found. Using local directory: {feature_engineering_dir}\")\n",
    "elif 'os' not in globals():\n",
    "    import os\n",
    "    print(\"Warning: 'os' module not found in globals. Imported os.\")\n",
    "\n",
    "final_processed_file_path = os.path.join(feature_engineering_dir, final_processed_file_name)\n",
    "\n",
    "try:\n",
    "    df.to_csv(final_processed_file_path, index=False)\n",
    "    print(f\"Final processed dataset saved successfully to: {final_processed_file_path}\")\n",
    "    print(f\"Final dataset shape: {df.shape}\")\n",
    "except NameError as ne:\n",
    "    if 'df' in str(ne):\n",
    "        print(\"Error: 'df' is not defined. Cannot save the DataFrame.\")\n",
    "    elif 'feature_engineering_dir' in str(ne):\n",
    "        print(\"Error: 'feature_engineering_dir' is not defined. Please ensure it's defined in an earlier cell or manually define it.\")\n",
    "        print(f\"Attempted to use path: {final_processed_file_path}\")\n",
    "    else:\n",
    "        print(f\"A NameError occurred: {ne}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final processed dataset: {e}\")\n",
    "\n",
    "# Display a sample of the final DataFrame for verification\n",
    "print(\"\\nSample of the final saved DataFrame (first 5 rows):\")\n",
    "try:\n",
    "    print(df.head())\n",
    "except NameError:\n",
    "    print(\"Error: 'df' is not defined. Cannot display head.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRYa528XeU62"
   },
   "source": [
    "# ML Pipeline Summary and Data Quality Report\n",
    "\n",
    "Comprehensive summary of the feature engineering process and ML pipeline implementation including data validation and reproducibility measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18867,
     "status": "ok",
     "timestamp": 1748179474255,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "K-9b4GtPeU62",
    "outputId": "e0179f89-502e-428d-f777-b305bd27d803"
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Data Scan (Unique Values) ===\")\n",
    "print(f\"Final Shape: {df.shape}\")\n",
    "print(f\"Final Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Calculate cardinality for each column\n",
    "cardinality_info = []\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    num_unique = len(unique_values)\n",
    "    cardinality_info.append((column, num_unique, unique_values))\n",
    "\n",
    "    # Print individual column details (as before)\n",
    "    print(f\"\\n###### Column: {column} ######\")\n",
    "    print(f\"Number of unique values: {num_unique}\")\n",
    "    limit = 20\n",
    "    if num_unique > limit:\n",
    "        print(f\"Sample unique values (first {limit}): {unique_values[:limit]}\")\n",
    "    else:\n",
    "        print(f\"Unique values: {unique_values}\")\n",
    "\n",
    "# Sort by cardinality (number of unique values) - descending\n",
    "cardinality_info.sort(key=lambda x: x[1], reverse=True) # Corrected lambda\n",
    "\n",
    "print(\"\\n=== Top 10 Columns by Cardinality ===\")\n",
    "for col, count, _ in cardinality_info[:10]: # Adjusted to unpack correctly\n",
    "    print(f\"  {col}: {count} unique values\")\n",
    "\n",
    "print(\"\\n=== ML PIPELINE SUMMARY AND DATA QUALITY REPORT ===\")\n",
    "\n",
    "# Dataset Statistics\n",
    "print(f\"\\n--- Dataset Statistics ---\")\n",
    "print(f\"Final Dataset Shape: {df.shape}\")\n",
    "print(f\"Total Features: {df.shape[1]}\")\n",
    "\n",
    "# Data Quality Metrics\n",
    "print(f\"\\n--- Data Quality Metrics ---\")\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "missing_percentage = (total_missing / total_cells) * 100\n",
    "\n",
    "print(f\"Total Missing Values: {total_missing}\")\n",
    "print(f\"Missing Data Percentage: {missing_percentage:.2f}%\")\n",
    "print(f\"Complete Rows: {df.dropna().shape[0]} ({df.dropna().shape[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Feature Type Distribution\n",
    "print(f\"\\n--- Feature Type Distribution ---\")\n",
    "feature_types = df.dtypes.value_counts()\n",
    "for dtype, count in feature_types.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "# Memory Usage\n",
    "print(f\"\\n--- Memory Usage ---\")\n",
    "memory_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Dataset Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "# Categorical Feature Cardinality\n",
    "print(f\"\\n--- Categorical Feature Cardinality (Top 10) ---\")\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "cardinality_info = []\n",
    "for col in cat_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    cardinality_info.append((col, unique_count))\n",
    "\n",
    "cardinality_info.sort(key=lambda x: x[1], reverse=True)\n",
    "for col, count in cardinality_info[:10]:\n",
    "    print(f\"  {col}: {count} unique values\")\n",
    "\n",
    "# Temporal Feature Summary\n",
    "print(f\"\\n--- Temporal Features Created ---\")\n",
    "temporal_features = ['HOUR', 'DAY', 'WEEKDAY', 'IS_WEEKEND', 'MONTH', 'YEAR', 'SEASON', 'TIME_BUCKET', 'IS_HOLIDAY', 'IS_PAYDAY']\n",
    "existing_temporal = [col for col in temporal_features if col in df.columns]\n",
    "print(f\"Temporal features: {existing_temporal}\")\n",
    "\n",
    "# POI Feature Summary\n",
    "print(f\"\\n--- POI Features Summary ---\")\n",
    "poi_features = [col for col in df.columns if any(keyword in col for keyword in ['POI', 'DISTANCE', 'COUNT', 'WITHIN'])]\n",
    "print(f\"POI-related features: {len(poi_features)}\")\n",
    "if poi_features:\n",
    "    print(f\"Sample POI features: {poi_features[:5]}{'...' if len(poi_features) > 5 else ''}\")\n",
    "\n",
    "# Data Validation Warnings\n",
    "print(f\"\\n--- Data Validation Warnings ---\")\n",
    "warnings_list = []\n",
    "\n",
    "# Check for high cardinality categorical features\n",
    "high_cardinality_threshold = 50\n",
    "high_cardinality_cols = [col for col, count in cardinality_info if count > high_cardinality_threshold]\n",
    "if high_cardinality_cols:\n",
    "    warnings_list.append(f\"High cardinality categorical features (>{high_cardinality_threshold}): {high_cardinality_cols}\")\n",
    "\n",
    "# Check for features with many missing values\n",
    "high_missing_threshold = 0.5\n",
    "high_missing_cols = []\n",
    "for col in df.columns:\n",
    "    missing_pct = df[col].isnull().sum() / len(df)\n",
    "    if missing_pct > high_missing_threshold:\n",
    "        high_missing_cols.append((col, missing_pct))\n",
    "\n",
    "if high_missing_cols:\n",
    "    warnings_list.append(f\"Features with >50% missing data: {[(col, f'{pct:.1%}') for col, pct in high_missing_cols]}\")\n",
    "\n",
    "# Check for potential data leakage (future dates)\n",
    "if 'YEAR' in df.columns:\n",
    "    current_year = datetime.now().year\n",
    "    future_dates = (df['YEAR'] > current_year).sum()\n",
    "    if future_dates > 0:\n",
    "        warnings_list.append(f\"Found {future_dates} records with future dates\")\n",
    "\n",
    "if warnings_list:\n",
    "    for warning in warnings_list:\n",
    "        print(f\"  ⚠️ {warning}\")\n",
    "else:\n",
    "    print(\"  ✅ No data quality warnings detected\")\n",
    "\n",
    "# Pipeline Files Summary\n",
    "print(f\"\\n--- Generated Files ---\")\n",
    "files_created = {\n",
    "    'Feature Engineered Dataset': os.path.join(feature_engineering_dir, \"feature_engineered_crime_data.csv\"),\n",
    "    'Final Processed Dataset': os.path.join(feature_engineering_dir, \"feature_engineered_scaled_pca_crime_data.csv\")\n",
    "}\n",
    "\n",
    "for file_type, file_path in files_created.items():\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path) / 1024**2\n",
    "        print(f\"  ✅ {file_type}: {file_path} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file_type}: {file_path} (Not found)\")\n",
    "\n",
    "print(f\"\\n✅ FEATURE ENGINEERING COMPLETED SUCCESSFULLY\")\n",
    "print(f\"\\n--- Next Steps for ML ---\")\n",
    "print(\"1. Load the processed dataset for modeling\")\n",
    "print(\"2. Split data into training/testing sets\")\n",
    "print(\"3. Train and evaluate machine learning models\")\n",
    "print(\"4. Address any data quality warnings if needed\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
