{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrW_BDbw1KHW"
   },
   "source": [
    "# Setup\n",
    "\n",
    "This section covers the initial setup, including library imports, path definitions, and mounting Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFd4B2od1KHd"
   },
   "source": [
    "## Mount Google Drive\n",
    "Mount Google Drive to access files stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19079,
     "status": "ok",
     "timestamp": 1748177356073,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "t72SsVp_1KHf",
    "outputId": "c85c7941-600f-46c7-b802-32cb24787602"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQokErAD1KHj"
   },
   "source": [
    "## Import Libraries\n",
    "Import necessary libraries for data manipulation, file operations, and warnings management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ohipASX41KHk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM6dbSn11KHl"
   },
   "source": [
    "## Define Paths\n",
    "Define base directory and paths for input (cleaned data) and output (integrated data) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DLinPnih1KHm"
   },
   "outputs": [],
   "source": [
    "# Path variables\n",
    "# base_dir = \"/drive/MyDrive/Data Mining and Machine Learning/Progetto\"\n",
    "base_dir = r\"C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\"\n",
    "processed_dir = os.path.join(base_dir, \"Processed\")\n",
    "cleaned_data_file = os.path.join(processed_dir, \"cleaned_crime_data_processed.csv\")\n",
    "os.makedirs(processed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUPkiHdA1KHo"
   },
   "source": [
    "# Load Cleaned Data\n",
    "\n",
    "Load the cleaned crime dataset and the cleaned POI dataset produced by the Data Cleaning phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pELP_als1KHs"
   },
   "source": [
    "## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15178,
     "status": "ok",
     "timestamp": 1748177372346,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "IrOEu7TN1KHs",
    "outputId": "97b4db24-2d5b-4cf6-bf0c-872ef0571490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Processed Data ===\n",
      "Dataset loaded successfully: 2496759 rows and 18 columns\n",
      "Columns in the dataset: ['BORO_NM', 'CMPLNT_FR_DT', 'CMPLNT_FR_TM', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'Latitude', 'Longitude', 'OFNS_DESC', 'PARKS_NM', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX']\n",
      "\n",
      "=== Dataset Overview ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2496759 entries, 0 to 2496758\n",
      "Data columns (total 18 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   BORO_NM            object \n",
      " 1   CMPLNT_FR_DT       object \n",
      " 2   CMPLNT_FR_TM       object \n",
      " 3   KY_CD              int64  \n",
      " 4   LAW_CAT_CD         object \n",
      " 5   LOC_OF_OCCUR_DESC  object \n",
      " 6   Latitude           float64\n",
      " 7   Longitude          float64\n",
      " 8   OFNS_DESC          object \n",
      " 9   PARKS_NM           object \n",
      " 10  PD_CD              float64\n",
      " 11  PREM_TYP_DESC      object \n",
      " 12  SUSP_AGE_GROUP     object \n",
      " 13  SUSP_RACE          object \n",
      " 14  SUSP_SEX           object \n",
      " 15  VIC_AGE_GROUP      object \n",
      " 16  VIC_RACE           object \n",
      " 17  VIC_SEX            object \n",
      "dtypes: float64(3), int64(1), object(14)\n",
      "memory usage: 342.9+ MB\n",
      "None\n",
      "\n",
      "=== Summary Statistics ===\n",
      "              KY_CD      Latitude     Longitude         PD_CD\n",
      "count  2.496759e+06  2.496759e+06  2.496759e+06  2.496759e+06\n",
      "mean   3.025155e+02  4.073661e+01 -7.392281e+01  4.084761e+02\n",
      "std    1.595039e+02  1.344466e-01  2.077602e-01  2.211384e+02\n",
      "min    1.020000e+02  0.000000e+00 -7.425474e+01  1.000000e+02\n",
      "25%    1.170000e+02  4.067561e+01 -7.397274e+01  2.540000e+02\n",
      "50%    3.410000e+02  4.073495e+01 -7.392571e+01  3.520000e+02\n",
      "75%    3.510000e+02  4.081194e+01 -7.387948e+01  6.380000e+02\n",
      "max    8.810000e+02  4.091271e+01  0.000000e+00  9.690000e+02\n",
      "\n",
      "=== Sample Data ===\n",
      "               BORO_NM CMPLNT_FR_DT CMPLNT_FR_TM  KY_CD   LAW_CAT_CD  \\\n",
      "376512          QUEENS   2020-10-11     00:00:00    578    VIOLATION   \n",
      "1386887  STATEN ISLAND   2023-07-17     13:50:00    109       FELONY   \n",
      "1563777       BROOKLYN   2023-12-19     12:10:00    341  MISDEMEANOR   \n",
      "1072318       BROOKLYN   2022-05-05     06:21:00    341  MISDEMEANOR   \n",
      "2045386      MANHATTAN   2024-06-08     09:20:00    578    VIOLATION   \n",
      "\n",
      "        LOC_OF_OCCUR_DESC   Latitude  Longitude      OFNS_DESC PARKS_NM  \\\n",
      "376512              FRONT  40.731343 -73.860054  HARRASSMENT 2   (NULL)   \n",
      "1386887            INSIDE  40.598516 -74.078966  GRAND LARCENY   (NULL)   \n",
      "1563777            INSIDE  40.630373 -73.947438  PETIT LARCENY   (NULL)   \n",
      "1072318           UNKNOWN  40.669992 -73.859112  PETIT LARCENY   (NULL)   \n",
      "2045386             FRONT  40.805396 -73.939840  HARRASSMENT 2   (NULL)   \n",
      "\n",
      "         PD_CD           PREM_TYP_DESC SUSP_AGE_GROUP       SUSP_RACE  \\\n",
      "376512   638.0  RESIDENCE - APT. HOUSE        UNKNOWN         UNKNOWN   \n",
      "1386887  403.0                    BANK          18-24         UNKNOWN   \n",
      "1563777  333.0                 UNKNOWN          45-64  BLACK HISPANIC   \n",
      "1072318  321.0                  STREET          25-44           BLACK   \n",
      "2045386  637.0                  STREET        UNKNOWN         UNKNOWN   \n",
      "\n",
      "        SUSP_SEX VIC_AGE_GROUP VIC_RACE VIC_SEX  \n",
      "376512         U         45-64    WHITE       F  \n",
      "1386887        M       UNKNOWN  UNKNOWN       D  \n",
      "1563777        F       UNKNOWN  UNKNOWN       D  \n",
      "1072318        M         45-64    BLACK       M  \n",
      "2045386        M         25-44    BLACK       M  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"=== Loading Processed Data ===\")\n",
    "try:\n",
    "    if os.path.exists(cleaned_data_file):\n",
    "        df = pd.read_csv(cleaned_data_file)\n",
    "        initial_rows = len(df)\n",
    "        print(f\"Dataset loaded successfully: {initial_rows} rows and {df.shape[1]} columns\")\n",
    "        print(f\"Columns in the dataset: {df.columns.tolist()}\")\n",
    "\n",
    "        # Basic data validation\n",
    "        if initial_rows == 0:\n",
    "            raise ValueError(\"Dataset is empty\")\n",
    "        if df.shape[1] < 5:\n",
    "            raise ValueError(\"Dataset has too few columns\")\n",
    "\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find cleaned dataset at: {cleaned_data_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise RuntimeError(f\"Failed to load required dataset: {e}\")\n",
    "\n",
    "# Display basic dataset overview\n",
    "print(\"\\n=== Dataset Overview ===\")\n",
    "print(df.info())\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(df.describe())\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "print(df.sample(min(5, len(df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eprbU8gD1KHu"
   },
   "source": [
    "# Data Integration – Enrichment with OpenStreetMap POI\n",
    "\n",
    "A **data integration task** was carried out to enrich the original dataset of crime records with **contextual geographic information** retrieved from **OpenStreetMap (OSM)**. The goal was to enhance the dataset with spatial features related to urban structure, mobility, and social activity, which are potentially correlated with criminal activity.\n",
    "\n",
    "The integration was performed in QGIS through the following steps:\n",
    "\n",
    "####  1. Construction of a spatial grid\n",
    "A **100m x 100m regular grid** was generated over the entire study area using a projected CRS (EPSG:32618 – UTM zone 18N), to allow metric spatial operations. Each cell acts as a local unit of spatial context for aggregation.\n",
    "\n",
    "####  2. Extraction of Points of Interest (POIs) from OSM\n",
    "Using the **QuickOSM plugin**, several types of POIs relevant to criminological analysis were downloaded and filtered:\n",
    "- Bars (`amenity=bar`)\n",
    "- Nightclubs (`amenity=nightclub`)\n",
    "- ATMs (`amenity=atm`)\n",
    "- Bus stops (`highway=bus_stop`)\n",
    "- Metro and train stations (`railway=station`, `public_transport=station`)\n",
    "- Schools (`amenity=school`)\n",
    "\n",
    "#### 3. Spatial integration and feature generation\n",
    "For each crime record, two types of features were computed:\n",
    "\n",
    "- **Distance features**:  \n",
    "  Euclidean distance (in meters) from the crime location to the **nearest POI** of a given category, using Nearest Neighbor Join (`NNJoin` plugin).  \n",
    "  These features are:\n",
    "  - `bar_distance`\n",
    "  - `nightclubs_distance`\n",
    "  - `atm_distance`\n",
    "  - `metro_distance`\n",
    "\n",
    "- **Density/count features**:  \n",
    "  Number of POIs of a given type **within the 100x100m grid cell** where the crime occurred. Computed using `Count points in polygon` and spatial joins (`Join attributes by location`).  \n",
    "  These features are:\n",
    "  - `bars_count`\n",
    "  - `nightclubs_count`\n",
    "  - `atms_count`\n",
    "  - `bus_stops_count`\n",
    "  - `metro_and_trains_count`\n",
    "  - `schools_count`\n",
    "\n",
    "#### Purpose of this integration\n",
    "These contextual features are expected to improve downstream analysis tasks such as:\n",
    "- **Clustering of crime patterns**\n",
    "- **Crime type classification**\n",
    "- **Hotspot detection**\n",
    "- **Interpretability of spatial models**\n",
    "\n",
    "By embedding urban environmental data directly into the crime dataset, this enrichment step helps move from raw geospatial points to **semantically informed locations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27352,
     "status": "ok",
     "timestamp": 1748177399717,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "yuCWdo6L2tKp",
    "outputId": "50df460f-698e-4115-ef3a-f1f54a93fd3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Integrated Data ===\n",
      "Integrated dataset loaded: 2496759 rows and 32 columns\n",
      "Columns: ['fid', 'BORO_NM', 'CMPLNT_FR_', 'CMPLNT_F_1', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCC', 'OFNS_DESC', 'PARKS_NM', 'PD_CD', 'PREM_TYP_D', 'SUSP_AGE_G', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GR', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'bar_distance', 'nightclubs_distance', 'atm_distance', 'atms_count', 'bars_count', 'bus_stops_count', 'metro_and_trains_count', 'nightclubs_count', 'schools_count', 'join_fid', 'join_full_id', 'join_osm_id', 'metro_distance']\n",
      "\n",
      "=== Spatial Data Quality Validation ===\n",
      "\n",
      "--- Spatial Validation Results ---\n",
      "WARNINGS:\n",
      "  17 records with latitude outside NYC bounds\n",
      "  17 records with longitude outside NYC bounds\n",
      "\n",
      "--- POI Validation Results ---\n",
      "All checks passed successfully.\n",
      "\n",
      "=== Integrated Dataset Overview ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2496759 entries, 0 to 2496758\n",
      "Data columns (total 32 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   fid                     int64  \n",
      " 1   BORO_NM                 object \n",
      " 2   CMPLNT_FR_              object \n",
      " 3   CMPLNT_F_1              object \n",
      " 4   KY_CD                   int64  \n",
      " 5   LAW_CAT_CD              object \n",
      " 6   LOC_OF_OCC              object \n",
      " 7   OFNS_DESC               object \n",
      " 8   PARKS_NM                object \n",
      " 9   PD_CD                   int64  \n",
      " 10  PREM_TYP_D              object \n",
      " 11  SUSP_AGE_G              object \n",
      " 12  SUSP_RACE               object \n",
      " 13  SUSP_SEX                object \n",
      " 14  VIC_AGE_GR              object \n",
      " 15  VIC_RACE                object \n",
      " 16  VIC_SEX                 object \n",
      " 17  Latitude                float64\n",
      " 18  Longitude               float64\n",
      " 19  bar_distance            float64\n",
      " 20  nightclubs_distance     float64\n",
      " 21  atm_distance            float64\n",
      " 22  atms_count              float64\n",
      " 23  bars_count              float64\n",
      " 24  bus_stops_count         float64\n",
      " 25  metro_and_trains_count  float64\n",
      " 26  nightclubs_count        float64\n",
      " 27  schools_count           float64\n",
      " 28  join_fid                int64  \n",
      " 29  join_full_id            object \n",
      " 30  join_osm_id             int64  \n",
      " 31  metro_distance          float64\n",
      "dtypes: float64(12), int64(5), object(15)\n",
      "memory usage: 609.6+ MB\n",
      "None\n",
      "\n",
      "=== Summary Statistics ===\n",
      "                fid         KY_CD         PD_CD      Latitude     Longitude  \\\n",
      "count  2.496759e+06  2.496759e+06  2.496759e+06  2.496759e+06  2.496759e+06   \n",
      "mean   1.248380e+06  3.025155e+02  4.084761e+02  4.073661e+01 -7.392281e+01   \n",
      "std    7.207524e+05  1.595039e+02  2.211384e+02  1.344466e-01  2.077602e-01   \n",
      "min    1.000000e+00  1.020000e+02  1.000000e+02  0.000000e+00 -7.425474e+01   \n",
      "25%    6.241905e+05  1.170000e+02  2.540000e+02  4.067561e+01 -7.397274e+01   \n",
      "50%    1.248380e+06  3.410000e+02  3.520000e+02  4.073495e+01 -7.392571e+01   \n",
      "75%    1.872570e+06  3.510000e+02  6.380000e+02  4.081194e+01 -7.387948e+01   \n",
      "max    2.496759e+06  8.810000e+02  9.690000e+02  4.091271e+01  0.000000e+00   \n",
      "\n",
      "       bar_distance  nightclubs_distance  atm_distance    atms_count  \\\n",
      "count  2.496759e+06         2.496759e+06  2.496759e+06  2.496742e+06   \n",
      "mean   9.304923e+02         2.115418e+03  1.444943e+03  1.463507e-02   \n",
      "std    3.581217e+04         3.584537e+04  3.582254e+04  1.318330e-01   \n",
      "min    4.102764e+00         5.889870e+00  9.675179e+00  0.000000e+00   \n",
      "25%    2.132541e+02         7.718525e+02  4.615127e+02  0.000000e+00   \n",
      "50%    5.848092e+02         1.524675e+03  1.065681e+03  0.000000e+00   \n",
      "75%    1.180303e+03         2.537621e+03  1.878874e+03  0.000000e+00   \n",
      "max    1.372163e+07         1.372083e+07  1.372270e+07  5.000000e+00   \n",
      "\n",
      "         bars_count  bus_stops_count  metro_and_trains_count  \\\n",
      "count  2.496742e+06     2.496742e+06            2.496742e+06   \n",
      "mean   6.523982e-02     4.109676e-01            5.793270e-02   \n",
      "std    3.305598e-01     7.815628e-01            2.529411e-01   \n",
      "min    0.000000e+00     0.000000e+00            0.000000e+00   \n",
      "25%    0.000000e+00     0.000000e+00            0.000000e+00   \n",
      "50%    0.000000e+00     0.000000e+00            0.000000e+00   \n",
      "75%    0.000000e+00     1.000000e+00            0.000000e+00   \n",
      "max    6.000000e+00     6.000000e+00            2.000000e+00   \n",
      "\n",
      "       nightclubs_count  schools_count      join_fid   join_osm_id  \\\n",
      "count      2.496742e+06   2.496742e+06  2.496759e+06  2.496759e+06   \n",
      "mean       4.794648e-03   2.806698e-02  2.806922e+02  2.741098e+09   \n",
      "std        7.512655e-02   2.069132e-01  1.529026e+02  2.272598e+09   \n",
      "min        0.000000e+00   0.000000e+00  1.000000e+00  4.273628e+07   \n",
      "25%        0.000000e+00   0.000000e+00  1.510000e+02  5.987259e+08   \n",
      "50%        0.000000e+00   0.000000e+00  2.820000e+02  2.013618e+09   \n",
      "75%        0.000000e+00   0.000000e+00  4.160000e+02  4.476486e+09   \n",
      "max        2.000000e+00   7.000000e+00  5.490000e+02  1.259443e+10   \n",
      "\n",
      "       metro_distance  \n",
      "count    2.496759e+06  \n",
      "mean     6.870371e+02  \n",
      "std      3.580790e+04  \n",
      "min      1.336284e-01  \n",
      "25%      1.926433e+02  \n",
      "50%      3.482616e+02  \n",
      "75%      6.627598e+02  \n",
      "max      1.372000e+07  \n",
      "\n",
      "=== Sample Data ===\n",
      "             fid BORO_NM  CMPLNT_FR_    CMPLNT_F_1  KY_CD LAW_CAT_CD  \\\n",
      "1591772  1591773   BRONX  2022/07/14  22:50:00.000    578  VIOLATION   \n",
      "1444409  1444410   BRONX  2024/09/30  10:08:00.000    107     FELONY   \n",
      "2318942  2318943  QUEENS  2021/08/08  03:00:00.000    121     FELONY   \n",
      "\n",
      "        LOC_OF_OCC                       OFNS_DESC PARKS_NM  PD_CD  ...  \\\n",
      "1591772     INSIDE                   HARRASSMENT 2   (NULL)    638  ...   \n",
      "1444409     INSIDE                        BURGLARY   (NULL)    221  ...   \n",
      "2318942      FRONT  CRIMINAL MISCHIEF & RELATED OF   (NULL)    267  ...   \n",
      "\n",
      "        atms_count bars_count bus_stops_count metro_and_trains_count  \\\n",
      "1591772        0.0        0.0             0.0                    0.0   \n",
      "1444409        0.0        0.0             0.0                    0.0   \n",
      "2318942        0.0        0.0             0.0                    0.0   \n",
      "\n",
      "        nightclubs_count schools_count join_fid  join_full_id  join_osm_id  \\\n",
      "1591772              0.0           0.0      388   n4248884830   4248884830   \n",
      "1444409              0.0           0.0      284   n2013618450   2013618450   \n",
      "2318942              0.0           0.0      495   n6624448793   6624448793   \n",
      "\n",
      "         metro_distance  \n",
      "1591772      454.217693  \n",
      "1444409      351.165964  \n",
      "2318942     1459.612467  \n",
      "\n",
      "[3 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced spatial data quality validation functions\n",
    "def validate_spatial_data(df):\n",
    "    \"\"\"\n",
    "    Validate spatial data quality for crime dataset\n",
    "    \"\"\"\n",
    "    validation_results = {'passed': [], 'warnings': [], 'errors': []}\n",
    "\n",
    "    # Check for coordinate columns\n",
    "    lat_cols = [col for col in df.columns if 'lat' in col.lower()]\n",
    "    lon_cols = [col for col in df.columns if 'lon' in col.lower()]\n",
    "\n",
    "    if not lat_cols or not lon_cols:\n",
    "        validation_results['errors'].append(\"Missing latitude or longitude columns\")\n",
    "        return validation_results\n",
    "\n",
    "    lat_col, lon_col = lat_cols[0], lon_cols[0]\n",
    "\n",
    "    # Validate coordinate ranges (NYC bounds approximately)\n",
    "    valid_lat_range = (40.4, 41.0)\n",
    "    valid_lon_range = (-74.3, -73.7)\n",
    "\n",
    "    invalid_lat = ((df[lat_col] < valid_lat_range[0]) | (df[lat_col] > valid_lat_range[1])).sum()\n",
    "    invalid_lon = ((df[lon_col] < valid_lon_range[0]) | (df[lon_col] > valid_lon_range[1])).sum()\n",
    "\n",
    "    if invalid_lat > 0:\n",
    "        validation_results['warnings'].append(f\"{invalid_lat} records with latitude outside NYC bounds\")\n",
    "    if invalid_lon > 0:\n",
    "        validation_results['warnings'].append(f\"{invalid_lon} records with longitude outside NYC bounds\")\n",
    "\n",
    "    # Check for missing coordinates\n",
    "    missing_coords = df[[lat_col, lon_col]].isna().any(axis=1).sum()\n",
    "    if missing_coords > 0:\n",
    "        validation_results['warnings'].append(f\"{missing_coords} records with missing coordinates\")\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "def validate_poi_features(df):\n",
    "    \"\"\"\n",
    "    Validate POI feature quality\n",
    "    \"\"\"\n",
    "    validation_results = {'passed': [], 'warnings': [], 'errors': []}\n",
    "\n",
    "    # Expected POI columns\n",
    "    expected_poi_cols = {\n",
    "        'distance': ['atm_distance', 'bar_distance', 'nightclubs_distance', 'metro_distance'],\n",
    "        'count': ['atms_count', 'bars_count', 'nightclubs_count', 'metro_and_trains_count', 'bus_stops_count', 'schools_count']\n",
    "    }\n",
    "\n",
    "    # Check for presence of POI columns\n",
    "    missing_distance_cols = [col for col in expected_poi_cols['distance'] if col not in df.columns]\n",
    "    missing_count_cols = [col for col in expected_poi_cols['count'] if col not in df.columns]\n",
    "\n",
    "    if missing_distance_cols:\n",
    "        validation_results['warnings'].append(f\"Missing distance columns: {missing_distance_cols}\")\n",
    "    if missing_count_cols:\n",
    "        validation_results['warnings'].append(f\"Missing count columns: {missing_count_cols}\")\n",
    "\n",
    "    # Validate distance values (should be non-negative)\n",
    "    for col in expected_poi_cols['distance']:\n",
    "        if col in df.columns:\n",
    "            negative_distances = (df[col] < 0).sum()\n",
    "            if negative_distances > 0:\n",
    "                validation_results['errors'].append(f\"Column {col} has {negative_distances} negative distance values\")\n",
    "\n",
    "    # Validate count values (should be non-negative integers)\n",
    "    for col in expected_poi_cols['count']:\n",
    "        if col in df.columns:\n",
    "            negative_counts = (df[col] < 0).sum()\n",
    "            if negative_counts > 0:\n",
    "                validation_results['errors'].append(f\"Column {col} has {negative_counts} negative count values\")\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "# Path variables and data loading\n",
    "# base_dir = \"/drive/MyDrive/Data Mining and Machine Learning/Progetto\"\n",
    "base_dir = r\"C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\"\n",
    "integrated_dir = os.path.join(base_dir, \"DataIntegrated\")\n",
    "integrated_data_file = os.path.join(integrated_dir, \"integrated_crime_data.csv\")\n",
    "os.makedirs(integrated_dir, exist_ok=True)\n",
    "\n",
    "# Load integrated dataset\n",
    "print(\"=== Loading Integrated Data ===\")\n",
    "try:\n",
    "    if os.path.exists(integrated_data_file):\n",
    "        df = pd.read_csv(integrated_data_file)\n",
    "        initial_rows = len(df)\n",
    "        print(f\"Integrated dataset loaded: {initial_rows} rows and {df.shape[1]} columns\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "        # Validate data quality\n",
    "        if initial_rows == 0:\n",
    "            raise ValueError(\"Integrated dataset is empty\")\n",
    "\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find integrated dataset at: {integrated_data_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading integrated dataset: {e}\")\n",
    "    raise RuntimeError(f\"Failed to load integrated dataset: {e}\")\n",
    "\n",
    "# Perform spatial data validation\n",
    "print(\"\\n=== Spatial Data Quality Validation ===\")\n",
    "spatial_validation = validate_spatial_data(df)\n",
    "poi_validation = validate_poi_features(df)\n",
    "\n",
    "# Display validation results\n",
    "for validation_name, results in [('Spatial', spatial_validation), ('POI', poi_validation)]:\n",
    "    print(f\"\\n--- {validation_name} Validation Results ---\")\n",
    "    if results['errors']:\n",
    "        print(\"ERRORS:\")\n",
    "        for error in results['errors']:\n",
    "            print(f\"  {error}\")\n",
    "    if results['warnings']:\n",
    "        print(\"WARNINGS:\")\n",
    "        for warning in results['warnings']:\n",
    "            print(f\"  {warning}\")\n",
    "    if not results['errors'] and not results['warnings']:\n",
    "        print(\"All checks passed successfully.\")\n",
    "\n",
    "# Display dataset overview\n",
    "print(\"\\n=== Integrated Dataset Overview ===\")\n",
    "print(df.info())\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(df.describe())\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "print(df.sample(min(3, len(df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Feature Engineering\n",
    "\n",
    "### 1. Intelligent Column Detection\n",
    "- **Existing column verification**: Only processes columns that actually exist in the dataset\n",
    "- **Safe column removal**: Only removes technical columns if they are present\n",
    "- **Robust POI detection**: Automatically finds POI-related columns regardless of naming\n",
    "\n",
    "### 2. Basic Feature Engineering\n",
    "- **Distance aggregations**: MIN_POI_DISTANCE, AVG_POI_DISTANCE, MAX_POI_DISTANCE (only if distance columns exist)\n",
    "- **Density metrics**: TOTAL_POI_COUNT, POI_DIVERSITY (only if count columns exist)\n",
    "\n",
    "### 3. Safe Column Standardization\n",
    "- **Conditional renaming**: Only renames columns that actually exist in the dataset\n",
    "- **POI standardization**: Converts POI column names to uppercase for consistency\n",
    "- **No assumptions**: Does not attempt to rename columns that don't exist\n",
    "\n",
    "### 4. Robust Data Validation\n",
    "- **Comprehensive logging**: Shows exactly which columns are found and processed\n",
    "- **Missing data handling**: Only processes POI columns that actually exist\n",
    "- **Final verification**: Reports actual column names and shapes\n",
    "\n",
    "## Save Cleaned Data\n",
    "\n",
    "Finally, the cleaned and integrated dataset is saved as a new CSV file. The code ensures that only valid, existing data is processed and exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13298,
     "status": "ok",
     "timestamp": 1748177413179,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "pbZnR2xk27yo",
    "outputId": "21d9000e-cf27-4c54-ad2a-850a74fb6242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Column Cleaning ===\n",
      "Removing technical columns: ['fid', 'join_fid', 'join_full_id', 'join_osm_id']\n",
      "Removed 4 technical columns\n",
      "\n",
      "Current columns in dataset: ['BORO_NM', 'CMPLNT_FR_', 'CMPLNT_F_1', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCC', 'OFNS_DESC', 'PARKS_NM', 'PD_CD', 'PREM_TYP_D', 'SUSP_AGE_G', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GR', 'VIC_RACE', 'VIC_SEX', 'Latitude', 'Longitude', 'bar_distance', 'nightclubs_distance', 'atm_distance', 'atms_count', 'bars_count', 'bus_stops_count', 'metro_and_trains_count', 'nightclubs_count', 'schools_count', 'metro_distance']\n",
      "\n",
      "Found POI columns: ['bar_distance', 'nightclubs_distance', 'atm_distance', 'atms_count', 'bars_count', 'bus_stops_count', 'metro_and_trains_count', 'nightclubs_count', 'schools_count', 'metro_distance']\n",
      "Removed 0 rows with missing POI data\n",
      "\n",
      "=== Column Renaming ===\n",
      "Checking 28 columns for renaming...\n",
      "Renamed 16 columns: {'CMPLNT_FR_': 'CMPLNT_FR_DT', 'CMPLNT_F_1': 'CMPLNT_FR_TM', 'LOC_OF_OCC': 'LOC_OF_OCCUR_DESC', 'PREM_TYP_D': 'PREM_TYP_DESC', 'SUSP_AGE_G': 'SUSP_AGE_GROUP', 'VIC_AGE_GR': 'VIC_AGE_GROUP', 'atms_count': 'ATMS_COUNT', 'bars_count': 'BARS_COUNT', 'bus_stops_count': 'BUS_STOPS_COUNT', 'metro_and_trains_count': 'METROS_COUNT', 'nightclubs_count': 'NIGHTCLUBS_COUNT', 'schools_count': 'SCHOOLS_COUNT', 'atm_distance': 'ATM_DISTANCE', 'bar_distance': 'BAR_DISTANCE', 'nightclubs_distance': 'NIGHTCLUB_DISTANCE', 'metro_distance': 'METRO_DISTANCE'}\n",
      "Creating basic derived features...\n",
      "Found distance columns: ['BAR_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE', 'METRO_DISTANCE']\n",
      "Found count columns: ['ATMS_COUNT', 'BARS_COUNT', 'BUS_STOPS_COUNT', 'METROS_COUNT', 'NIGHTCLUBS_COUNT', 'SCHOOLS_COUNT']\n",
      "\n",
      "=== Final Dataset Summary ===\n",
      "Final shape: (2496759, 34)\n",
      "Columns: 34\n",
      "Memory usage: 2550.96 MB\n",
      "\n",
      "Columns with missing data:\n",
      "  ATMS_COUNT: 17 (0.0%)\n",
      "  BARS_COUNT: 17 (0.0%)\n",
      "  BUS_STOPS_COUNT: 17 (0.0%)\n",
      "  METROS_COUNT: 17 (0.0%)\n",
      "  NIGHTCLUBS_COUNT: 17 (0.0%)\n",
      "  SCHOOLS_COUNT: 17 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Simple and robust column standardization\n",
    "def create_basic_features(df):\n",
    "    \"\"\"\n",
    "    Create only basic derived features from existing POI data\n",
    "    \"\"\"\n",
    "    print(\"Creating basic derived features...\")\n",
    "\n",
    "    # Distance-based features - only if distance columns exist\n",
    "    distance_cols = [col for col in df.columns if 'distance' in col.lower()]\n",
    "    if distance_cols:\n",
    "        print(f\"Found distance columns: {distance_cols}\")\n",
    "        # Create aggregated distance features\n",
    "        df['MIN_POI_DISTANCE'] = df[distance_cols].min(axis=1)\n",
    "        df['AVG_POI_DISTANCE'] = df[distance_cols].mean(axis=1)\n",
    "        df['MAX_POI_DISTANCE'] = df[distance_cols].max(axis=1)\n",
    "\n",
    "    # Count-based features - only if count columns exist\n",
    "    count_cols = [col for col in df.columns if 'count' in col.lower()]\n",
    "    if count_cols:\n",
    "        print(f\"Found count columns: {count_cols}\")\n",
    "        # Total POI density\n",
    "        df['TOTAL_POI_COUNT'] = df[count_cols].sum(axis=1)\n",
    "\n",
    "        # Create POI diversity index (number of different POI types present)\n",
    "        df['POI_DIVERSITY'] = (df[count_cols] > 0).sum(axis=1)\n",
    "\n",
    "        #Normalized POI counts\n",
    "        df['POI_DENSITY_SCORE'] = df['TOTAL_POI_COUNT'] / (df['TOTAL_POI_COUNT'].max() if df['TOTAL_POI_COUNT'].max() > 0 else 1)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "# List of columns to remove (technical/join-related)\n",
    "columns_to_remove = [\n",
    "    'fid', 'join_fid', 'join_full_id', 'join_osm_id', 'layer', 'path'\n",
    "]\n",
    "\n",
    "# Check which columns actually exist\n",
    "existing_columns_to_remove = [col for col in columns_to_remove if col in df.columns]\n",
    "missing_columns = [col for col in columns_to_remove if col not in df.columns]\n",
    "\n",
    "print(f\"=== Column Cleaning ===\")\n",
    "if existing_columns_to_remove:\n",
    "    print(f\"Removing technical columns: {existing_columns_to_remove}\")\n",
    "else:\n",
    "    print(\"No technical columns found to remove\")\n",
    "\n",
    "# Remove technical columns\n",
    "df_cleaned = df.drop(columns=existing_columns_to_remove, errors='ignore')\n",
    "print(f\"Removed {len(existing_columns_to_remove)} technical columns\")\n",
    "\n",
    "# Print current columns to see what we actually have\n",
    "print(f\"\\nCurrent columns in dataset: {list(df_cleaned.columns)}\")\n",
    "\n",
    "# Handle missing POI data more intelligently\n",
    "# First, let's see what POI columns actually exist\n",
    "poi_columns = [col for col in df_cleaned.columns if 'count' in col.lower() or 'distance' in col.lower()]\n",
    "print(f\"\\nFound POI columns: {poi_columns}\")\n",
    "\n",
    "if poi_columns:\n",
    "    rows_before = len(df_cleaned)\n",
    "    # Only remove rows where ALL POI features are missing (likely integration failures)\n",
    "    df_cleaned = df_cleaned.dropna(subset=poi_columns, how='all')\n",
    "    rows_after = len(df_cleaned)\n",
    "    print(f\"Removed {rows_before - rows_after} rows with missing POI data\")\n",
    "else:\n",
    "    print(\"Warning: No POI columns found for missing data filtering\")\n",
    "\n",
    "# CAREFUL column renaming - only rename columns that actually exist\n",
    "print(\"\\n=== Column Renaming ===\")\n",
    "actual_columns = set(df_cleaned.columns)\n",
    "print(f\"Checking {len(actual_columns)} columns for renaming...\")\n",
    "\n",
    "# Define renaming map but only apply to existing columns\n",
    "column_rename_map = {\n",
    "# Date/Time columns\n",
    "    'CMPLNT_FR_': 'CMPLNT_FR_DT',\n",
    "    'CMPLNT_F_1': 'CMPLNT_FR_TM',\n",
    "\n",
    "    # Description columns\n",
    "    'LOC_OF_OCC': 'LOC_OF_OCCUR_DESC',\n",
    "    'PREM_TYP_D': 'PREM_TYP_DESC',\n",
    "    'PD_DESC': 'PD_DESCRIPTION',\n",
    "\n",
    "    # Age group columns\n",
    "    'SUSP_AGE_G': 'SUSP_AGE_GROUP',\n",
    "    'VIC_AGE_GR': 'VIC_AGE_GROUP',\n",
    "\n",
    "    # POI count columns (standardize to uppercase)\n",
    "    'atms_count': 'ATMS_COUNT',\n",
    "    'bars_count': 'BARS_COUNT',\n",
    "    'bus_stops_count': 'BUS_STOPS_COUNT',\n",
    "    'metro_and_trains_count': 'METROS_COUNT',\n",
    "    'nightclubs_count': 'NIGHTCLUBS_COUNT',\n",
    "    'schools_count': 'SCHOOLS_COUNT',\n",
    "\n",
    "    # POI distance columns (standardize to uppercase)\n",
    "    'atm_distance': 'ATM_DISTANCE',\n",
    "    'bar_distance': 'BAR_DISTANCE',\n",
    "    'nightclubs_distance': 'NIGHTCLUB_DISTANCE',\n",
    "    'metro_distance': 'METRO_DISTANCE'\n",
    "}\n",
    "\n",
    "# Apply renaming only for columns that actually exist\n",
    "existing_renames = {old: new for old, new in column_rename_map.items() if old in actual_columns}\n",
    "if existing_renames:\n",
    "    df_cleaned.rename(columns=existing_renames, inplace=True)\n",
    "    print(f\"Renamed {len(existing_renames)} columns: {existing_renames}\")\n",
    "else:\n",
    "    print(\"No columns found for renaming\")\n",
    "\n",
    "# Create basic derived features\n",
    "df_cleaned = create_basic_features(df_cleaned)\n",
    "\n",
    "# Final data quality check\n",
    "print(f\"\\n=== Final Dataset Summary ===\")\n",
    "print(f\"Final shape: {df_cleaned.shape}\")\n",
    "print(f\"Columns: {len(df_cleaned.columns)}\")\n",
    "print(f\"Memory usage: {df_cleaned.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Check for missing data\n",
    "missing_data_summary = df_cleaned.isnull().sum()\n",
    "if missing_data_summary.sum() > 0:\n",
    "    print(f\"\\nColumns with missing data:\")\n",
    "    for col, missing_count in missing_data_summary[missing_data_summary > 0].items():\n",
    "        print(f\"  {col}: {missing_count} ({missing_count/len(df_cleaned)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo missing data in final dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107518,
     "status": "ok",
     "timestamp": 1748177520726,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "5ccFd9hn3Qkj",
    "outputId": "3f4f6814-2f4e-409b-e7da-f8d9726339fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Validation Before Export ===\n",
      "All essential columns present\n",
      "No duplicate records found\n",
      "\n",
      "--- Data Type Summary ---\n",
      "float64    17\n",
      "object     14\n",
      "int64       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Processing Summary ===\n",
      "Original rows: 2496759\n",
      "Final rows: 2496759\n",
      "Original columns: 32\n",
      "Final columns: 34\n",
      "Rows removed: 0\n",
      "\n",
      "Successfully saved cleaned integrated dataset to: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\DataIntegrated\\cleaned_integrated_crime_data.csv\n",
      "Dataset shape: (2496759, 34)\n",
      "File size: 756.06 MB\n",
      "Processing log saved to: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\DataIntegrated\\integration_processing_log.json\n",
      "\n",
      "DATA INTEGRATION COMPLETED SUCCESSFULLY\n",
      "Final dataset contains 2496759 records with 34 columns\n"
     ]
    }
   ],
   "source": [
    "# Final validation before saving\n",
    "print(\"\\n=== Final Validation Before Export ===\")\n",
    "\n",
    "# Validate essential columns are present\n",
    "essential_columns = ['BORO_NM', 'LAW_CAT_CD', 'Latitude', 'Longitude']\n",
    "missing_essential = [col for col in essential_columns if col not in df_cleaned.columns]\n",
    "\n",
    "if missing_essential:\n",
    "    print(f\"Warning: Missing essential columns: {missing_essential}\")\n",
    "else:\n",
    "    print(\"All essential columns present\")\n",
    "\n",
    "# Check for duplicate records\n",
    "duplicates = df_cleaned.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"Found {duplicates} duplicate records - consider deduplication\")\n",
    "    # Optionally remove duplicates\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(f\"Removed duplicates. Final shape: {df_cleaned.shape}\")\n",
    "else:\n",
    "    print(\"No duplicate records found\")\n",
    "\n",
    "# Validate data types\n",
    "print(\"\\n--- Data Type Summary ---\")\n",
    "print(df_cleaned.dtypes.value_counts())\n",
    "\n",
    "# Create processing log\n",
    "processing_log = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'original_rows': initial_rows,\n",
    "    'final_rows': len(df_cleaned),\n",
    "    'original_columns': len(df.columns),\n",
    "    'final_columns': len(df_cleaned.columns),\n",
    "    'rows_removed': initial_rows - len(df_cleaned),\n",
    "    'columns_in_final_dataset': list(df_cleaned.columns),\n",
    "    'processing_successful': True\n",
    "}\n",
    "\n",
    "print(f\"\\n=== Processing Summary ===\")\n",
    "print(f\"Original rows: {processing_log['original_rows']}\")\n",
    "print(f\"Final rows: {processing_log['final_rows']}\")\n",
    "print(f\"Original columns: {processing_log['original_columns']}\")\n",
    "print(f\"Final columns: {processing_log['final_columns']}\")\n",
    "print(f\"Rows removed: {processing_log['rows_removed']}\")\n",
    "\n",
    "# Save the cleaned integrated dataset\n",
    "cleaned_integrated_file_path = os.path.join(integrated_dir, \"cleaned_integrated_crime_data.csv\")\n",
    "try:\n",
    "    df_cleaned.to_csv(cleaned_integrated_file_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved cleaned integrated dataset to: {cleaned_integrated_file_path}\")\n",
    "    print(f\"Dataset shape: {df_cleaned.shape}\")\n",
    "    print(f\"File size: {os.path.getsize(cleaned_integrated_file_path) / (1024*1024):.2f} MB\")\n",
    "\n",
    "    # Save processing log\n",
    "    log_file_path = os.path.join(integrated_dir, \"integration_processing_log.json\")\n",
    "    import json\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        json.dump(processing_log, f, indent=2)\n",
    "    print(f\"Processing log saved to: {log_file_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving files: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nDATA INTEGRATION COMPLETED SUCCESSFULLY\")\n",
    "print(f\"Final dataset contains {len(df_cleaned)} records with {len(df_cleaned.columns)} columns\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
