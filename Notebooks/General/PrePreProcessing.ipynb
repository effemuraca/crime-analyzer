{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1SAVV5BfYJW"
   },
   "source": [
    "# Setup\n",
    "\n",
    "## Import Libraries and Define Paths\n",
    "\n",
    "Import the necessary libraries (pandas, numpy, os, warnings) and define the paths for input (historical and current year datasets) and the merged output dataset. Also create the output directory if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq_YJO1KQ9rm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Path to the datasets (for local environment)\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"JupyterOutputs\"))\n",
    "year_to_date_dataset = os.path.join(base_dir, \"Raw\", \"NYPD_Complaint_Data_Current__Year_To_Date__20250410.csv\")\n",
    "historic_dataset = os.path.join(base_dir, \"Raw\", \"NYPD_Complaint_Data_Historic_20250313.csv\")\n",
    "output_dataset = os.path.join(base_dir, \"Merged\", \"NYPD_Complaints_Merged.csv\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(output_dataset), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VADuRss2fYJb"
   },
   "source": [
    "# Data Loading and Merging\n",
    "\n",
    "Load the two source CSV files, align their columns (adding NaN for missing columns), concatenate them, convert 'CMPLNT_FR_DT' column to datetime, filter records from 2020 onwards, and save the merged dataset. Include fallback mechanisms for potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 212437,
     "status": "ok",
     "timestamp": 1748159084273,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "DipKMTUTZNLP",
    "outputId": "d8571fca-987e-4e13-add2-60d5155f3c70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Loading and Merging ===\n",
      "Loading historic dataset from C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Raw\\NYPD_Complaint_Data_Historic_20250313.csv...\n",
      "Loading year-to-date dataset from C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Raw\\NYPD_Complaint_Data_Current__Year_To_Date__20250410.csv...\n",
      "Historic dataset shape: (8914838, 35)\n",
      "Year-to-date dataset shape: (577108, 36)\n",
      "Year-to-date columns:\n",
      "['CMPLNT_NUM', 'ADDR_PCT_CD', 'BORO_NM', 'CMPLNT_FR_DT', 'CMPLNT_FR_TM', 'CMPLNT_TO_DT', 'CMPLNT_TO_TM', 'CRM_ATPT_CPTD_CD', 'HADEVELOPT', 'HOUSING_PSA', 'JURISDICTION_CODE', 'JURIS_DESC', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PARKS_NM', 'PATROL_BORO', 'PD_CD', 'PD_DESC', 'PREM_TYP_DESC', 'RPT_DT', 'STATION_NAME', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'TRANSIT_DISTRICT', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'X_COORD_CD', 'Y_COORD_CD', 'Latitude', 'Longitude', 'Lat_Lon', 'New Georeferenced Column']\n",
      "Historic columns:\n",
      "['CMPLNT_NUM', 'CMPLNT_FR_DT', 'CMPLNT_FR_TM', 'CMPLNT_TO_DT', 'CMPLNT_TO_TM', 'ADDR_PCT_CD', 'RPT_DT', 'KY_CD', 'OFNS_DESC', 'PD_CD', 'PD_DESC', 'CRM_ATPT_CPTD_CD', 'LAW_CAT_CD', 'BORO_NM', 'LOC_OF_OCCUR_DESC', 'PREM_TYP_DESC', 'JURIS_DESC', 'JURISDICTION_CODE', 'PARKS_NM', 'HADEVELOPT', 'HOUSING_PSA', 'X_COORD_CD', 'Y_COORD_CD', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'TRANSIT_DISTRICT', 'Latitude', 'Longitude', 'Lat_Lon', 'PATROL_BORO', 'STATION_NAME', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX']\n",
      "Aligning columns between datasets...\n",
      "Merging datasets...\n",
      "Converting CMPLNT_FR_DT to datetime...\n",
      "Filtering data from 2020 onwards...\n",
      "Saving merged and filtered dataset to C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Merged\\NYPD_Complaints_Merged.csv...\n",
      "Merge and filtering completed. Total rows from 2020 onwards: 2512541\n",
      "Merged dataset saved to C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\Merged\\NYPD_Complaints_Merged.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Data Loading and Merging ===\")\n",
    "print(f\"Loading historic dataset from {historic_dataset}...\")\n",
    "print(f\"Loading year-to-date dataset from {year_to_date_dataset}...\")\n",
    "\n",
    "# Check if datasets exist\n",
    "historic_exists = os.path.exists(historic_dataset)\n",
    "ytd_exists = os.path.exists(year_to_date_dataset)\n",
    "\n",
    "df = None # Initialize df to None\n",
    "\n",
    "if historic_exists and ytd_exists:\n",
    "    # Load the datasets\n",
    "    try:\n",
    "        historic_df = pd.read_csv(historic_dataset)\n",
    "        year_to_date_df = pd.read_csv(year_to_date_dataset)\n",
    "\n",
    "        print(f\"Historic dataset shape: {historic_df.shape}\")\n",
    "        print(f\"Year-to-date dataset shape: {year_to_date_df.shape}\")\n",
    "\n",
    "        print(\"Year-to-date columns:\")\n",
    "        print(year_to_date_df.columns.tolist())\n",
    "\n",
    "        print(\"Historic columns:\")\n",
    "        print(historic_df.columns.tolist())\n",
    "\n",
    "        # Align columns (adds NaN for missing columns)\n",
    "        print(\"Aligning columns between datasets...\")\n",
    "        common_columns = year_to_date_df.columns.union(historic_df.columns)\n",
    "        historic_df_aligned = historic_df.reindex(columns=common_columns)\n",
    "        year_to_date_df_aligned = year_to_date_df.reindex(columns=common_columns)\n",
    "\n",
    "        # Merge the two datasets\n",
    "        print(\"Merging datasets...\")\n",
    "        merged_df = pd.concat([historic_df_aligned, year_to_date_df_aligned], ignore_index=True)\n",
    "\n",
    "        # Convert CMPLNT_FR_DT to datetime\n",
    "        print(\"Converting CMPLNT_FR_DT to datetime...\")\n",
    "        merged_df['CMPLNT_FR_DT'] = pd.to_datetime(merged_df['CMPLNT_FR_DT'], errors='coerce')\n",
    "\n",
    "        # Filter out rows before 2020\n",
    "        print(\"Filtering data from 2020 onwards...\")\n",
    "        merged_df = merged_df[merged_df['CMPLNT_FR_DT'] >= '2020-01-01']\n",
    "\n",
    "        # Save the merged dataset\n",
    "        print(f\"Saving merged and filtered dataset to {output_dataset}...\")\n",
    "        merged_df.to_csv(output_dataset, index=False)\n",
    "\n",
    "        print(f\"Merge and filtering completed. Total rows from 2020 onwards: {merged_df.shape[0]}\")\n",
    "        print(f\"Merged dataset saved to {output_dataset}\")\n",
    "\n",
    "        # Continue with the existing dataset for prepreprocessing\n",
    "        df = merged_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error during dataset merging: {e}\")\n",
    "        print(\"Attempting to load the pre-merged dataset instead...\")\n",
    "        # Try to load the pre-merged dataset as fallback\n",
    "        if os.path.exists(output_dataset):\n",
    "            try:\n",
    "                df = pd.read_csv(output_dataset)\n",
    "                print(f\"Pre-merged dataset loaded successfully: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "            except Exception as load_err:\n",
    "                 print(f\"Error loading pre-merged dataset {output_dataset}: {load_err}\")\n",
    "                 raise RuntimeError(f\"Could not load any dataset: {load_err}\")\n",
    "        else:\n",
    "            print(f\"Error: Could not load pre-merged dataset {output_dataset}\")\n",
    "            raise FileNotFoundError(f\"No datasets available for processing\")\n",
    "else:\n",
    "    print(\"One or both datasets do not exist.\")\n",
    "    if not historic_exists:\n",
    "        print(f\"Historic dataset not found at: {historic_dataset}\")\n",
    "    if not ytd_exists:\n",
    "        print(f\"Year-to-date dataset not found at: {year_to_date_dataset}\")\n",
    "    raise FileNotFoundError(\"Required datasets are missing. Please check the file paths.\")\n",
    "\n",
    "# Final check if df was loaded\n",
    "if df is None:\n",
    "    print(\"Error: DataFrame 'df' could not be loaded or created.\")\n",
    "    raise RuntimeError(\"DataFrame loading failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8sfTgh1fYJd"
   },
   "source": [
    "# Data Cleaning - Initial Overview\n",
    "\n",
    "Display basic information and the list of columns in the loaded (merged and filtered) dataset before cleaning. Perform duplicate checks and basic data validation (missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14152,
     "status": "ok",
     "timestamp": 1748159098432,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "q-tAoRSnMaa0",
    "outputId": "241d80ab-c835-4b25-bba2-17c723a10a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Preprocessing - Initial Overview ===\n",
      "Dataset loaded successfully: 2512541 rows and 36 columns.\n",
      "Columns in the original dataset: ['ADDR_PCT_CD', 'BORO_NM', 'CMPLNT_FR_DT', 'CMPLNT_FR_TM', 'CMPLNT_NUM', 'CMPLNT_TO_DT', 'CMPLNT_TO_TM', 'CRM_ATPT_CPTD_CD', 'HADEVELOPT', 'HOUSING_PSA', 'JURISDICTION_CODE', 'JURIS_DESC', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'Lat_Lon', 'Latitude', 'Longitude', 'New Georeferenced Column', 'OFNS_DESC', 'PARKS_NM', 'PATROL_BORO', 'PD_CD', 'PD_DESC', 'PREM_TYP_DESC', 'RPT_DT', 'STATION_NAME', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'TRANSIT_DISTRICT', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX', 'X_COORD_CD', 'Y_COORD_CD']\n",
      "\n",
      "=== Data Quality Checks ===\n",
      "No duplicate rows found.\n",
      "Date range: 2020-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "Missing values per column:\n",
      "  TRANSIT_DISTRICT: 2443447 (97.3%)\n",
      "  New Georeferenced Column: 1936566 (77.1%)\n",
      "  HOUSING_PSA: 540598 (21.5%)\n",
      "  CMPLNT_TO_DT: 180993 (7.2%)\n",
      "  PD_CD: 2090 (0.1%)\n",
      "  ADDR_PCT_CD: 97 (0.0%)\n",
      "  Lat_Lon: 28 (0.0%)\n",
      "  Longitude: 28 (0.0%)\n",
      "  Latitude: 28 (0.0%)\n",
      "  X_COORD_CD: 24 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Data Preprocessing - Initial Overview ===\")\n",
    "\n",
    "# Information about the original dataset\n",
    "print(f\"Dataset loaded successfully: {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "print(f\"Columns in the original dataset: {df.columns.tolist()}\")\n",
    "\n",
    "# Data validation and quality checks\n",
    "print(\"\\n=== Data Quality Checks ===\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"Found {duplicates} duplicate rows ({duplicates/len(df)*100:.2f}% of data)\")\n",
    "    print(\"Removing duplicate rows...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"After removing duplicates: {df.shape[0]} rows\")\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n",
    "\n",
    "# Basic data validation\n",
    "print(f\"Date range: {df['CMPLNT_FR_DT'].min()} to {df['CMPLNT_FR_DT'].max()}\")\n",
    "print(f\"Missing values per column:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "if len(missing_summary) > 0:\n",
    "    for col, count in missing_summary.head(10).items():\n",
    "        print(f\"  {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No missing values found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMr2FdOSfYJe"
   },
   "source": [
    "# Data Cleaning - Column Removal\n",
    "\n",
    "Remove specified irrelevant, redundant, or non-useful columns from the DataFrame to reduce dimensionality and noise. Warn if columns not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1748159110538,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "j_1G1EY9fYJe",
    "outputId": "3b55f93e-61e6-444d-8b14-e8da47ce5ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to remove columns: ['CMPLNT_NUM', 'ADDR_PCT_CD', 'CMPLNT_TO_DT', 'CMPLNT_TO_TM', 'CRM_ATPT_CPTD_CD', 'HADEVELOPT', 'HOUSING_PSA', 'JURISDICTION_CODE', 'JURIS_DESC', 'PATROL_BORO', 'PD_DESC', 'RPT_DT', 'STATION_NAME', 'TRANSIT_DISTRICT', 'X_COORD_CD', 'Y_COORD_CD', 'Lat_Lon', 'New Georeferenced Column']\n",
      "Removed columns: ['CMPLNT_NUM', 'ADDR_PCT_CD', 'CMPLNT_TO_DT', 'CMPLNT_TO_TM', 'CRM_ATPT_CPTD_CD', 'HADEVELOPT', 'HOUSING_PSA', 'JURISDICTION_CODE', 'JURIS_DESC', 'PATROL_BORO', 'PD_DESC', 'RPT_DT', 'STATION_NAME', 'TRANSIT_DISTRICT', 'X_COORD_CD', 'Y_COORD_CD', 'Lat_Lon', 'New Georeferenced Column']\n",
      "Final cleaned dataset: 2512541 rows and 18 columns\n",
      "Columns in the final dataset: ['BORO_NM', 'CMPLNT_FR_DT', 'CMPLNT_FR_TM', 'KY_CD', 'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'Latitude', 'Longitude', 'OFNS_DESC', 'PARKS_NM', 'PD_CD', 'PREM_TYP_DESC', 'SUSP_AGE_GROUP', 'SUSP_RACE', 'SUSP_SEX', 'VIC_AGE_GROUP', 'VIC_RACE', 'VIC_SEX']\n",
      "\n",
      "=== First rows of the final cleaned dataset ===\n",
      "     BORO_NM CMPLNT_FR_DT CMPLNT_FR_TM  KY_CD   LAW_CAT_CD LOC_OF_OCCUR_DESC  \\\n",
      "53     BRONX   2021-09-08     21:00:00    341  MISDEMEANOR            INSIDE   \n",
      "6494  QUEENS   2021-04-01     00:10:00    121       FELONY          FRONT OF   \n",
      "6710   BRONX   2021-01-02     14:22:00    341  MISDEMEANOR          FRONT OF   \n",
      "7951   BRONX   2021-02-03     22:00:00    106       FELONY            (null)   \n",
      "9103   BRONX   2021-01-22     10:00:00    113       FELONY            (null)   \n",
      "\n",
      "       Latitude  Longitude                       OFNS_DESC PARKS_NM  PD_CD  \\\n",
      "53    40.820743 -73.848431                   PETIT LARCENY   (null)  333.0   \n",
      "6494  40.757117 -73.860091  CRIMINAL MISCHIEF & RELATED OF   (null)  269.0   \n",
      "6710  40.851284 -73.852132                   PETIT LARCENY   (null)  333.0   \n",
      "7951  40.877393 -73.866491                  FELONY ASSAULT   (null)  109.0   \n",
      "9103  40.873066 -73.858433                         FORGERY   (null)  725.0   \n",
      "\n",
      "               PREM_TYP_DESC SUSP_AGE_GROUP SUSP_RACE SUSP_SEX VIC_AGE_GROUP  \\\n",
      "53            GROCERY/BODEGA        UNKNOWN     BLACK        F       UNKNOWN   \n",
      "6494  RESIDENCE - APT. HOUSE        UNKNOWN   UNKNOWN        U         45-64   \n",
      "6710        DEPARTMENT STORE          25-44   UNKNOWN        M       UNKNOWN   \n",
      "7951                  STREET          25-44     BLACK        F         45-64   \n",
      "9103                  STREET          45-64     BLACK        M       UNKNOWN   \n",
      "\n",
      "                      VIC_RACE VIC_SEX  \n",
      "53                     UNKNOWN       D  \n",
      "6494  ASIAN / PACIFIC ISLANDER       M  \n",
      "6710                   UNKNOWN       D  \n",
      "7951                     BLACK       M  \n",
      "9103                   UNKNOWN       E  \n"
     ]
    }
   ],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = [\n",
    "    'CMPLNT_NUM', 'ADDR_PCT_CD', 'CMPLNT_TO_DT', 'CMPLNT_TO_TM',\n",
    "    'CRM_ATPT_CPTD_CD', 'HADEVELOPT', 'HOUSING_PSA', 'JURISDICTION_CODE',\n",
    "    'JURIS_DESC', 'PATROL_BORO', 'PD_DESC', 'RPT_DT', 'STATION_NAME',\n",
    "    'TRANSIT_DISTRICT', 'X_COORD_CD', 'Y_COORD_CD', 'Lat_Lon',\n",
    "    'New Georeferenced Column'\n",
    "]\n",
    "\n",
    "print(f\"Attempting to remove columns: {columns_to_remove}\")\n",
    "\n",
    "# Verify existing columns to remove\n",
    "existing_columns = [col for col in columns_to_remove if col in df.columns]\n",
    "missing_columns = [col for col in columns_to_remove if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"Warning: the following columns intended for removal are not present in the dataset: {missing_columns}\")\n",
    "\n",
    "# Remove specified columns (only those that exist)\n",
    "df_cleaned = df.drop(columns=existing_columns, errors='ignore')\n",
    "print(f\"Removed columns: {existing_columns}\")\n",
    "\n",
    "# Information about the dataset after cleaning\n",
    "print(f\"Final cleaned dataset: {df_cleaned.shape[0]} rows and {df_cleaned.shape[1]} columns\")\n",
    "print(f\"Columns in the final dataset: {df_cleaned.columns.tolist()}\")\n",
    "\n",
    "# Display the first rows of the cleaned dataset\n",
    "print(\"\\n=== First rows of the final cleaned dataset ===\")\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc6z40pdfYJe"
   },
   "source": [
    "# Save Cleaned Dataset\n",
    "\n",
    "Save the cleaned dataset (with selected columns removed) to the 'PrePreProcessed' directory. Ensures consistency with the global pipeline and provides a standardized input for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42453,
     "status": "ok",
     "timestamp": 1748159159594,
     "user": {
      "displayName": "Ferdinando Muraca",
      "userId": "08570199584316918220"
     },
     "user_tz": -120
    },
    "id": "Dq-STy5LfYJf",
    "outputId": "325b75be-c833-4bc3-d703-ee10ac1c221a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to output directory: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\PrePreProcessed\n",
      "Cleaned dataset saved to: C:\\Users\\ferdi\\Documents\\GitHub\\crime-analyzer\\JupyterOutputs\\PrePreProcessed\\cleaned_crime_data.csv\n",
      "\n",
      "=== Preprocessing completed ===\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "output_dir = os.path.join(base_dir, \"PrePreProcessed\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Saving to output directory: {output_dir}\")\n",
    "\n",
    "cleaned_file_path = os.path.join(output_dir, \"cleaned_crime_data.csv\")\n",
    "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"Cleaned dataset saved to: {cleaned_file_path}\")\n",
    "\n",
    "print(\"\\n=== Preprocessing completed ===\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
