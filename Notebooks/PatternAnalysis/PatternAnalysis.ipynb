{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8629c333",
   "metadata": {},
   "source": [
    "# Pattern Mining of NYC Crime Data with FP-Growth\n",
    "\n",
    "This notebook builds a reproducible pipeline to mine association rules from the finalized crime dataset using FP-Growth. It includes:\n",
    "\n",
    "- Global mining on the full dataset\n",
    "- Conditional rules per Borough and per Time Bucket\n",
    "- Temporal validation (hold-out) and rolling-window stability checks\n",
    "- Rule pruning, scoring, and ranking with multiple quality metrics\n",
    "- Exports of JSON catalogs, CSVs, and human-readable insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e583c",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "Import core libraries, set deterministic options, and configure the environment for pattern mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb86e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from itertools import combinations\n",
    "\n",
    "# Deterministic options and clean logs\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported. Seeds set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa89dc",
   "metadata": {},
   "source": [
    "## Paths, Constants, and Parameters\n",
    "\n",
    "Define input/output paths, key parameters for discretization, and FP-Growth mining configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Path setup ---\n",
    "\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"JupyterOutputs\"))\n",
    "path_input_dir = os.path.join(base_dir, \"Final\")\n",
    "# Use the uncompressed version explicitly\n",
    "input_csv = os.path.join(path_input_dir, \"final_crime_data.csv\")\n",
    "\n",
    "notebook_name = \"PatternAnalysis\"\n",
    "path_output = os.path.join(base_dir, notebook_name)\n",
    "os.makedirs(path_output, exist_ok=True)\n",
    "\n",
    "# Define uppercase aliases used throughout the notebook for consistency\n",
    "INPUT_CSV = Path(input_csv)\n",
    "PATH_OUTPUT = Path(path_output)\n",
    "NOTEBOOK_NAME = notebook_name\n",
    "\n",
    "print(f\"Project Root: {base_dir}\")\n",
    "print(f\"Input CSV: {INPUT_CSV}\")\n",
    "print(f\"Output Directory: {PATH_OUTPUT}\")\n",
    "\n",
    "# Top-N category caps (fixed)\n",
    "TOPN_LOC_OF_OCCUR = 20\n",
    "TOPN_SUSP_AGE = 10\n",
    "TOPN_VIC_AGE = 10\n",
    "\n",
    "# Bins for numeric features (fixed)\n",
    "DISTANCE_BINS = [0, 250, 1000, float('inf')]\n",
    "DISTANCE_LABELS = ['<250m', '250-1000m', '>1000m']\n",
    "\n",
    "# FP-Growth defaults and auto-tuning thresholds (fixed)\n",
    "GLOBAL_MIN_SUPPORT = 0.5\n",
    "SLICE_MIN_SUPPORT_RANGE = (0.01, 0.02)\n",
    "MIN_CONFIDENCE = 0.40\n",
    "MAX_ITEMSET_LEN = 4\n",
    "AUTO_MIN_RULES = 150\n",
    "AUTO_MAX_RULES = 800\n",
    "AUTO_SUPPORT_BOUNDS = (0.02, 0.20)\n",
    "AUTO_MAX_ITERS = 10\n",
    "TOP_K = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56eaa65",
   "metadata": {},
   "source": [
    "## Load and Validate Data\n",
    "\n",
    "Load the finalized dataset, apply temporal filters, and ensure date consistency for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading data from: {INPUT_CSV}\")\n",
    "\n",
    "# Load from uncompressed file with inferred dtypes (no explicit dtype mapping)\n",
    "df_raw = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "# Drop unneeded identifier and geo columns if present\n",
    "df_raw.drop(columns=['KY_CD', 'PD_CD', 'Latitude', 'Longitude', 'TO_CHECK_CITIZENS'], errors='ignore', inplace=True)\n",
    "\n",
    "# Add HAS_POI column: 1 if TOTAL_POI_COUNT > 0, else 0\n",
    "if 'TOTAL_POI_COUNT' in df_raw.columns:\n",
    "    df_raw['HAS_POI'] = (df_raw['TOTAL_POI_COUNT'] > 0).astype(int)\n",
    "    print(\"HAS_POI column added. Distribution:\")\n",
    "    print(df_raw['HAS_POI'].value_counts())\n",
    "    df_raw.drop(columns=['TOTAL_POI_COUNT'], errors='ignore', inplace=True)\n",
    "else:\n",
    "    print(\"Warning: TOTAL_POI_COUNT column not found. Setting HAS_POI to 0 for all records.\")\n",
    "    df_raw['HAS_POI'] = 0\n",
    "\n",
    "# --- Temporal Filter: Keep only data for the year 2024 ---\n",
    "date_construction_successful = True\n",
    "\n",
    "if 'YEAR' in df_raw.columns:\n",
    "    df_raw = df_raw[df_raw['YEAR'] == 2024].copy()\n",
    "\n",
    "# Always construct DATE from YEAR/MONTH/DAY\n",
    "for c in ['YEAR', 'MONTH', 'DAY']:\n",
    "    if c in df_raw.columns:\n",
    "        df_raw[c] = pd.to_numeric(df_raw[c], errors='coerce')\n",
    "    else:\n",
    "        print(f\"Warning: {c} column not found. Date construction may be incomplete.\")\n",
    "        df_raw[c] = np.nan\n",
    "\n",
    "# If DAY is missing or NaN, default to 1 to avoid NaT when only year/month exist\n",
    "df_raw['DAY'] = df_raw['DAY'].fillna(1)\n",
    "\n",
    "try:\n",
    "    df_raw['DATE'] = pd.to_datetime(\n",
    "        df_raw[['YEAR', 'MONTH', 'DAY']].rename(columns={'YEAR': 'year', 'MONTH': 'month', 'DAY': 'day'}),\n",
    "        errors='coerce',\n",
    "    )\n",
    "    \n",
    "    # Validate date construction\n",
    "    invalid_dates_count = df_raw['DATE'].isna().sum()\n",
    "    if invalid_dates_count > 0:\n",
    "        print(f\"Warning: {invalid_dates_count:,} records have invalid dates and will be excluded from temporal analysis.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during date construction: {e}\")\n",
    "    df_raw['DATE'] = pd.NaT\n",
    "    date_construction_successful = False\n",
    "\n",
    "# --- Drop columns not used in this specific analysis to speed up processing ---\n",
    "unused_cols = [\n",
    "    'PREM_TYP_DESC', 'POI_DIVERSITY', 'POI_DENSITY_SCORE',\n",
    "    'IS_WEEKEND', 'IS_HOLIDAY', 'IS_PAYDAY',\n",
    "    'SAME_AGE_GROUP', 'SAME_SEX', 'OFNS_DESC'\n",
    "]\n",
    "df_raw.drop(columns=unused_cols, errors='ignore', inplace=True)\n",
    "\n",
    "# Materialize analysis DataFrame\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Drop invalid dates for temporal analyses\n",
    "if date_construction_successful:\n",
    "    initial_count = len(df)\n",
    "    df.dropna(subset=['DATE'], inplace=True)\n",
    "    dropped_count = initial_count - len(df)\n",
    "    if dropped_count > 0:\n",
    "        print(f\"Removed {dropped_count:,} records with invalid dates.\")\n",
    "\n",
    "print(f\"Data loaded: {len(df):,} rows after filtering and date construction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb488e",
   "metadata": {},
   "source": [
    "### Override Parameters from Diagnostics\n",
    "\n",
    "Apply overrides for Top-N caps and binning if suggested by the diagnostics profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics directory path\n",
    "PATH_DIAGNOSTICS = PATH_OUTPUT / 'Diagnostics'\n",
    "PROFILE = {}\n",
    "\n",
    "profile_path = PATH_DIAGNOSTICS / 'profile_summary.json'\n",
    "if profile_path.exists():\n",
    "    try:\n",
    "        with open(profile_path, 'r', encoding='utf-8') as f:\n",
    "            PROFILE = json.load(f)\n",
    "        print(f\"Diagnostics profile loaded from: {profile_path}\")\n",
    "        # Quick sanity: show suggested top-N caps if present\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to read diagnostics profile: {e}\")\n",
    "else:\n",
    "    print(\"No diagnostics profile found; proceeding without external guidance.\")\n",
    "\n",
    "# Use Top-N suggestions from PROFILE when present; otherwise keep current defaults\n",
    "if isinstance(PROFILE, dict) and PROFILE.get('topn_suggestions'):\n",
    "    info = PROFILE['topn_suggestions']\n",
    "    # Map notebook vars -> profile keys\n",
    "    prev_vals = (TOPN_LOC_OF_OCCUR, TOPN_SUSP_AGE, TOPN_VIC_AGE)\n",
    "    TOPN_LOC_OF_OCCUR = int(info.get('LOC_OF_OCCUR_DESC', {}).get('top_n', TOPN_LOC_OF_OCCUR))\n",
    "    TOPN_SUSP_AGE = int(info.get('SUSP_AGE_GROUP', {}).get('top_n', TOPN_SUSP_AGE))\n",
    "    TOPN_VIC_AGE = int(info.get('VIC_AGE_GROUP', {}).get('top_n', TOPN_VIC_AGE))\n",
    "    if prev_vals != (TOPN_LOC_OF_OCCUR, TOPN_SUSP_AGE, TOPN_VIC_AGE):\n",
    "        applied.append('Top-N caps')\n",
    "\n",
    "applied = []\n",
    "\n",
    "# Adopt binning from Diagnostics if consistent and safe\n",
    "if isinstance(PROFILE, dict) and PROFILE.get('proposed_bins'):\n",
    "    pb = PROFILE['proposed_bins']\n",
    "\n",
    "    # Helper to safely adopt shared bins and labels\n",
    "    def adopt_bins_and_labels(section_key, current_bins, current_labels):\n",
    "        section = pb.get(section_key)\n",
    "        if not isinstance(section, dict) or not section:\n",
    "            return current_bins, current_labels, False\n",
    "        # Collect candidate bins/labels\n",
    "        bins_list = []\n",
    "        labels_list = []\n",
    "        for v in section.values():\n",
    "            if isinstance(v, dict):\n",
    "                b = v.get('bins')\n",
    "                l = v.get('labels')\n",
    "                if isinstance(b, list):\n",
    "                    bins_list.append(b)\n",
    "                if isinstance(l, list):\n",
    "                    labels_list.append(l)\n",
    "        changed = False\n",
    "        # If all bins proposals are identical and lengths match labels, adopt\n",
    "        if bins_list and all(b == bins_list[0] for b in bins_list):\n",
    "            new_bins = bins_list[0]\n",
    "            # Labels adoption optional but must match in length\n",
    "            new_labels = current_labels\n",
    "            if labels_list and all(l == labels_list[0] for l in labels_list):\n",
    "                if len(labels_list[0]) == len(current_labels):\n",
    "                    new_labels = labels_list[0]\n",
    "            # Extra safety: ensure cut compatibility\n",
    "            if isinstance(new_bins, list) and len(new_bins) >= 2 and len(new_labels) == len(current_labels):\n",
    "                return new_bins, new_labels, True\n",
    "        return current_bins, current_labels, changed\n",
    "\n",
    "    # Distances\n",
    "    DISTANCE_BINS, DISTANCE_LABELS, ch1 = adopt_bins_and_labels('distances', DISTANCE_BINS, DISTANCE_LABELS)\n",
    "    if ch1:\n",
    "        applied.append('Distance bins/labels')\n",
    "\n",
    "print(f\"Diagnostics-driven overrides applied: {', '.join(applied) if applied else 'none'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f3557",
   "metadata": {},
   "source": [
    "## Data Readiness and Discretization\n",
    "\n",
    "Summarize the columns removed for performance, and outline the main preprocessing steps: normalization, capping, binning, boolean flag creation, and data quality validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99602b",
   "metadata": {},
   "source": [
    "## Preprocess Categorical Features\n",
    "\n",
    "Normalize and cap main categorical features, unify borough labels, and save the set of retained categories for transparency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2786b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_cap(df_in: pd.DataFrame, col: str, top_n: int | None) -> pd.Series:\n",
    "    if col not in df_in.columns:\n",
    "        return pd.Series(['UNKNOWN'] * len(df_in), index=df_in.index, dtype='category')\n",
    "    s = df_in[col].astype('string').fillna('UNKNOWN').str.strip().str.upper()\n",
    "    if top_n is None:\n",
    "        return s.astype('category')\n",
    "    top = s.value_counts().nlargest(top_n).index\n",
    "    return s.where(s.isin(top), 'OTHER').astype('category')\n",
    "\n",
    "# Process main categorical features\n",
    "retained_categories = {}\n",
    "df_proc = pd.DataFrame(index=df.index)\n",
    "\n",
    "cfg = {\n",
    "    'LOC_OF_OCCUR': ( 'LOC_OF_OCCUR_DESC', TOPN_LOC_OF_OCCUR ),\n",
    "    'SUSP_AGE': ( 'SUSP_AGE_GROUP', TOPN_SUSP_AGE ),\n",
    "    'VIC_AGE': ( 'VIC_AGE_GROUP', TOPN_VIC_AGE ),\n",
    "    'LAW_CAT': ( 'LAW_CAT_CD', None ),\n",
    "    'SUSP_RACE': ( 'SUSP_RACE', None ),\n",
    "    'SUSP_SEX': ( 'SUSP_SEX', None ),\n",
    "    'VIC_RACE': ( 'VIC_RACE', None ),\n",
    "    'VIC_SEX': ( 'VIC_SEX', None ),\n",
    "    'SEASON': ( 'SEASON', None ),\n",
    "    'TIME_BUCKET': ( 'TIME_BUCKET', None ),\n",
    "}\n",
    "\n",
    "for new_name, (src, topn) in cfg.items():\n",
    "    if src in df.columns:\n",
    "        df_proc[new_name] = norm_cap(df, src, topn)\n",
    "        retained_categories[new_name] = sorted(df_proc[new_name].cat.categories.tolist())\n",
    "    else:\n",
    "        print(f\"Warning: Source column '{src}' for '{new_name}' not found. Setting to UNKNOWN.\")\n",
    "        df_proc[new_name] = pd.Series(['UNKNOWN'] * len(df), index=df.index, dtype='category')\n",
    "        retained_categories[new_name] = ['UNKNOWN']\n",
    "\n",
    "# HAS_POI column: binary categorical\n",
    "if 'HAS_POI' in df.columns:\n",
    "    df_proc['HAS_POI'] = df['HAS_POI'].map({1: 'YES', 0: 'NO'}).fillna('UNKNOWN').astype('category')\n",
    "    retained_categories['HAS_POI'] = sorted(df_proc['HAS_POI'].cat.categories.tolist())\n",
    "else:\n",
    "    print(\"Warning: 'HAS_POI' not found in df. Setting to UNKNOWN.\")\n",
    "    df_proc['HAS_POI'] = pd.Series(['UNKNOWN'] * len(df), index=df.index, dtype='category')\n",
    "    retained_categories['HAS_POI'] = ['UNKNOWN']\n",
    "\n",
    "# BORO unified: source column is BORO_NM\n",
    "if 'BORO_NM' in df.columns:\n",
    "    df_proc['BORO'] = df['BORO_NM'].astype('string').fillna('UNKNOWN').str.strip().str.upper().astype('category')\n",
    "    retained_categories['BORO'] = sorted(df_proc['BORO'].cat.categories.tolist())\n",
    "else:\n",
    "    print(\"Warning: 'BORO_NM' not found as expected. 'BORO' column will be all UNKNOWN.\")\n",
    "    df_proc['BORO'] = pd.Series(['UNKNOWN'] * len(df), index=df.index, dtype='category')\n",
    "    retained_categories['BORO'] = ['UNKNOWN']\n",
    "    \n",
    "# Save retained categories for transparency\n",
    "with open(PATH_OUTPUT / 'retained_categories.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(retained_categories, f, indent=2, sort_keys=True)\n",
    "\n",
    "# Validate that we have meaningful data\n",
    "meaningful_features = 0\n",
    "for feature, categories in retained_categories.items():\n",
    "    if len(categories) > 1 and 'UNKNOWN' not in categories:\n",
    "        meaningful_features += 1\n",
    "    elif len(categories) == 1 and categories[0] == 'UNKNOWN':\n",
    "        print(f\"Warning: Feature '{feature}' contains only UNKNOWN values.\")\n",
    "\n",
    "print(f\"Created {len(retained_categories)} categorical features, {meaningful_features} have meaningful variation.\")\n",
    "\n",
    "if PROFILE and 'topn_suggestions' in PROFILE:\n",
    "    print('Top-N guidance (from Diagnostics):')\n",
    "    for k, info in PROFILE['topn_suggestions'].items():\n",
    "        print(f\"  - {k}: top {info['top_n']} cover {info['coverage_pct']:.1f}%\")\n",
    "\n",
    "print(\"Categorical preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be831b2",
   "metadata": {},
   "source": [
    "## Engineer Time, Distance, and Count Buckets\n",
    "\n",
    "Create engineered bins for time and distance features, using available columns and diagnostics-driven binning if present. Standardize missing values and validate feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b085988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc['DATE'] = df['DATE']\n",
    "\n",
    "# TIME_BUCKET normalized: prefer existing column, normalized to uppercase strings\n",
    "if 'TIME_BUCKET' in df.columns:\n",
    "    s_tb = df['TIME_BUCKET'].astype('string').fillna('UNKNOWN').str.strip().str.upper()\n",
    "    df_proc['TIME_BUCKET'] = s_tb.astype('category')\n",
    "    print(f\"TIME_BUCKET found in data. Distribution: {df_proc['TIME_BUCKET'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    # Fallback if TIME_BUCKET is missing: derive from HOUR\n",
    "    if 'HOUR' in df.columns:\n",
    "        hours = pd.to_numeric(df['HOUR'], errors='coerce').fillna(0).astype(int).clip(0, 23)\n",
    "        HOUR_BUCKETS_LOCAL = {\n",
    "            'NIGHT': range(0, 6), 'MORNING': range(6, 12),\n",
    "            'AFTERNOON': range(12, 18), 'EVENING': range(18, 24)\n",
    "        }\n",
    "        bucket_map = {h: name for name, rng in HOUR_BUCKETS_LOCAL.items() for h in rng}\n",
    "        df_proc['TIME_BUCKET'] = hours.map(bucket_map).fillna('NIGHT').astype('category')\n",
    "        print(\"TIME_BUCKET derived from HOUR column.\")\n",
    "    else:\n",
    "        df_proc['TIME_BUCKET'] = pd.Series(['UNKNOWN'] * len(df), index=df.index, dtype='category')\n",
    "        print(\"Warning: Neither TIME_BUCKET nor HOUR found. Setting TIME_BUCKET to UNKNOWN.\")\n",
    "\n",
    "# Numeric bin helpers\n",
    "def bin_numeric(df_in: pd.DataFrame, col: str | None, bins: list, labels: list, default: str) -> pd.Series:\n",
    "    if col is None or col not in df_in.columns:\n",
    "        return pd.Series([default] * len(df_in), index=df_in.index, dtype='category')\n",
    "    vals = pd.to_numeric(df_in[col], errors='coerce')\n",
    "    try:\n",
    "        cut = pd.cut(vals, bins=bins, labels=labels, include_lowest=True)\n",
    "        return cut.cat.add_categories([default]).fillna(default).astype('category')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Binning failed for {col}: {e}\")\n",
    "        return pd.Series([default] * len(df_in), index=df_in.index, dtype='category')\n",
    "\n",
    "# Optional quantile binning for skewed metrics\n",
    "def bin_quantiles(df_in: pd.DataFrame, col: str | None, q: int, labels: list[str] | None = None, default: str = 'UNKNOWN') -> pd.Series:\n",
    "    if col is None or col not in df_in.columns:\n",
    "        return pd.Series([default] * len(df_in), index=df_in.index, dtype='category')\n",
    "    vals = pd.to_numeric(df_in[col], errors='coerce')\n",
    "    try:\n",
    "        qcut = pd.qcut(vals, q=q, labels=labels if labels is not None else False, duplicates='drop')\n",
    "        if labels is None and hasattr(qcut, 'cat'):\n",
    "            qcut = qcut.cat.rename_categories([f'Q{i+1}' for i in range(qcut.cat.categories.size)])\n",
    "        return qcut.astype('category').cat.add_categories([default]).fillna(default)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Quantile binning failed for {col}: {e}\")\n",
    "        return pd.Series([default] * len(df_in), index=df_in.index, dtype='category')\n",
    "\n",
    "# --- Binning for Numeric Features ---\n",
    "# Use the first available column from the list as the primary source\n",
    "def find_first_col(df_in: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    found_cols = [c for c in candidates if c in df_in.columns]\n",
    "    if found_cols:\n",
    "        print(f\"Distance columns available: {found_cols}\")\n",
    "        return found_cols[0]\n",
    "    else:\n",
    "        print(f\"Warning: No distance columns found from candidates: {candidates}\")\n",
    "        return None\n",
    "\n",
    "engineered_features = []\n",
    "\n",
    "# Distance Bins (primary distance proxy)\n",
    "dist_candidates = ['MIN_POI_DISTANCE', 'AVG_POI_DISTANCE', 'MAX_POI_DISTANCE', 'BAR_DISTANCE', 'METRO_DISTANCE', 'NIGHTCLUB_DISTANCE', 'ATM_DISTANCE']\n",
    "primary_dist = find_first_col(df, dist_candidates)\n",
    "df_proc['DIST_BIN'] = bin_numeric(df, primary_dist, DISTANCE_BINS, DISTANCE_LABELS, 'UNKNOWN')\n",
    "engineered_features.append('DIST_BIN')\n",
    "\n",
    "if primary_dist:\n",
    "    print(f\"Using '{primary_dist}' for distance binning. Distribution: {df_proc['DIST_BIN'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    print(\"Warning: No distance columns available for binning. DIST_BIN will be all UNKNOWN.\")\n",
    "\n",
    "print(\"Feature engineering completed.\")\n",
    "print(f\"Engineered features created: {engineered_features}\")\n",
    "print(f\"Diagnostics-driven overrides applied: {', '.join(applied) if applied else 'none'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4132d9a",
   "metadata": {},
   "source": [
    "## Build Transactional One-Hot Encoding\n",
    "\n",
    "Transform the processed features into a transactional one-hot encoded matrix, suitable for FP-Growth mining, and add context columns for downstream conditional analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all engineered and preprocessed features to be used for mining\n",
    "item_cols_for_mining = [\n",
    "    # Core Crime Descriptors\n",
    "    'LAW_CAT', 'LOC_OF_OCCUR',\n",
    "    # Geographic & Temporal Context\n",
    "    'BORO', 'TIME_BUCKET', 'SEASON',\n",
    "    # Suspect & Victim Demographics\n",
    "    'SUSP_AGE', 'SUSP_RACE', 'SUSP_SEX',\n",
    "    'VIC_AGE', 'VIC_RACE', 'VIC_SEX',\n",
    "    # Engineered Bins\n",
    "    'DIST_BIN',\n",
    "    # Engineered Boolean/Flags\n",
    "    'HAS_POI',\n",
    "]\n",
    "\n",
    "# Filter to only include columns that actually exist in the processed DataFrame\n",
    "present_cols = [c for c in item_cols_for_mining if c in df_proc.columns]\n",
    "missing_cols = [c for c in item_cols_for_mining if c not in df_proc.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns for mining: {missing_cols}\")\n",
    "    \n",
    "print(f\"Columns selected for one-hot encoding ({len(present_cols)}/{len(item_cols_for_mining)}): {present_cols}\")\n",
    "\n",
    "if not present_cols:\n",
    "    raise ValueError(\"No valid columns found for pattern mining. Check data preprocessing.\")\n",
    "\n",
    "print(\"Constructing item lists for one-hot encoding...\")\n",
    "# Create series of \"KEY=VALUE\" strings, e.g., \"BORO=BRONX\"\n",
    "item_series = [df_proc[c].astype(str).apply(lambda v: f\"{c}={v}\") for c in present_cols]\n",
    "\n",
    "if item_series:\n",
    "    items_df = pd.concat(item_series, axis=1)\n",
    "\n",
    "    # Stack all item strings into a single series\n",
    "    stacked = items_df.stack()\n",
    "    print(f\"Total item instances before filtering: {len(stacked):,}\")\n",
    "    \n",
    "    # Remove items with UNKNOWN values to reduce noise\n",
    "    stacked = stacked[~stacked.str.endswith('=UNKNOWN')]\n",
    "    print(f\"Item instances after removing UNKNOWN: {len(stacked):,}\")\n",
    "    \n",
    "    if not stacked.empty:\n",
    "        # Efficiently create the one-hot encoded matrix\n",
    "        booleans = pd.get_dummies(stacked, prefix='', prefix_sep='').groupby(level=0).sum().astype(bool)\n",
    "        # Remove any potential duplicate columns that might arise\n",
    "        booleans = booleans.loc[:, ~booleans.columns.duplicated()]\n",
    "        print(f\"One-hot encoding successful. Matrix shape: {booleans.shape}\")\n",
    "    else:\n",
    "        print(\"ERROR: No valid items to encode after filtering UNKNOWNs. Check data quality.\")\n",
    "        booleans = pd.DataFrame(index=df_proc.index)\n",
    "else:\n",
    "    print(\"ERROR: No item columns found to process.\")\n",
    "    booleans = pd.DataFrame(index=df_proc.index)\n",
    "\n",
    "# Ensure the index matches the original DataFrame for alignment\n",
    "booleans = booleans.reindex(df_proc.index, fill_value=False)\n",
    "\n",
    "# Add special context columns (prefixed with '__') for filtering during analysis\n",
    "booleans['__DATE__'] = df_proc['DATE']\n",
    "booleans['__BORO__'] = df_proc['BORO']\n",
    "booleans['__TIME_BUCKET__'] = df_proc['TIME_BUCKET']\n",
    "\n",
    "print(f\"One-hot encoded matrix shape: {booleans.shape}\")\n",
    "mining_cols_count = len([c for c in booleans.columns if not c.startswith('__')])\n",
    "print(f\"Number of unique items for mining: {mining_cols_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca4c8a",
   "metadata": {},
   "source": [
    "## FP-Growth Mining with Auto-Tuning\n",
    "\n",
    "Define FP-Growth mining functions with automatic support tuning to control rule set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_fpgrowth(df_bool: pd.DataFrame, min_support: float, min_conf: float, use_cols: Optional[List[str]] = None, max_len: int = MAX_ITEMSET_LEN):\n",
    "    \"\"\"\n",
    "    Runs the FP-Growth algorithm on a one-hot encoded DataFrame with robust error handling.\n",
    "    \"\"\"\n",
    "    cols = use_cols or [c for c in df_bool.columns if not c.startswith('__')]\n",
    "    if not cols:\n",
    "        print(\"Warning: No columns available for FP-Growth mining.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    basket = df_bool[cols]\n",
    "    if basket.dtypes.ne(bool).any():\n",
    "        basket = basket.astype(bool)\n",
    "\n",
    "    # Check if basket has sufficient support for any itemsets\n",
    "    col_support = basket.mean()\n",
    "    valid_cols = col_support[col_support >= min_support].index.tolist()\n",
    "    \n",
    "    if not valid_cols:\n",
    "        print(f\"Warning: No columns meet minimum support threshold of {min_support:.4f}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    if len(valid_cols) < len(cols):\n",
    "        print(f\"Filtering columns: {len(valid_cols)}/{len(cols)} columns meet support threshold\")\n",
    "        basket = basket[valid_cols]\n",
    "\n",
    "    # Limit search space and show progress\n",
    "    try:\n",
    "        frequent_itemsets = fpgrowth(basket, min_support=min_support, use_colnames=True, max_len=max_len)\n",
    "    except TypeError:\n",
    "        # Fallback for older mlxtend versions without max_len\n",
    "        frequent_itemsets = fpgrowth(basket, min_support=min_support, use_colnames=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during FP-Growth: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    if frequent_itemsets.empty:\n",
    "        print(f\"No frequent itemsets found with min_support={min_support:.4f}\")\n",
    "        return frequent_itemsets, pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=min_conf)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating association rules: {e}\")\n",
    "        return frequent_itemsets, pd.DataFrame()\n",
    "    \n",
    "    print(f\"FP-Growth completed: {len(frequent_itemsets)} itemsets, {len(rules)} rules\")\n",
    "    return frequent_itemsets, rules\n",
    "\n",
    "\n",
    "def auto_tune_support(df_bool: pd.DataFrame, \n",
    "                      target_range=(AUTO_MIN_RULES, AUTO_MAX_RULES),\n",
    "                      support_bounds=AUTO_SUPPORT_BOUNDS, \n",
    "                      init_support=GLOBAL_MIN_SUPPORT,\n",
    "                      min_conf=MIN_CONFIDENCE, \n",
    "                      use_cols: Optional[List[str]] = None,\n",
    "                      context_name: str = \"global\"):\n",
    "    \"\"\"\n",
    "    Automatically adjusts the min_support threshold to find a number of rules\n",
    "    within a specified target range using a binary-search-like approach.\n",
    "    \"\"\"\n",
    "    lo_support, hi_support = support_bounds\n",
    "    min_rules, max_rules = target_range\n",
    "    \n",
    "    # Validate input parameters\n",
    "    if lo_support >= hi_support:\n",
    "        print(f\"Error: Invalid support bounds ({lo_support}, {hi_support})\")\n",
    "        return lo_support, pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    if len(df_bool) < 50:\n",
    "        print(f\"Warning: Dataset too small for reliable mining ({len(df_bool)} records)\")\n",
    "        return lo_support, pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    best_support = float(np.clip(init_support, lo_support, hi_support))\n",
    "    best_rules_count = -1\n",
    "    best_freq = pd.DataFrame()\n",
    "    best_rules = pd.DataFrame()\n",
    "    \n",
    "    print(f\"Auto-tuning for '{context_name}' with target rule range ({min_rules}, {max_rules})...\")\n",
    "    print(f\"Dataset size: {len(df_bool):,} records\")\n",
    "\n",
    "    for i in range(AUTO_MAX_ITERS):\n",
    "        support = (lo_support + hi_support) / 2\n",
    "        \n",
    "        freq, rules = mine_fpgrowth(df_bool, support, min_conf, use_cols)\n",
    "        n_rules = len(rules)\n",
    "        \n",
    "        print(f\"[DEBUG] Iter {i+1}/{AUTO_MAX_ITERS}: support={support:.5f} -> rules={n_rules}\")\n",
    "\n",
    "        # Store the best result found so far that is closest to the target range\n",
    "        if best_rules_count == -1 or abs(n_rules - min_rules) < abs(best_rules_count - min_rules):\n",
    "            best_support = support\n",
    "            best_rules_count = n_rules\n",
    "            best_freq = freq.copy() if not freq.empty else pd.DataFrame()\n",
    "            best_rules = rules.copy() if not rules.empty else pd.DataFrame()\n",
    "\n",
    "        if min_rules <= n_rules <= max_rules:\n",
    "            print(f\"Success: Found {n_rules} rules within target range.\")\n",
    "            return support, freq, rules\n",
    "        \n",
    "        if n_rules < min_rules:\n",
    "            hi_support = support  # Decrease support to find more rules\n",
    "        else: # n_rules > max_rules\n",
    "            lo_support = support  # Increase support to find fewer rules\n",
    "        \n",
    "        if (hi_support - lo_support) < 1e-5:\n",
    "            print(\"Warning: Support range is very narrow. Stopping early.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Warning: Auto-tuning for '{context_name}' finished after {AUTO_MAX_ITERS} iterations without converging.\")\n",
    "\n",
    "    # Fallback: try with very low support if no rules found\n",
    "    if best_rules_count == 0 and lo_support > 0.001:\n",
    "        print(f\"Attempting fallback with very low support (0.001) for '{context_name}'...\")\n",
    "        fallback_freq, fallback_rules = mine_fpgrowth(df_bool, 0.001, min_conf, use_cols)\n",
    "        if len(fallback_rules) > 0:\n",
    "            print(f\"Fallback successful: {len(fallback_rules)} rules found.\")\n",
    "            return 0.001, fallback_freq, fallback_rules\n",
    "\n",
    "    print(f\"Auto-tune for '{context_name}': final support={best_support:.5f}, rules found={best_rules_count}\")\n",
    "    return best_support, best_freq, best_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe90a20",
   "metadata": {},
   "source": [
    "## Generate Association Rule Metrics\n",
    "\n",
    "Compute additional metrics for each discovered rule, including conviction and the Kulczynski metric, to enhance rule evaluation and ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b59533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_additional_metrics(rules_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes additional, useful metrics for evaluating association rules,\n",
    "    such as conviction and Kulczynski metric.\n",
    "    \"\"\"\n",
    "    if rules_df.empty:\n",
    "        return rules_df\n",
    "        \n",
    "    rules = rules_df.copy()\n",
    "    eps = 1e-9  # Small epsilon to avoid division by zero\n",
    "\n",
    "    # --- Conviction ---\n",
    "    # Measures how much a consequent depends on the antecedent.\n",
    "    # High conviction means the rule is very strong.\n",
    "    conf = rules['confidence'].clip(eps, 1 - eps)\n",
    "    rules['conviction'] = (1 - rules['consequent support']) / (1 - conf)\n",
    "\n",
    "    # --- Kulczynski Metric ---\n",
    "    # Kulczynski = 0.5 * (P(B|A) + P(A|B))\n",
    "    # P(B|A) = confidence\n",
    "    # P(A|B) = support(A and B) / support(B) = support / [consequent support]\n",
    "    kulczynski = 0.5 * (\n",
    "        rules['confidence'] +\n",
    "        (rules['support'] / rules['consequent support'].clip(lower=eps))\n",
    "    )\n",
    "    rules[\"kulczynski\"] = kulczynski.fillna(0)\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d38cd",
   "metadata": {},
   "source": [
    "## Prune Redundant Rules and Rank Top-K\n",
    "\n",
    "Remove duplicate and subsumed rules, then score and rank the top-K rules using the Kulczynski metric as the primary criterion for reporting and export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_rule(row):\n",
    "    \"\"\"Creates a canonical, hashable representation of a rule.\"\"\"\n",
    "    antecedent = tuple(sorted(map(str, row['antecedents']))) if row['antecedents'] else ()\n",
    "    consequent = tuple(sorted(map(str, row['consequents']))) if row['consequents'] else ()\n",
    "    return (antecedent, consequent)\n",
    "\n",
    "\n",
    "def prune_redundant_rules(rules_df: pd.DataFrame, conf_tol=1e-6, lift_tol=1e-6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prunes rules that are either duplicates or are subsumed by more general,\n",
    "    stronger rules.\n",
    "    \"\"\"\n",
    "    if rules_df.empty:\n",
    "        return rules_df\n",
    "        \n",
    "    rules = rules_df.copy()\n",
    "    \n",
    "    # 1. Remove exact duplicates based on canonical representation\n",
    "    rules['key'] = rules.apply(canonicalize_rule, axis=1)\n",
    "    rules.drop_duplicates(subset=['key'], inplace=True)\n",
    "    \n",
    "    # 2. Prune by subsumption\n",
    "    # A rule A -> C is subsumed by B -> C if B is a subset of A and the\n",
    "    # metrics of A -> C are not significantly better.\n",
    "    to_drop = set()\n",
    "    # Sort by consequent, then by metrics to process related rules together\n",
    "    rules['consequents_tuple'] = rules['consequents'].apply(lambda s: tuple(sorted(s)))\n",
    "    rules.sort_values(by=['consequents_tuple', 'confidence', 'lift'],\n",
    "                      ascending=[True, False, False],\n",
    "                      inplace=True)\n",
    "\n",
    "    grouped = rules.groupby('consequents_tuple')\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        # Compare each rule with every other rule in the same consequent group\n",
    "        for i, j in combinations(group.index, 2):\n",
    "            if i in to_drop or j in to_drop:\n",
    "                continue\n",
    "            \n",
    "            rule1, rule2 = group.loc[i], group.loc[j]\n",
    "            ant1, ant2 = set(rule1['antecedents']), set(rule2['antecedents'])\n",
    "            \n",
    "            # Check for subsumption\n",
    "            if ant1.issubset(ant2):  # rule2 has a more specific antecedent\n",
    "                # If rule2 is not significantly better, it's redundant\n",
    "                if rule2['confidence'] <= rule1['confidence'] + conf_tol and \\\n",
    "                   rule2['lift'] <= rule1['lift'] + lift_tol:\n",
    "                    to_drop.add(j)\n",
    "            elif ant2.issubset(ant1):  # rule1 has a more specific antecedent\n",
    "                # If rule1 is not significantly better, it's redundant\n",
    "                if rule1['confidence'] <= rule2['confidence'] + conf_tol and \\\n",
    "                   rule1['lift'] <= rule2['lift'] + lift_tol:\n",
    "                    to_drop.add(i)\n",
    "\n",
    "    pruned = rules.drop(index=list(to_drop))\n",
    "    return pruned.drop(columns=['key', 'consequents_tuple'], errors='ignore')\n",
    "\n",
    "\n",
    "def score_and_rank_rules(rules_df: pd.DataFrame, top_k: int = TOP_K) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scores rules based on Kulczynski (main), confidence, and support, and returns the top-k.\n",
    "    \"\"\"\n",
    "    if rules_df.empty:\n",
    "        return rules_df\n",
    "        \n",
    "    r = rules_df.copy()\n",
    "    # Use Kulczynski as primary score, adjusted by support\n",
    "    r['score'] = r['kulczynski'] * np.sqrt(r['support'])\n",
    "    r = r[\n",
    "        (r['kulczynski'] > 0.4) &\n",
    "        (r['lift'] > 1.01) &\n",
    "        (r['confidence'] > 0.4)\n",
    "    ]\n",
    "\n",
    "\n",
    "    def is_trivial_rule(row):\n",
    "        # Terms to consider trivial\n",
    "        trivial_terms = {\n",
    "            'HAS_POI=NO', 'HAS_POI=YES',\n",
    "            'DIST_BIN=250-1000m', 'DIST_BIN=<250m', 'DIST_BIN=>1000m',\n",
    "        }\n",
    "        ant = set(map(str, row['antecedents']))\n",
    "        con = set(map(str, row['consequents']))\n",
    "        # Exclude rules where antecedent and consequent are both trivial (even if different)\n",
    "        if (ant | con) <= trivial_terms:\n",
    "            return True\n",
    "        # Exclude rules where both antecedent and consequent contain at least one trivial term (trivial symmetry)\n",
    "        if ant & trivial_terms and con & trivial_terms:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    r = r[~r.apply(is_trivial_rule, axis=1)]\n",
    "\n",
    "    # Sort by score, then confidence and support as tie-breaker\n",
    "    r.sort_values(by=['score', 'confidence', 'support'], \n",
    "                  ascending=[False, False, False], \n",
    "                  inplace=True)\n",
    "    return r.head(top_k)\n",
    "\n",
    "\n",
    "def rules_to_records(rules_df: pd.DataFrame, context: str, tuned_support: float) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame of rules into a list of structured dictionaries\n",
    "    for easy serialization (e.g., to JSON).\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for _, row in rules_df.iterrows():\n",
    "        records.append({\n",
    "            'context': context,\n",
    "            'antecedent': sorted(map(str, row['antecedents'])),\n",
    "            'consequent': sorted(map(str, row['consequents'])),\n",
    "            'support': float(row.get('support', 0)),\n",
    "            'confidence': float(row.get('confidence', 0)),\n",
    "            'lift': float(row.get('lift', 0)),\n",
    "            'leverage': float(row.get('leverage', 0)),\n",
    "            'conviction': float(row.get('conviction', 0)),\n",
    "            'zhangs_metric': float(row.get(\"zhang's_metric\", 0)),\n",
    "            'score': float(row.get('score', 0)),\n",
    "            'min_support_used': float(tuned_support),\n",
    "        })\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111688fb",
   "metadata": {},
   "source": [
    "## Global Mining, Reporting, and Export\n",
    "\n",
    "Run association rule mining on the full dataset, filter and rank rules primarily by the Kulczynski metric, and export results as JSON, CSV, and human-readable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_global_mining_and_export(df_bool: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Orchestrates the end-to-end process for global rule mining:\n",
    "    - Auto-tunes support\n",
    "    - Calculates metrics\n",
    "    - Prunes and ranks rules\n",
    "    - Exports all artifacts (JSON, CSV, insights)\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Global Mining ---\")\n",
    "\n",
    "    tuned_support, _, all_rules = auto_tune_support(\n",
    "        df_bool,\n",
    "        init_support=0.15,\n",
    "        support_bounds=AUTO_SUPPORT_BOUNDS,\n",
    "        context_name=\"global\"\n",
    "    )\n",
    "\n",
    "    if all_rules.empty:\n",
    "        print(\"Global mining yielded no rules. Aborting export.\")\n",
    "        return {}\n",
    "\n",
    "    all_rules = add_additional_metrics(all_rules)\n",
    "    pruned_rules = prune_redundant_rules(all_rules)\n",
    "    ranked_rules = score_and_rank_rules(pruned_rules, top_k=TOP_K)\n",
    "    \n",
    "    print(f\"Found {len(all_rules):,} rules, pruned to {len(pruned_rules):,}, ranked top {len(ranked_rules)}.\")\n",
    "\n",
    "    # --- Export JSON Catalog ---\n",
    "    global_catalog = {\n",
    "        'context': 'global',\n",
    "        'tuned_min_support': tuned_support,\n",
    "        'total_rules_found': len(all_rules),\n",
    "        'pruned_rules_count': len(pruned_rules),\n",
    "        'top_k_rules': rules_to_records(ranked_rules, 'global', tuned_support)\n",
    "    }\n",
    "    catalog_path = PATH_OUTPUT / 'global_rules.json'\n",
    "    with open(catalog_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(global_catalog, f, indent=2)\n",
    "\n",
    "    # --- Export Human-Readable Insights ---\n",
    "    insights_path = PATH_OUTPUT / 'global_insights.txt'\n",
    "    with open(insights_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"--- Top-K Global Association Rules ---\\n\\n\")\n",
    "        for _, r in ranked_rules.iterrows():\n",
    "            ant = \" AND \".join(sorted(map(str, r['antecedents'])))\n",
    "            con = \" AND \".join(sorted(map(str, r['consequents'])))\n",
    "            insight = (\n",
    "                f\"IF ({ant}) THEN ({con})\\n\"\n",
    "                f\"  - Kulc: {r['kulczynski']:.3f}, Confidence: {r['confidence']:.2%}, Lift: {r['lift']:.2f}, \"\n",
    "                f\"Support: {r['support']:.4f}, Score: {r.get('score', 0):.3f}\\n\\n\"\n",
    "            )\n",
    "            f.write(insight)\n",
    "\n",
    "    # --- Export Top-K Rules to CSV for easy analysis ---\n",
    "    csv_path = PATH_OUTPUT / 'global_topk_rules.csv'\n",
    "    ranked_rules.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"Global mining artifacts exported to: {PATH_OUTPUT}\")\n",
    "    \n",
    "    return {\n",
    "        'tuned_support': tuned_support,\n",
    "        'all_rules': all_rules,\n",
    "        'pruned_rules': pruned_rules,\n",
    "        'ranked_rules': ranked_rules\n",
    "    }\n",
    "\n",
    "# Execute the global mining process\n",
    "GLOBAL_RESULTS = run_global_mining_and_export(booleans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbed446",
   "metadata": {},
   "source": [
    "### Visualize Top-K Global Rules\n",
    "\n",
    "Display a bar chart of the top-K global rules, visualizing their composite score, confidence, and lift for quick interpretation of the most important patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54684a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_k_rules(ranked_rules: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates and displays a bar chart for the top-K rules, showing their\n",
    "    score, confidence, and lift.\n",
    "    \"\"\"\n",
    "    if ranked_rules.empty:\n",
    "        print(\"Cannot plot rules: DataFrame is empty.\")\n",
    "        return\n",
    "\n",
    "    # Create a human-readable label for each rule\n",
    "    ranked_rules['rule_label'] = ranked_rules.apply(\n",
    "        lambda r: f\"IF ({' & '.join(sorted(map(str, r['antecedents'])))}) THEN ({' & '.join(sorted(map(str, r['consequents'])))})\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    fig = px.bar(\n",
    "        ranked_rules.sort_values('score', ascending=True),\n",
    "        x='score',\n",
    "        y='rule_label',\n",
    "        orientation='h',\n",
    "        title='Top-K Global Rules by Score',\n",
    "        labels={'score': 'Composite Score', 'rule_label': 'Association Rule'},\n",
    "        hover_data=['confidence', 'lift', 'support'],\n",
    "        color='confidence',\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        height=max(400, len(ranked_rules) * 30) # Dynamic height\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        yaxis={'categoryorder':'total ascending'},\n",
    "        xaxis_title=\"Composite Score\",\n",
    "        yaxis_title=\"Rule\",\n",
    "        margin=dict(l=400) # Adjust left margin to prevent label cutoff\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Visualize the top-K rules from the global mining results\n",
    "if 'ranked_rules' in GLOBAL_RESULTS and not GLOBAL_RESULTS['ranked_rules'].empty:\n",
    "    plot_top_k_rules(GLOBAL_RESULTS['ranked_rules'])\n",
    "else:\n",
    "    print(\"No ranked rules available to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e52b4",
   "metadata": {},
   "source": [
    "## Per-Borough Conditional Rules\n",
    "\n",
    "Mine and export association rules for each borough, excluding trivial rules, and generate contextual insights for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe816f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_contextual_rules(df_bool: pd.DataFrame, context_col: str, context_name: str):\n",
    "    \"\"\"\n",
    "    Mines association rules for each unique value in a given context column (e.g., for each borough).\n",
    "    \"\"\"\n",
    "    catalog = []\n",
    "    \n",
    "    if f'__{context_col}__' not in df_bool.columns:\n",
    "        print(f\"Error: Context column '__{context_col}__' not found in boolean DataFrame.\")\n",
    "        return catalog\n",
    "    \n",
    "    context_values = sorted(df_bool[f'__{context_col}__'].dropna().unique())\n",
    "    \n",
    "    if not context_values:\n",
    "        print(f\"Warning: No valid values found in context column '{context_col}'.\")\n",
    "        return catalog\n",
    "    \n",
    "    # Exclude the context column itself from the items to be mined to avoid trivial rules\n",
    "    all_mining_cols = [c for c in df_bool.columns if not c.startswith('__')]\n",
    "    \n",
    "    print(f\"Mining {context_name} rules for {len(context_values)} values: {context_values}\")\n",
    "    \n",
    "    for value in tqdm(context_values, desc=f\"Mining per-{context_name}\"):\n",
    "        mask = (df_bool[f'__{context_col}__'] == value)\n",
    "        subset = df_bool.loc[mask]\n",
    "        \n",
    "        # Skip if the subset is too small for meaningful analysis\n",
    "        min_records = max(200, len(df_bool) * 0.01)  # At least 1% of data or 200 records\n",
    "        if len(subset) < min_records:\n",
    "            print(f\"Skipping {context_name} '{value}': insufficient records ({len(subset)} < {min_records:.0f}).\")\n",
    "            continue\n",
    "\n",
    "        # Define items to use for this specific context, excluding any item related to the context value itself\n",
    "        # This prevents rules like IF (X) -> (BORO=BROOKLYN) when mining only for Brooklyn.\n",
    "        item_prefix_to_exclude = f\"{context_col.upper()}=\"\n",
    "        contextual_mining_cols = [\n",
    "            c for c in all_mining_cols if not c.startswith(item_prefix_to_exclude)\n",
    "        ]\n",
    "        \n",
    "        if not contextual_mining_cols:\n",
    "            print(f\"Warning: No valid mining columns for {context_name} '{value}' after filtering.\")\n",
    "            continue\n",
    "            \n",
    "        # Auto-tune support for this specific slice of data using faster bounds\n",
    "        init_support = np.clip(SLICE_MIN_SUPPORT_RANGE[0], *AUTO_SUPPORT_BOUNDS)\n",
    "        tuned_supp, _, rules = auto_tune_support(\n",
    "            subset, \n",
    "            init_support=init_support, \n",
    "            support_bounds=AUTO_SUPPORT_BOUNDS,\n",
    "            context_name=f\"{context_name}:{value}\",\n",
    "            use_cols=contextual_mining_cols\n",
    "        )\n",
    "        \n",
    "        if rules.empty:\n",
    "            print(f\"No rules found for {context_name} '{value}' with support >= {tuned_supp:.4f}.\")\n",
    "            continue\n",
    "\n",
    "        rules = add_additional_metrics(rules)\n",
    "        rules_p = prune_redundant_rules(rules)\n",
    "        rules_k = score_and_rank_rules(rules_p, TOP_K)\n",
    "\n",
    "        context_label = f'{context_name}:{value}'\n",
    "        catalog.append({\n",
    "            'context': context_label,\n",
    "            'value': value,\n",
    "            'records_count': len(subset),\n",
    "            'min_support_used': tuned_supp,\n",
    "            'rule_count': len(rules_p),\n",
    "            'top_k_rules': rules_to_records(rules_k, context_label, tuned_supp)\n",
    "        })\n",
    "\n",
    "        # --- Export insights for this specific context ---\n",
    "        context_slug = f\"{context_name}_{str(value).replace(' ', '_')}\"\n",
    "        insights_path = PATH_OUTPUT / f\"{context_slug}_insights.txt\"\n",
    "        with open(insights_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"--- Top-K Rules for {context_name} = {value} ({len(subset):,} records) ---\\n\\n\")\n",
    "            for _, r in rules_k.iterrows():\n",
    "                ant = \" AND \".join(sorted(map(str, r['antecedents'])))\n",
    "                con = \" AND \".join(sorted(map(str, r['consequents'])))\n",
    "                f.write(\n",
    "                    f\"IF ({ant}) THEN ({con})\\n\"\n",
    "                    f\"  - Kulc: {r['kulczynski']:.3f}, Conf: {r['confidence']:.2%}, Lift: {r['lift']:.2f}, \"\n",
    "                    f\"Support: {r['support']:.4f}, Score: {r.get('score', 0):.3f}\\n\\n\"\n",
    "                )\n",
    "    \n",
    "    print(f\"Completed {context_name} mining: {len(catalog)} contexts with rules found.\")\n",
    "    return catalog\n",
    "\n",
    "# --- Mine rules for each Borough ---\n",
    "print(\"\\n--- Starting Per-Borough Mining ---\")\n",
    "BORO_CATALOG = mine_contextual_rules(booleans, 'BORO', 'Borough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e10d1d",
   "metadata": {},
   "source": [
    "## Per-Time-Bucket Conditional Rules\n",
    "\n",
    "Mine and export association rules for each time bucket, including contextual insights for each temporal segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Per-Time-Bucket Mining ---\")\n",
    "HOUR_CATALOG = mine_contextual_rules(booleans, 'TIME_BUCKET', 'TimeBucket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d60563",
   "metadata": {},
   "source": [
    "## Temporal Hold-out Validation\n",
    "\n",
    "Split the data temporally, mine rules on the training set, and evaluate their generalization on the hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_holdout_split(df_bool: pd.DataFrame, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data into a training and a test set based on time.\n",
    "    \"\"\"\n",
    "    if df_bool['__DATE__'].isna().all():\n",
    "        print(\"Warning: No date information available for temporal split. Skipping hold-out validation.\")\n",
    "        return None, None\n",
    "\n",
    "    valid_dates = df_bool['__DATE__'].dropna()\n",
    "    if len(valid_dates) < 1000:\n",
    "        print(f\"Warning: Insufficient data with valid dates ({len(valid_dates)} records). Skipping hold-out validation.\")\n",
    "        return None, None\n",
    "\n",
    "    sorted_dates = valid_dates.sort_values()\n",
    "    split_point = sorted_dates.iloc[int(len(sorted_dates) * (1 - test_size))]\n",
    "    \n",
    "    train_df = df_bool[df_bool['__DATE__'] <= split_point]\n",
    "    test_df = df_bool[df_bool['__DATE__'] > split_point]\n",
    "    \n",
    "    # Ensure both sets have minimum viable size\n",
    "    min_size = 500\n",
    "    if len(train_df) < min_size or len(test_df) < min_size:\n",
    "        print(f\"Warning: Train ({len(train_df)}) or test ({len(test_df)}) set too small (< {min_size}). Skipping validation.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Temporal split: {split_point.date()} | Train: {len(train_df):,} | Test: {len(test_df):,}\")\n",
    "    return train_df, test_df\n",
    "\n",
    "def evaluate_rules_on_holdout(rules: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Evaluates the support and confidence of discovered rules on a holdout test set.\n",
    "    \"\"\"\n",
    "    if rules.empty or test_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    test_metrics = []\n",
    "    for _, rule in rules.iterrows():\n",
    "        ant = list(rule['antecedents'])\n",
    "        con = list(rule['consequents'])\n",
    "        \n",
    "        # Check if all items in the rule exist in the test set columns\n",
    "        if not all(item in test_df.columns for item in ant + con):\n",
    "            test_metrics.append({'test_support': 0, 'test_confidence': 0})\n",
    "            continue\n",
    "\n",
    "        # Calculate support and confidence on the test set\n",
    "        support_ant = test_df[ant].all(axis=1).mean()\n",
    "        support_union = test_df[ant + con].all(axis=1).mean()\n",
    "        \n",
    "        confidence = support_union / support_ant if support_ant > 0 else 0\n",
    "        \n",
    "        test_metrics.append({\n",
    "            'test_support': support_union,\n",
    "            'test_confidence': confidence\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(test_metrics, index=rules.index)\n",
    "\n",
    "def run_holdout_validation(df_bool: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Orchestrates the temporal hold-out validation process.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Temporal Hold-out Validation ---\")\n",
    "    train_df, test_df = temporal_holdout_split(df_bool)\n",
    "    \n",
    "    if train_df is None or test_df is None or train_df.empty or test_df.empty:\n",
    "        print(\"Skipping holdout validation due to insufficient data in train or test sets.\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Train set: {len(train_df)} records, Test set: {len(test_df)} records.\")\n",
    "    \n",
    "    # Mine rules on the training set\n",
    "    tuned_supp, _, train_rules = auto_tune_support(train_df, context_name=\"holdout_train\")\n",
    "    if train_rules.empty:\n",
    "        print(\"No rules found in the training set. Aborting holdout validation.\")\n",
    "        return {}\n",
    "\n",
    "    train_rules = add_additional_metrics(train_rules)\n",
    "    train_rules_p = prune_redundant_rules(train_rules)\n",
    "    train_rules_k = score_and_rank_rules(train_rules_p, TOP_K)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_metrics = evaluate_rules_on_holdout(train_rules_k, test_df)\n",
    "    \n",
    "    # Combine results\n",
    "    comparison_df = train_rules_k.join(test_metrics)\n",
    "    \n",
    "    # Export artifacts\n",
    "    comparison_df.to_csv(PATH_OUTPUT / 'holdout_train_test_comparison.csv', index=False)\n",
    "    \n",
    "    summary = {\n",
    "        'train_records': len(train_df),\n",
    "        'test_records': len(test_df),\n",
    "        'tuned_min_support': tuned_supp,\n",
    "        'train_rules_found': len(train_rules),\n",
    "        'top_k_evaluated': len(comparison_df),\n",
    "        'mean_test_confidence': comparison_df['test_confidence'].mean(),\n",
    "        'mean_confidence_dropoff': (comparison_df['confidence'] - comparison_df['test_confidence']).mean()\n",
    "    }\n",
    "    with open(PATH_OUTPUT / 'holdout_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "        \n",
    "    print(\"Holdout validation completed and artifacts exported.\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d815023",
   "metadata": {},
   "source": [
    "## Rolling Window Evaluation and Stability Tracking\n",
    "\n",
    "Perform rule mining on rolling temporal windows to assess the stability of discovered patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22398847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_windows(df_bool: pd.DataFrame, window_days: int = 90, step_days: int = 30):\n",
    "    \"\"\"\n",
    "    Generates rolling time windows from the dataset for temporal stability analysis.\n",
    "    \"\"\"\n",
    "    dates = df_bool['__DATE__']\n",
    "    if dates.isna().all():\n",
    "        print(\"Warning: No date information for rolling window analysis. Using a single window.\")\n",
    "        yield (None, None, df_bool)\n",
    "        return\n",
    "\n",
    "    min_date, max_date = dates.min(), dates.max()\n",
    "    start_date = min_date\n",
    "    \n",
    "    print(f\"Creating rolling windows of {window_days} days, with a {step_days}-day step...\")\n",
    "    while start_date <= max_date: # Use <= to include the last partial window\n",
    "        end_date = start_date + timedelta(days=window_days)\n",
    "        mask = (dates >= start_date) & (dates < end_date)\n",
    "        subset = df_bool.loc[mask]\n",
    "        \n",
    "        if len(subset) >= 200:  # Ensure window has enough data\n",
    "            print(f\"  - Yielding window: {start_date.date()} to {end_date.date()} ({len(subset)} records)\")\n",
    "            yield (start_date, end_date, subset)\n",
    "            \n",
    "        start_date += timedelta(days=step_days)\n",
    "\n",
    "\n",
    "def run_rolling_window_evaluation(df_bool: pd.DataFrame, window_days: int = 90, step_days: int = 30):\n",
    "    \"\"\"\n",
    "    Performs rule mining on each rolling window to evaluate the stability of rules over time.\n",
    "    \"\"\"\n",
    "    window_outputs = []\n",
    "    \n",
    "    for start, end, subset in create_rolling_windows(df_bool, window_days, step_days):\n",
    "        window_label = f\"{start.date()}_to_{end.date()}\" if start and end else \"full_dataset\"\n",
    "        print(f\"  - Mining window: {window_label} ({len(subset)} records)\")\n",
    "\n",
    "        tuned_supp, _, rules = auto_tune_support(subset, context_name=window_label)\n",
    "        if rules.empty:\n",
    "            continue\n",
    "            \n",
    "        rules = add_additional_metrics(rules)\n",
    "        rules_p = prune_redundant_rules(rules)\n",
    "        rules_k = score_and_rank_rules(rules_p, TOP_K)\n",
    "        \n",
    "        # For stability analysis, we need a canonical ID for each rule\n",
    "        rule_ids = [canonicalize_rule(r) for _, r in rules_k.iterrows()]\n",
    "        \n",
    "        window_outputs.append({\n",
    "            'window_start': start.isoformat() if start else None,\n",
    "            'window_end': end.isoformat() if end else None,\n",
    "            'min_support_used': tuned_supp,\n",
    "            'rule_ids': rule_ids,\n",
    "            'top_k_rules': rules_to_records(rules_k, f'window:{window_label}', tuned_supp)\n",
    "        })\n",
    "        \n",
    "    return window_outputs\n",
    "\n",
    "# --- Execute Rolling Window Analysis ---\n",
    "print(\"\\n--- Starting Rolling Window Evaluation ---\")\n",
    "WINDOW_RESULTS = run_rolling_window_evaluation(booleans, window_days=90, step_days=30)\n",
    "\n",
    "# --- Export results to JSON and a flattened CSV for easy analysis ---\n",
    "if WINDOW_RESULTS:\n",
    "    with open(PATH_OUTPUT / 'rolling_window_results.json', 'w', encoding='utf-8') as f:\n",
    "        # Custom encoder to handle frozensets in rule IDs\n",
    "        class SetEncoder(json.JSONEncoder):\n",
    "            def default(self, obj):\n",
    "                if isinstance(obj, (set, frozenset)):\n",
    "                    return sorted(list(obj))\n",
    "                return super().default(obj)\n",
    "        json.dump(WINDOW_RESULTS, f, indent=2, cls=SetEncoder)\n",
    "\n",
    "    # Flatten the results for a CSV export\n",
    "    flat_records = []\n",
    "    for window in WINDOW_RESULTS:\n",
    "        for rule_record in window['top_k_rules']:\n",
    "            flat_records.append({\n",
    "                'window_start': window['window_start'],\n",
    "                'window_end': window['window_end'],\n",
    "                **rule_record\n",
    "            })\n",
    "    if flat_records:\n",
    "        pd.DataFrame(flat_records).to_csv(PATH_OUTPUT / 'rolling_window_results.csv', index=False)\n",
    "    print(\"Rolling window results exported.\")\n",
    "else:\n",
    "    print(\"No results from rolling window evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e2957",
   "metadata": {},
   "source": [
    "## Stability Analysis and Comparative Reporting\n",
    "\n",
    "Analyze the overlap of top-K rules between consecutive windows using Jaccard similarity and export stability metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stability(window_outputs: List[dict]):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard similarity between the rule sets of consecutive windows.\n",
    "    \"\"\"\n",
    "    if not window_outputs or len(window_outputs) < 2:\n",
    "        print(\"Skipping stability analysis: not enough windows to compare.\")\n",
    "        return {'mean_jaccard_overlap': None, 'overlaps': []}\n",
    "        \n",
    "    print(f\"\\n--- Analyzing stability across {len(window_outputs)} windows ---\")\n",
    "    overlaps = []\n",
    "    for i in range(len(window_outputs) - 1):\n",
    "        # The rule_ids are lists of tuples: ( (ant1, ant2), (con1,) )\n",
    "        # These need to be converted to a set of hashable tuples for comparison.\n",
    "        set1 = set(window_outputs[i].get('rule_ids', []))\n",
    "        set2 = set(window_outputs[i+1].get('rule_ids', []))\n",
    "        \n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        \n",
    "        jaccard = intersection / union if union > 0 else 0\n",
    "        overlaps.append(jaccard)\n",
    "        \n",
    "    return {\n",
    "        'mean_jaccard_overlap': float(np.mean(overlaps)) if overlaps else 0,\n",
    "        'overlaps': overlaps\n",
    "    }\n",
    "\n",
    "# --- Perform and Export Stability Analysis ---\n",
    "if 'WINDOW_RESULTS' in globals() and WINDOW_RESULTS:\n",
    "    STABILITY_ANALYSIS = analyze_stability(WINDOW_RESULTS)\n",
    "    with open(PATH_OUTPUT / 'stability_analysis.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(STABILITY_ANALYSIS, f, indent=2)\n",
    "\n",
    "    print(\"Stability analysis completed and exported.\")\n",
    "else:\n",
    "    print(\"Skipping stability analysis as no window results were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15b042",
   "metadata": {},
   "source": [
    "### Visualize Rule Stability Over Time\n",
    "\n",
    "Show a line chart of the Jaccard similarity of top-K rules between consecutive rolling windows to visualize the temporal stability of discovered patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf453b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stability_analysis(stability_analysis: dict, window_results: list):\n",
    "    \"\"\"\n",
    "    Generates and displays a line chart showing the Jaccard similarity\n",
    "    between consecutive windows.\n",
    "    \"\"\"\n",
    "    if not stability_analysis or not stability_analysis.get('overlaps'):\n",
    "        print(\"Cannot plot stability: No overlap data available.\")\n",
    "        return\n",
    "\n",
    "    overlaps = stability_analysis['overlaps']\n",
    "    \n",
    "    # Create labels for the x-axis representing the window transitions\n",
    "    window_labels = []\n",
    "    for i in range(len(window_results) - 1):\n",
    "        start1 = pd.to_datetime(window_results[i]['window_start']).strftime('%Y-%m-%d')\n",
    "        start2 = pd.to_datetime(window_results[i+1]['window_start']).strftime('%Y-%m-%d')\n",
    "        window_labels.append(f\"Window {i+1} to {i+2}<br>({start1} / {start2})\")\n",
    "\n",
    "    fig = go.Figure(data=go.Scatter(\n",
    "        x=window_labels,\n",
    "        y=overlaps,\n",
    "        mode='lines+markers',\n",
    "        marker=dict(color='royalblue', size=10),\n",
    "        line=dict(color='lightgray', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Rule Stability: Jaccard Overlap Between Consecutive Windows',\n",
    "        xaxis_title='Window Transition',\n",
    "        yaxis_title='Jaccard Similarity',\n",
    "        yaxis=dict(range=[0, 1]),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Visualize the stability analysis results\n",
    "if 'STABILITY_ANALYSIS' in globals() and 'WINDOW_RESULTS' in globals():\n",
    "    plot_stability_analysis(STABILITY_ANALYSIS, WINDOW_RESULTS)\n",
    "else:\n",
    "    print(\"No stability analysis results available to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b0ce3",
   "metadata": {},
   "source": [
    "## Export Final Rule Catalog and Consolidated Insights\n",
    "\n",
    "Consolidate all results and insights into a single master JSON catalog and a unified human-readable summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b284672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_export_final_catalog():\n",
    "    \"\"\"\n",
    "    Consolidates all results from the different mining contexts into a single,\n",
    "    master JSON catalog file.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building and Exporting Final Rule Catalog ---\")\n",
    "    \n",
    "    # --- Load results from individual files ---\n",
    "    def load_json_artifact(filename, default=None):\n",
    "        path = PATH_OUTPUT / filename\n",
    "        if path.exists():\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return default or {}\n",
    "\n",
    "    # --- Consolidate Holdout Details ---\n",
    "    holdout_details = {}\n",
    "    if (PATH_OUTPUT / 'holdout_train_test_comparison.csv').exists():\n",
    "        comp_df = pd.read_csv(PATH_OUTPUT / 'holdout_train_test_comparison.csv')\n",
    "        holdout_details = {\n",
    "            'summary': load_json_artifact('holdout_summary.json'),\n",
    "            'top_k_comparison': comp_df.to_dict('records')\n",
    "        }\n",
    "\n",
    "    # --- Build the master catalog ---\n",
    "    master_catalog = {\n",
    "        'metadata': {\n",
    "            'creation_timestamp': datetime.now().isoformat(),\n",
    "            'notebook': NOTEBOOK_NAME,\n",
    "            'input_data': str(INPUT_CSV),\n",
    "        },\n",
    "        'global': load_json_artifact('global_rules.json'),\n",
    "        'by_borough': BORO_CATALOG if 'BORO_CATALOG' in globals() else [],\n",
    "        'by_time_bucket': HOUR_CATALOG if 'HOUR_CATALOG' in globals() else [],\n",
    "        'temporal_holdout_validation': holdout_details,\n",
    "        'temporal_stability': {\n",
    "            'rolling_window_results': load_json_artifact('rolling_window_results.json', default=[]),\n",
    "            'stability_metrics': load_json_artifact('stability_analysis.json'),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # --- Write the final catalog to disk ---\n",
    "    final_catalog_path = PATH_OUTPUT / 'rule_catalog.json'\n",
    "    with open(final_catalog_path, 'w', encoding='utf-8') as f:\n",
    "        # Use the same custom encoder for sets/frozensets\n",
    "        class SetEncoder(json.JSONEncoder):\n",
    "            def default(self, obj):\n",
    "                if isinstance(obj, (set, frozenset)):\n",
    "                    return sorted(list(obj))\n",
    "                return super().default(obj)\n",
    "        json.dump(master_catalog, f, indent=2, cls=SetEncoder)\n",
    "    \n",
    "    print(f\"Final rule catalog exported to: {final_catalog_path}\")\n",
    "\n",
    "    # --- Consolidate all human-readable insights into one file ---\n",
    "    consolidated_insights_path = PATH_OUTPUT / 'consolidated_insights.txt'\n",
    "    with open(consolidated_insights_path, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write(\"--- CONSOLIDATED INSIGHTS ---\\n\\n\")\n",
    "        for insight_file in sorted(PATH_OUTPUT.glob('*_insights.txt')):\n",
    "            f_out.write(f\"--- {insight_file.stem.replace('_', ' ').upper()} ---\\n\")\n",
    "            f_out.write(insight_file.read_text(encoding='utf-8'))\n",
    "            f_out.write(\"\\n\\n\")\n",
    "            \n",
    "    print(f\"Consolidated insights exported to: {consolidated_insights_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfef92",
   "metadata": {},
   "source": [
    "## End-to-End Run Orchestration and Sanity Checks\n",
    "\n",
    "Verify the presence of all key output artifacts and provide a summary of the pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d139ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_artifacts_exist():\n",
    "    \"\"\"\n",
    "    Checks for the presence of key output files to verify that the\n",
    "    pipeline has run successfully.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Verifying Artifacts ---\")\n",
    "    expected_artifacts = [\n",
    "        PATH_OUTPUT / 'global_rules.json',\n",
    "        PATH_OUTPUT / 'global_insights.txt',\n",
    "        PATH_OUTPUT / 'global_topk_rules.csv',\n",
    "        PATH_OUTPUT / 'rule_catalog.json',\n",
    "        PATH_OUTPUT / 'retained_categories.json',\n",
    "    ]\n",
    "    \n",
    "    missing_required = [str(p) for p in expected_artifacts if not p.exists()]\n",
    "    \n",
    "    if not missing_required:\n",
    "        print(\"All required artifacts were created successfully.\")\n",
    "    else:\n",
    "        print(\"Missing required artifacts:\")\n",
    "        for f in missing_required:\n",
    "            print(f\"  - {f}\")\n",
    "\n",
    "    # Check data quality indicators\n",
    "    if (PATH_OUTPUT / 'retained_categories.json').exists():\n",
    "        with open(PATH_OUTPUT / 'retained_categories.json', 'r') as f:\n",
    "            categories = json.load(f)\n",
    "        \n",
    "        print(f\"\\n--- Data Quality Summary ---\")\n",
    "        total_features = len(categories)\n",
    "        meaningful_features = sum(1 for cats in categories.values() if len(cats) > 1 and 'UNKNOWN' not in cats)\n",
    "        print(f\"Total categorical features: {total_features}\")\n",
    "        print(f\"Features with meaningful variation: {meaningful_features}\")\n",
    "        \n",
    "    return not missing_required\n",
    "\n",
    "\n",
    "def main_orchestrator():\n",
    "    \"\"\"\n",
    "    A main function to provide a final summary and verification.\n",
    "    This cell can be run to confirm the end-to-end execution.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Pipeline Execution Summary ---\")\n",
    "    \n",
    "    # Preview the main rule catalog\n",
    "    catalog_path = PATH_OUTPUT / 'rule_catalog.json'\n",
    "    if catalog_path.exists():\n",
    "        try:\n",
    "            with open(catalog_path, 'r', encoding='utf-8') as f:\n",
    "                catalog_data = json.load(f)\n",
    "            \n",
    "            print(\"\\nSuccessfully loaded the final rule catalog.\")\n",
    "            print(f\"Catalog contains contexts: {list(catalog_data.keys())}\")\n",
    "            \n",
    "            # Display a preview of the top global rules\n",
    "            if 'global' in catalog_data and catalog_data['global'].get('top_k_rules'):\n",
    "                print(\"\\n--- Top 3 Global Rules Preview ---\")\n",
    "                for rule in catalog_data['global']['top_k_rules'][:3]:\n",
    "                    ant = \" AND \".join(rule['antecedent'])\n",
    "                    con = \" AND \".join(rule['consequent'])\n",
    "                    print(f\"IF ({ant}) -> THEN ({con}) | Confidence: {rule['confidence']:.2%}\")\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error reading or parsing the rule catalog: {e}\")\n",
    "    else:\n",
    "        print(\"Rule catalog not found. Please ensure the full notebook has been run.\")\n",
    "\n",
    "    # Final verification of all generated files\n",
    "    verify_artifacts_exist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303aef8",
   "metadata": {},
   "source": [
    "## Finalize Catalog and Run Orchestrator\n",
    "\n",
    "Build the consolidated catalog and run the orchestrator to summarize and verify the end-to-end pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the consolidated catalog from all previously generated artifacts.\n",
    "build_and_export_final_catalog()\n",
    "\n",
    "# Run the main orchestrator to provide a final summary and verify artifacts.\n",
    "main_orchestrator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9396fa",
   "metadata": {},
   "source": [
    "# Crime Analyzer (Frequent Pattern Analysis) – Final Results and Comparative Analysis\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "The **Crime Pattern Miner** pipeline was developed to extract **frequent itemsets and association rules** from the **NYC Crime Dataset**, focusing on interpretability and temporal robustness. It uses **FP-Growth** with adaptive support tuning, contextual mining (by borough, time buckets), and temporal validation (hold-out and rolling windows).\n",
    "\n",
    "### Key Design Features:\n",
    "\n",
    "* **Data**:\n",
    "\n",
    "  * NYC crime data (2024 subset), enriched with spatial proxies (POIs, distances).\n",
    "  * Derived features: categorical normalization, distance/temporal binning, transposed into transactional one-hot encoding (`KEY=VALUE` format).\n",
    "* **Preprocessing & Feature Engineering**:\n",
    "\n",
    "  * Category capping (`TOP-N` + “OTHER”), unknown handling, temporal binning.\n",
    "  * One-hot transactional encoding for FP-Growth mining.\n",
    "* **Mining setup**:\n",
    "\n",
    "  * FP-Growth with auto-tuned minimum support.\n",
    "  * Advanced metrics: lift, conviction, Kulczynski.\n",
    "  * Rule pruning (redundancy removal) and ranking with composite scoring.\n",
    "* **Validation**:\n",
    "\n",
    "  * Hold-out temporal split for generalization.\n",
    "  * Rolling-window stability analysis via Jaccard similarity.\n",
    "* **Outputs**:\n",
    "\n",
    "  * Global, borough-specific, and time-bucket-specific catalogs.\n",
    "  * JSON/CSV/text export and interactive visualizations.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Global Results\n",
    "\n",
    "The pipeline consistently produced interpretable association rules (e.g., linking **crime type, time-of-day, and POI context**).\n",
    "\n",
    "* **Adaptive support tuning** avoided trivial or overly sparse rules.\n",
    "* **Temporal hold-out evaluation** showed variable generalizability, with some rules stable across test windows, others highly transient.\n",
    "* **Rolling stability analysis** confirmed that certain frequent rules (e.g., time-of-day + assault) persist, while context-specific rules (e.g., borough + POI proximity) fluctuate.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Comparison with Related Work\n",
    "\n",
    "| Paper & Year                 | Task                                           | Data & Features                                             | Evaluation                          | Method(s)                       | Reported Findings                                            | Notes                                                                                             |\n",
    "| ---------------------------- | ---------------------------------------------- | ----------------------------------------------------------- | ----------------------------------- | ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------- |\n",
    "| **Pereira & Brandão (2014)** | ARM for police resource allocation             | Brazilian crime data, multidimensional OLAP schema          | Descriptive, no temporal validation | Apriori (OLAP-based)            | Extracted category-level rules (e.g., crime type ↔ location) | Focus on rule interpretability and police decision support, no stability analysis                 |\n",
    "| **Buczak & Gifford (2010)**  | Fuzzy ARM for socio-demographic crime          | U.S. *Communities and Crime* dataset (demographics + crime) | Cross-sectional                     | Fuzzy Apriori                   | Patterns across national/regional/local scales               | Handles uncertainty with fuzzy sets; different from strict categorical encoding                   |\n",
    "| **Matto (2018)**             | Frequent itemset mining with multiple supports | Crime dataset (unspecified)                                 | Offline tuning                      | FP-Growth + MIS (entropy-based) | Emerging/seasonal rules captured                             | Highlights flexible thresholds; comparable to our adaptive support, but different tuning approach |\n",
    "| **Marzan et al. (2017)**     | Crime hotspot analysis + ARM                   | Manila geocoded crimes (2012–16)                            | Spatial-temporal hotspot evaluation | Apriori + hotspot analysis      | Identified recurring spatial-temporal patterns               | Strong focus on geography; less emphasis on temporal validation of rules                          |\n",
    "| **Chen et al. (2020)**       | Spatial ARM for near-repeat crime              | Near-repeat crime data (grid-based)                         | Spatial rule mining                 | Custom Spatial ARM              | Detected “source” and “sink” crime regions                   | Similar in concept to borough/time-bucket conditioning; more spatially formalized                 |\n",
    "| **Dang et al. (2020)**       | Efficiency in ARM for crime                    | Real/synthetic crime datasets                               | Performance benchmarks              | IRM (Apriori variant)           | Reduced scans, faster ARM                                    | Optimization of mining speed, not stability or interpretability                                   |\n",
    "| **Caliskan et al. (2021)**   | ARM on state-level crime                       | Maryland crime data (2016–18)                               | Descriptive                         | Apriori & FP-Growth             | Rules linking time, district, crime type                     | Similar structure to ours, but without auto-tuning or temporal generalization                     |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Interpretation of Results\n",
    "\n",
    "* **Techniques**:\n",
    "\n",
    "  * Many prior works rely on **Apriori**, with some extensions (spatial, fuzzy, efficiency-focused). Our approach uses **FP-Growth**, aligning with Caliskan (2021), but introduces **temporal validation and adaptive tuning**, which most studies omit.\n",
    "* **Data & Features**:\n",
    "\n",
    "  * Pereira (2014) and Buczak (2010) integrate socio-demographic or OLAP-style attributes, while Marzan (2017) and Chen (2020) emphasize spatial dimensions. Our pipeline instead emphasizes **transactional encoding and POI-enriched features**, more aligned with urban context mining.\n",
    "* **Validation**:\n",
    "\n",
    "  * Most works are descriptive (static datasets). Few (e.g., Chen 2020) include spatial validation, but **temporal hold-out and rolling stability checks are rarely addressed**. Our design adds this dimension.\n",
    "* **Metrics**:\n",
    "\n",
    "  * Lift and confidence dominate the literature. Conviction and Kulczynski are less common, suggesting our metric choice broadens interpretability.\n",
    "* **Interpretability vs. Scalability**:\n",
    "\n",
    "  * Studies like Dang (2020) optimize performance but not interpretability, while Pereira (2014) maximizes interpretability but not scalability. Our approach positions itself between: pruning and ranking rules for usability, but with computational efficiency via FP-Growth.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "Compared to existing studies:\n",
    "\n",
    "* **Overlap**: Shared use of frequent pattern mining (Apriori/FP-Growth) and focus on interpretable rules (crime type, location, time).\n",
    "* **Differences**: Our pipeline emphasizes **temporal generalization and stability**, aspects largely absent in earlier works. Feature engineering also diverges, with stronger use of **POI context** rather than purely demographic or spatial grids.\n",
    "* **Critical note**: While temporal validation improves robustness, it also highlights instability of many rules — an issue not explored in most related work, where static descriptive mining risks overfitting to historical data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
